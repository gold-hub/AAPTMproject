{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8261fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/sandeep3031/breast-cancer-diagnosis-using-ann/notebook\n",
    "# https://www.kaggle.com/code/damienpark/artificial-neural-network-using-keras/notebook\n",
    "#importing libaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b09513c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/gold/Desktop/models/datas/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87560b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>profession</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>coronary heart disease</th>\n",
       "      <th>stroke</th>\n",
       "      <th>others</th>\n",
       "      <th>Treatment time period</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Pulse</th>\n",
       "      <th>RR</th>\n",
       "      <th>BP</th>\n",
       "      <th>SPO2</th>\n",
       "      <th>anemic appearance</th>\n",
       "      <th>complexion</th>\n",
       "      <th>consciousness</th>\n",
       "      <th>Way to enter wards</th>\n",
       "      <th>borborygmus</th>\n",
       "      <th>obvious cause</th>\n",
       "      <th>pain position</th>\n",
       "      <th>nausea and vomiting</th>\n",
       "      <th>diarrhea and fever</th>\n",
       "      <th>haematemesis</th>\n",
       "      <th>tenesmus</th>\n",
       "      <th>syncope</th>\n",
       "      <th>Persistent pain</th>\n",
       "      <th>other property of pain</th>\n",
       "      <th>Pain scores</th>\n",
       "      <th>pressing pain and rebounding pain</th>\n",
       "      <th>abdominal wall tensity</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  sex  age  profession  hypertension  diabetes  coronary heart disease  \\\n",
       "0   1    1    3           0             1         0                       0   \n",
       "1   2    2    3           0             1         1                       1   \n",
       "2   3    2    3           0             1         1                       1   \n",
       "3   4    1    3           0             1         0                       1   \n",
       "4   5    2    3           0             1         0                       0   \n",
       "\n",
       "   stroke  others  Treatment time period  Temperature  Pulse  RR  BP  SPO2   \\\n",
       "0       1       1                      2            1      2   2   2      1   \n",
       "1       1       1                      2            1      2   2   2      1   \n",
       "2       0       1                      2            1      2   2   2      1   \n",
       "3       1       0                      1            1      1   2   2      2   \n",
       "4       0       1                      2            2      2   2   2      1   \n",
       "\n",
       "   anemic appearance  complexion  consciousness  Way to enter wards  \\\n",
       "0                  1           2              1                   2   \n",
       "1                  0           0              1                   1   \n",
       "2                  1           0              1                   1   \n",
       "3                  1           2              1                   2   \n",
       "4                  1           2              1                   1   \n",
       "\n",
       "   borborygmus  obvious cause  pain position  nausea and vomiting  \\\n",
       "0            2              0              5                    0   \n",
       "1            1              0              1                    0   \n",
       "2            3              0              5                    0   \n",
       "3            1              0              4                    0   \n",
       "4            1              0              2                    0   \n",
       "\n",
       "   diarrhea and fever  haematemesis  tenesmus  syncope  Persistent pain  \\\n",
       "0                   0             0         0        0                1   \n",
       "1                   0             1         0        0                2   \n",
       "2                   0             1         0        0                2   \n",
       "3                   0             1         0        0                2   \n",
       "4                   1             1         0        0                2   \n",
       "\n",
       "   other property of pain  Pain scores  pressing pain and rebounding pain  \\\n",
       "0                       2            7                                  1   \n",
       "1                       1            2                                  1   \n",
       "2                       1            1                                  0   \n",
       "3                       1            1                                  0   \n",
       "4                       1            1                                  0   \n",
       "\n",
       "   abdominal wall tensity  level  \n",
       "0                       1      1  \n",
       "1                       0      1  \n",
       "2                       0      1  \n",
       "3                       0      2  \n",
       "4                       0      2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reviewing dataset\n",
    "pd.set_option('display.max_columns',None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31dba0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3231 entries, 0 to 3230\n",
      "Data columns (total 33 columns):\n",
      " #   Column                             Non-Null Count  Dtype\n",
      "---  ------                             --------------  -----\n",
      " 0   id                                 3231 non-null   int64\n",
      " 1   sex                                3231 non-null   int64\n",
      " 2   age                                3231 non-null   int64\n",
      " 3   profession                         3231 non-null   int64\n",
      " 4   hypertension                       3231 non-null   int64\n",
      " 5   diabetes                           3231 non-null   int64\n",
      " 6   coronary heart disease             3231 non-null   int64\n",
      " 7   stroke                             3231 non-null   int64\n",
      " 8   others                             3231 non-null   int64\n",
      " 9   Treatment time period              3231 non-null   int64\n",
      " 10  Temperature                        3231 non-null   int64\n",
      " 11  Pulse                              3231 non-null   int64\n",
      " 12  RR                                 3231 non-null   int64\n",
      " 13  BP                                 3231 non-null   int64\n",
      " 14  SPO2                               3231 non-null   int64\n",
      " 15  anemic appearance                  3231 non-null   int64\n",
      " 16  complexion                         3231 non-null   int64\n",
      " 17  consciousness                      3231 non-null   int64\n",
      " 18  Way to enter wards                 3231 non-null   int64\n",
      " 19  borborygmus                        3231 non-null   int64\n",
      " 20  obvious cause                      3231 non-null   int64\n",
      " 21  pain position                      3231 non-null   int64\n",
      " 22  nausea and vomiting                3231 non-null   int64\n",
      " 23  diarrhea and fever                 3231 non-null   int64\n",
      " 24  haematemesis                       3231 non-null   int64\n",
      " 25  tenesmus                           3231 non-null   int64\n",
      " 26  syncope                            3231 non-null   int64\n",
      " 27  Persistent pain                    3231 non-null   int64\n",
      " 28  other property of pain             3231 non-null   int64\n",
      " 29  Pain scores                        3231 non-null   int64\n",
      " 30  pressing pain and rebounding pain  3231 non-null   int64\n",
      " 31  abdominal wall tensity             3231 non-null   int64\n",
      " 32  level                              3231 non-null   int64\n",
      "dtypes: int64(33)\n",
      "memory usage: 833.1 KB\n"
     ]
    }
   ],
   "source": [
    "#checking type of feaures\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b96e9a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3231, 33)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset has 600 rows and 35 columns\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40f48ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking care of categorical values\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "data['level']=le.fit_transform(data['level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4913335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining dependent and independent variables\n",
    "X = data.drop('level', axis=1)\n",
    "y = data['level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db2a621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data into training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c978a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0e7da44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout,Activation\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "input_size = 32\n",
    "output_size = 4\n",
    "hidden_layer_size = 50\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    #Input layer\n",
    "    tf.keras.layers.Dense(input_size),\n",
    "    \n",
    "    #Hidden layer 1\n",
    "    tf.keras.layers.Dense(hidden_layer_size,activation='relu'),\n",
    "    #Hidden layer 2\n",
    "    tf.keras.layers.Dense(hidden_layer_size,activation='relu'),\n",
    "    #Hidden layer 3\n",
    "    tf.keras.layers.Dense(hidden_layer_size,activation='relu'),\n",
    "    \n",
    "    #Output layer\n",
    "    tf.keras.layers.Dense(output_size,activation='softmax')\n",
    "    # return model\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55c3c1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e788e900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "26/26 - 2s - loss: 1.1043 - accuracy: 0.4926 - val_loss: 0.8380 - val_accuracy: 0.5840\n",
      "Epoch 2/50\n",
      "26/26 - 0s - loss: 0.7412 - accuracy: 0.5948 - val_loss: 0.6679 - val_accuracy: 0.6470\n",
      "Epoch 3/50\n",
      "26/26 - 0s - loss: 0.6569 - accuracy: 0.6316 - val_loss: 0.6272 - val_accuracy: 0.6660\n",
      "Epoch 4/50\n",
      "26/26 - 0s - loss: 0.6240 - accuracy: 0.6478 - val_loss: 0.6070 - val_accuracy: 0.6710\n",
      "Epoch 5/50\n",
      "26/26 - 0s - loss: 0.6003 - accuracy: 0.6641 - val_loss: 0.5818 - val_accuracy: 0.6780\n",
      "Epoch 6/50\n",
      "26/26 - 0s - loss: 0.5737 - accuracy: 0.6865 - val_loss: 0.5514 - val_accuracy: 0.7230\n",
      "Epoch 7/50\n",
      "26/26 - 0s - loss: 0.5551 - accuracy: 0.6908 - val_loss: 0.5327 - val_accuracy: 0.7180\n",
      "Epoch 8/50\n",
      "26/26 - 0s - loss: 0.5409 - accuracy: 0.7012 - val_loss: 0.5187 - val_accuracy: 0.7050\n",
      "Epoch 9/50\n",
      "26/26 - 0s - loss: 0.5176 - accuracy: 0.7128 - val_loss: 0.4985 - val_accuracy: 0.7320\n",
      "Epoch 10/50\n",
      "26/26 - 0s - loss: 0.5006 - accuracy: 0.7279 - val_loss: 0.4792 - val_accuracy: 0.7430\n",
      "Epoch 11/50\n",
      "26/26 - 0s - loss: 0.4860 - accuracy: 0.7353 - val_loss: 0.4592 - val_accuracy: 0.7700\n",
      "Epoch 12/50\n",
      "26/26 - 0s - loss: 0.4688 - accuracy: 0.7450 - val_loss: 0.4517 - val_accuracy: 0.7590\n",
      "Epoch 13/50\n",
      "26/26 - 0s - loss: 0.4545 - accuracy: 0.7632 - val_loss: 0.4261 - val_accuracy: 0.7950\n",
      "Epoch 14/50\n",
      "26/26 - 0s - loss: 0.4347 - accuracy: 0.7786 - val_loss: 0.4095 - val_accuracy: 0.8000\n",
      "Epoch 15/50\n",
      "26/26 - 0s - loss: 0.4201 - accuracy: 0.8046 - val_loss: 0.3877 - val_accuracy: 0.8310\n",
      "Epoch 16/50\n",
      "26/26 - 0s - loss: 0.3877 - accuracy: 0.8278 - val_loss: 0.3561 - val_accuracy: 0.8480\n",
      "Epoch 17/50\n",
      "26/26 - 0s - loss: 0.3656 - accuracy: 0.8402 - val_loss: 0.3305 - val_accuracy: 0.8630\n",
      "Epoch 18/50\n",
      "26/26 - 0s - loss: 0.3320 - accuracy: 0.8618 - val_loss: 0.3013 - val_accuracy: 0.8780\n",
      "Epoch 19/50\n",
      "26/26 - 0s - loss: 0.3090 - accuracy: 0.8750 - val_loss: 0.2802 - val_accuracy: 0.8910\n",
      "Epoch 20/50\n",
      "26/26 - 0s - loss: 0.2914 - accuracy: 0.8808 - val_loss: 0.2594 - val_accuracy: 0.9020\n",
      "Epoch 21/50\n",
      "26/26 - 0s - loss: 0.2693 - accuracy: 0.8974 - val_loss: 0.2466 - val_accuracy: 0.8980\n",
      "Epoch 22/50\n",
      "26/26 - 0s - loss: 0.2491 - accuracy: 0.9052 - val_loss: 0.2294 - val_accuracy: 0.9230\n",
      "Epoch 23/50\n",
      "26/26 - 0s - loss: 0.2340 - accuracy: 0.9106 - val_loss: 0.2223 - val_accuracy: 0.9120\n",
      "Epoch 24/50\n",
      "26/26 - 0s - loss: 0.2232 - accuracy: 0.9187 - val_loss: 0.1975 - val_accuracy: 0.9320\n",
      "Epoch 25/50\n",
      "26/26 - 0s - loss: 0.2072 - accuracy: 0.9257 - val_loss: 0.1873 - val_accuracy: 0.9420\n",
      "Epoch 26/50\n",
      "26/26 - 0s - loss: 0.1944 - accuracy: 0.9392 - val_loss: 0.1735 - val_accuracy: 0.9440\n",
      "Epoch 27/50\n",
      "26/26 - 0s - loss: 0.1809 - accuracy: 0.9396 - val_loss: 0.1664 - val_accuracy: 0.9460\n",
      "Epoch 28/50\n",
      "26/26 - 0s - loss: 0.1742 - accuracy: 0.9404 - val_loss: 0.1530 - val_accuracy: 0.9480\n",
      "Epoch 29/50\n",
      "26/26 - 0s - loss: 0.1662 - accuracy: 0.9443 - val_loss: 0.1438 - val_accuracy: 0.9550\n",
      "Epoch 30/50\n",
      "26/26 - 0s - loss: 0.1561 - accuracy: 0.9493 - val_loss: 0.1372 - val_accuracy: 0.9570\n",
      "Epoch 31/50\n",
      "26/26 - 0s - loss: 0.1463 - accuracy: 0.9543 - val_loss: 0.1294 - val_accuracy: 0.9650\n",
      "Epoch 32/50\n",
      "26/26 - 0s - loss: 0.1400 - accuracy: 0.9543 - val_loss: 0.1289 - val_accuracy: 0.9600\n",
      "Epoch 33/50\n",
      "26/26 - 0s - loss: 0.1424 - accuracy: 0.9532 - val_loss: 0.1245 - val_accuracy: 0.9620\n",
      "Epoch 34/50\n",
      "26/26 - 0s - loss: 0.1301 - accuracy: 0.9625 - val_loss: 0.1076 - val_accuracy: 0.9680\n",
      "Epoch 35/50\n",
      "26/26 - 0s - loss: 0.1224 - accuracy: 0.9628 - val_loss: 0.1028 - val_accuracy: 0.9700\n",
      "Epoch 36/50\n",
      "26/26 - 0s - loss: 0.1169 - accuracy: 0.9659 - val_loss: 0.0963 - val_accuracy: 0.9770\n",
      "Epoch 37/50\n",
      "26/26 - 0s - loss: 0.1093 - accuracy: 0.9698 - val_loss: 0.0909 - val_accuracy: 0.9760\n",
      "Epoch 38/50\n",
      "26/26 - 0s - loss: 0.1139 - accuracy: 0.9675 - val_loss: 0.0843 - val_accuracy: 0.9840\n",
      "Epoch 39/50\n",
      "26/26 - 0s - loss: 0.1011 - accuracy: 0.9725 - val_loss: 0.0921 - val_accuracy: 0.9800\n",
      "Epoch 40/50\n",
      "26/26 - 0s - loss: 0.1013 - accuracy: 0.9737 - val_loss: 0.0813 - val_accuracy: 0.9800\n",
      "Epoch 41/50\n",
      "26/26 - 0s - loss: 0.0978 - accuracy: 0.9729 - val_loss: 0.0874 - val_accuracy: 0.9720\n",
      "Epoch 42/50\n",
      "26/26 - 0s - loss: 0.0986 - accuracy: 0.9706 - val_loss: 0.0803 - val_accuracy: 0.9770\n",
      "Epoch 43/50\n",
      "26/26 - 0s - loss: 0.0901 - accuracy: 0.9760 - val_loss: 0.0703 - val_accuracy: 0.9810\n",
      "Epoch 44/50\n",
      "26/26 - 0s - loss: 0.0907 - accuracy: 0.9717 - val_loss: 0.0678 - val_accuracy: 0.9870\n",
      "Epoch 45/50\n",
      "26/26 - 0s - loss: 0.0810 - accuracy: 0.9814 - val_loss: 0.0686 - val_accuracy: 0.9800\n",
      "Epoch 46/50\n",
      "26/26 - 0s - loss: 0.0796 - accuracy: 0.9776 - val_loss: 0.0676 - val_accuracy: 0.9800\n",
      "Epoch 47/50\n",
      "26/26 - 0s - loss: 0.0806 - accuracy: 0.9776 - val_loss: 0.0659 - val_accuracy: 0.9850\n",
      "Epoch 48/50\n",
      "26/26 - 0s - loss: 0.0785 - accuracy: 0.9803 - val_loss: 0.0714 - val_accuracy: 0.9810\n",
      "Epoch 49/50\n",
      "26/26 - 0s - loss: 0.0765 - accuracy: 0.9807 - val_loss: 0.0546 - val_accuracy: 0.9900\n",
      "Epoch 50/50\n",
      "26/26 - 0s - loss: 0.0716 - accuracy: 0.9791 - val_loss: 0.0544 - val_accuracy: 0.9910\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "early_stopping=tf.keras.callbacks.EarlyStopping(patience=20)\n",
    "\n",
    "results = model.fit(X_train, y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=NUM_EPOCHS,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(X_train, y_train),\n",
    "          verbose=2,validation_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86816861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.104308</td>\n",
       "      <td>0.492647</td>\n",
       "      <td>0.838029</td>\n",
       "      <td>0.584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.741227</td>\n",
       "      <td>0.594814</td>\n",
       "      <td>0.667893</td>\n",
       "      <td>0.647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.656870</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.627226</td>\n",
       "      <td>0.666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.624005</td>\n",
       "      <td>0.647833</td>\n",
       "      <td>0.607036</td>\n",
       "      <td>0.671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.600348</td>\n",
       "      <td>0.664087</td>\n",
       "      <td>0.581792</td>\n",
       "      <td>0.678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.573750</td>\n",
       "      <td>0.686532</td>\n",
       "      <td>0.551416</td>\n",
       "      <td>0.723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.555088</td>\n",
       "      <td>0.690789</td>\n",
       "      <td>0.532724</td>\n",
       "      <td>0.718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.540938</td>\n",
       "      <td>0.701238</td>\n",
       "      <td>0.518656</td>\n",
       "      <td>0.705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.517620</td>\n",
       "      <td>0.712848</td>\n",
       "      <td>0.498549</td>\n",
       "      <td>0.732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.500553</td>\n",
       "      <td>0.727941</td>\n",
       "      <td>0.479243</td>\n",
       "      <td>0.743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.486023</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.459170</td>\n",
       "      <td>0.770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.468813</td>\n",
       "      <td>0.744969</td>\n",
       "      <td>0.451663</td>\n",
       "      <td>0.759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.454513</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.426069</td>\n",
       "      <td>0.795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.434726</td>\n",
       "      <td>0.778638</td>\n",
       "      <td>0.409475</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.420125</td>\n",
       "      <td>0.804567</td>\n",
       "      <td>0.387739</td>\n",
       "      <td>0.831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.387683</td>\n",
       "      <td>0.827786</td>\n",
       "      <td>0.356101</td>\n",
       "      <td>0.848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.365568</td>\n",
       "      <td>0.840170</td>\n",
       "      <td>0.330467</td>\n",
       "      <td>0.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.332004</td>\n",
       "      <td>0.861842</td>\n",
       "      <td>0.301321</td>\n",
       "      <td>0.878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.308996</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.280216</td>\n",
       "      <td>0.891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.291428</td>\n",
       "      <td>0.880805</td>\n",
       "      <td>0.259430</td>\n",
       "      <td>0.902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.269279</td>\n",
       "      <td>0.897446</td>\n",
       "      <td>0.246634</td>\n",
       "      <td>0.898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.249135</td>\n",
       "      <td>0.905186</td>\n",
       "      <td>0.229352</td>\n",
       "      <td>0.923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.234004</td>\n",
       "      <td>0.910604</td>\n",
       "      <td>0.222340</td>\n",
       "      <td>0.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.223169</td>\n",
       "      <td>0.918731</td>\n",
       "      <td>0.197532</td>\n",
       "      <td>0.932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.207182</td>\n",
       "      <td>0.925697</td>\n",
       "      <td>0.187273</td>\n",
       "      <td>0.942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.194398</td>\n",
       "      <td>0.939241</td>\n",
       "      <td>0.173464</td>\n",
       "      <td>0.944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.180902</td>\n",
       "      <td>0.939628</td>\n",
       "      <td>0.166361</td>\n",
       "      <td>0.946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.174225</td>\n",
       "      <td>0.940402</td>\n",
       "      <td>0.153013</td>\n",
       "      <td>0.948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.166228</td>\n",
       "      <td>0.944272</td>\n",
       "      <td>0.143779</td>\n",
       "      <td>0.955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.156064</td>\n",
       "      <td>0.949303</td>\n",
       "      <td>0.137199</td>\n",
       "      <td>0.957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.146259</td>\n",
       "      <td>0.954334</td>\n",
       "      <td>0.129414</td>\n",
       "      <td>0.965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.140035</td>\n",
       "      <td>0.954334</td>\n",
       "      <td>0.128932</td>\n",
       "      <td>0.960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.142396</td>\n",
       "      <td>0.953173</td>\n",
       "      <td>0.124484</td>\n",
       "      <td>0.962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.130145</td>\n",
       "      <td>0.962461</td>\n",
       "      <td>0.107608</td>\n",
       "      <td>0.968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.122395</td>\n",
       "      <td>0.962848</td>\n",
       "      <td>0.102843</td>\n",
       "      <td>0.970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.116860</td>\n",
       "      <td>0.965944</td>\n",
       "      <td>0.096349</td>\n",
       "      <td>0.977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.109349</td>\n",
       "      <td>0.969814</td>\n",
       "      <td>0.090867</td>\n",
       "      <td>0.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.113881</td>\n",
       "      <td>0.967492</td>\n",
       "      <td>0.084312</td>\n",
       "      <td>0.984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.101099</td>\n",
       "      <td>0.972523</td>\n",
       "      <td>0.092102</td>\n",
       "      <td>0.980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.101339</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.081348</td>\n",
       "      <td>0.980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.097778</td>\n",
       "      <td>0.972910</td>\n",
       "      <td>0.087386</td>\n",
       "      <td>0.972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.098640</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.080286</td>\n",
       "      <td>0.977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.090080</td>\n",
       "      <td>0.976006</td>\n",
       "      <td>0.070331</td>\n",
       "      <td>0.981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.090691</td>\n",
       "      <td>0.971749</td>\n",
       "      <td>0.067848</td>\n",
       "      <td>0.987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.080991</td>\n",
       "      <td>0.981424</td>\n",
       "      <td>0.068641</td>\n",
       "      <td>0.980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.079602</td>\n",
       "      <td>0.977554</td>\n",
       "      <td>0.067604</td>\n",
       "      <td>0.980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.080616</td>\n",
       "      <td>0.977554</td>\n",
       "      <td>0.065943</td>\n",
       "      <td>0.985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.078488</td>\n",
       "      <td>0.980263</td>\n",
       "      <td>0.071370</td>\n",
       "      <td>0.981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.076517</td>\n",
       "      <td>0.980650</td>\n",
       "      <td>0.054632</td>\n",
       "      <td>0.990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.071643</td>\n",
       "      <td>0.979102</td>\n",
       "      <td>0.054369</td>\n",
       "      <td>0.991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  accuracy  val_loss  val_accuracy\n",
       "0   1.104308  0.492647  0.838029         0.584\n",
       "1   0.741227  0.594814  0.667893         0.647\n",
       "2   0.656870  0.631579  0.627226         0.666\n",
       "3   0.624005  0.647833  0.607036         0.671\n",
       "4   0.600348  0.664087  0.581792         0.678\n",
       "5   0.573750  0.686532  0.551416         0.723\n",
       "6   0.555088  0.690789  0.532724         0.718\n",
       "7   0.540938  0.701238  0.518656         0.705\n",
       "8   0.517620  0.712848  0.498549         0.732\n",
       "9   0.500553  0.727941  0.479243         0.743\n",
       "10  0.486023  0.735294  0.459170         0.770\n",
       "11  0.468813  0.744969  0.451663         0.759\n",
       "12  0.454513  0.763158  0.426069         0.795\n",
       "13  0.434726  0.778638  0.409475         0.800\n",
       "14  0.420125  0.804567  0.387739         0.831\n",
       "15  0.387683  0.827786  0.356101         0.848\n",
       "16  0.365568  0.840170  0.330467         0.863\n",
       "17  0.332004  0.861842  0.301321         0.878\n",
       "18  0.308996  0.875000  0.280216         0.891\n",
       "19  0.291428  0.880805  0.259430         0.902\n",
       "20  0.269279  0.897446  0.246634         0.898\n",
       "21  0.249135  0.905186  0.229352         0.923\n",
       "22  0.234004  0.910604  0.222340         0.912\n",
       "23  0.223169  0.918731  0.197532         0.932\n",
       "24  0.207182  0.925697  0.187273         0.942\n",
       "25  0.194398  0.939241  0.173464         0.944\n",
       "26  0.180902  0.939628  0.166361         0.946\n",
       "27  0.174225  0.940402  0.153013         0.948\n",
       "28  0.166228  0.944272  0.143779         0.955\n",
       "29  0.156064  0.949303  0.137199         0.957\n",
       "30  0.146259  0.954334  0.129414         0.965\n",
       "31  0.140035  0.954334  0.128932         0.960\n",
       "32  0.142396  0.953173  0.124484         0.962\n",
       "33  0.130145  0.962461  0.107608         0.968\n",
       "34  0.122395  0.962848  0.102843         0.970\n",
       "35  0.116860  0.965944  0.096349         0.977\n",
       "36  0.109349  0.969814  0.090867         0.976\n",
       "37  0.113881  0.967492  0.084312         0.984\n",
       "38  0.101099  0.972523  0.092102         0.980\n",
       "39  0.101339  0.973684  0.081348         0.980\n",
       "40  0.097778  0.972910  0.087386         0.972\n",
       "41  0.098640  0.970588  0.080286         0.977\n",
       "42  0.090080  0.976006  0.070331         0.981\n",
       "43  0.090691  0.971749  0.067848         0.987\n",
       "44  0.080991  0.981424  0.068641         0.980\n",
       "45  0.079602  0.977554  0.067604         0.980\n",
       "46  0.080616  0.977554  0.065943         0.985\n",
       "47  0.078488  0.980263  0.071370         0.981\n",
       "48  0.076517  0.980650  0.054632         0.990\n",
       "49  0.071643  0.979102  0.054369         0.991"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a13dd180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(results.history)[['accuracy', 'val_accuracy','loss', 'val_loss']].plot()\n",
    "plt.savefig('C:\\\\Users\\\\gold\\\\Desktop\\\\models\\\\experiment2\\\\thesisfigures\\\\Learning Curve1.png',dpi = 1000)\n",
    "model.save('my_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bf481ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 798us/step - loss: 0.2685 - accuracy: 0.9212\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2684568464756012, 0.9211746454238892]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b981a3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C:\\Users\\gold\\Desktop\\models\\datasets\n",
    "# epoch = [1:600]\n",
    "save = pd.DataFrame(results.history,columns = ['accuracy', 'val_accuracy','loss', 'val_loss'])\n",
    "# save.to_csv('C:\\\\Users\\\\gold\\\\Desktop\\\\models\\\\results.csv',index = False,header = False)\n",
    "save.to_csv('C:\\\\Users\\\\gold\\\\Desktop\\\\models\\\\experiment2\\\\thesisfigures\\\\results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38638ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import csv  # 导入csv模块\n",
    "import matplotlib.pyplot as plt\n",
    "path = 'C:\\\\Users\\\\gold\\\\Desktop\\\\models\\\\results.csv'\n",
    "data = pd.read_csv(path) #读取文件中所有数据\n",
    "# epoch = data[['epoch']]\n",
    "\n",
    "accuracy = data[['accuracy']]\n",
    "val_accuracy = data[['val_accuracy']]\n",
    "loss = data[['loss']]\n",
    "val_loss = data[['val_loss']]\n",
    "\n",
    "#画折线图\n",
    "plt.plot(accuracy,label='accuracy',color='black',linestyle='-')\n",
    "plt.plot(val_accuracy,label='val_accuracy',color='black',linestyle='--')\n",
    "plt.plot(loss,label='loss',color='black',linestyle='-.')\n",
    "plt.plot(val_loss,label='val_loss',color='black',linestyle=':')\n",
    "\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('performance')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend()\n",
    "plt.savefig('C:\\\\Users\\\\gold\\\\Desktop\\\\models\\\\experiment2\\\\thesisfigures\\\\Learning Curve Image.png',dpi = 1000)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87886538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9211746522411128\n",
      "f1_weighted: 0.9212192557633172\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "# y_pred=model.predict_classes(X_test)\n",
    "y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# %%Accuracy\n",
    "\n",
    "# print(\"Accuracy:\",metrics.accuracy_score(np.argmax(y_test, axis = 0),y_pred))\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "# %%f1 score\n",
    "\n",
    "# print(\"f1_weighted:\",metrics.f1_score(np.argmax(y_test, axis=1), y_pred,average='weighted'))\n",
    "print(\"f1_weighted:\",metrics.f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb190d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 27 candidates, totalling 270 fits\n",
      "Epoch 1/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 1.2705 - accuracy: 0.3814\n",
      "Epoch 2/150\n",
      "81/81 [==============================] - 0s 997us/step - loss: 0.6962 - accuracy: 0.5382\n",
      "Epoch 3/150\n",
      "81/81 [==============================] - 0s 985us/step - loss: 0.6955 - accuracy: 0.5356\n",
      "Epoch 4/150\n",
      "81/81 [==============================] - 0s 993us/step - loss: 0.6893 - accuracy: 0.5467\n",
      "Epoch 5/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.6752 - accuracy: 0.5718\n",
      "Epoch 6/150\n",
      "81/81 [==============================] - 0s 961us/step - loss: 0.6364 - accuracy: 0.6152\n",
      "Epoch 7/150\n",
      "81/81 [==============================] - 0s 985us/step - loss: 0.6102 - accuracy: 0.6419\n",
      "Epoch 8/150\n",
      "81/81 [==============================] - 0s 974us/step - loss: 0.5908 - accuracy: 0.6719\n",
      "Epoch 9/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.5631 - accuracy: 0.7096\n",
      "Epoch 10/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.5459 - accuracy: 0.7011\n",
      "Epoch 11/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.5423 - accuracy: 0.7082\n",
      "Epoch 12/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.5067 - accuracy: 0.7419\n",
      "Epoch 13/150\n",
      "81/81 [==============================] - 0s 910us/step - loss: 0.4972 - accuracy: 0.7431\n",
      "Epoch 14/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.4765 - accuracy: 0.7569\n",
      "Epoch 15/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.4500 - accuracy: 0.7904\n",
      "Epoch 16/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.4344 - accuracy: 0.8028\n",
      "Epoch 17/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.3962 - accuracy: 0.8236\n",
      "Epoch 18/150\n",
      "81/81 [==============================] - 0s 985us/step - loss: 0.3728 - accuracy: 0.8484\n",
      "Epoch 19/150\n",
      "81/81 [==============================] - 0s 985us/step - loss: 0.3554 - accuracy: 0.8493\n",
      "Epoch 20/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.3708 - accuracy: 0.8403\n",
      "Epoch 21/150\n",
      "81/81 [==============================] - 0s 992us/step - loss: 0.3481 - accuracy: 0.8464\n",
      "Epoch 22/150\n",
      "81/81 [==============================] - 0s 997us/step - loss: 0.3267 - accuracy: 0.8675\n",
      "Epoch 23/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.3241 - accuracy: 0.8567\n",
      "Epoch 24/150\n",
      "81/81 [==============================] - 0s 998us/step - loss: 0.3074 - accuracy: 0.8720\n",
      "Epoch 25/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.3002 - accuracy: 0.8705\n",
      "Epoch 26/150\n",
      "81/81 [==============================] - 0s 998us/step - loss: 0.2816 - accuracy: 0.8767\n",
      "Epoch 27/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.2826 - accuracy: 0.8813\n",
      "Epoch 28/150\n",
      "81/81 [==============================] - 0s 985us/step - loss: 0.2857 - accuracy: 0.8764\n",
      "Epoch 29/150\n",
      "81/81 [==============================] - 0s 972us/step - loss: 0.2590 - accuracy: 0.8965\n",
      "Epoch 30/150\n",
      "81/81 [==============================] - 0s 973us/step - loss: 0.2428 - accuracy: 0.8972\n",
      "Epoch 31/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.2397 - accuracy: 0.8963\n",
      "Epoch 32/150\n",
      "81/81 [==============================] - 0s 997us/step - loss: 0.2105 - accuracy: 0.9144\n",
      "Epoch 33/150\n",
      "81/81 [==============================] - 0s 972us/step - loss: 0.2171 - accuracy: 0.8948\n",
      "Epoch 34/150\n",
      "81/81 [==============================] - 0s 997us/step - loss: 0.2254 - accuracy: 0.9072\n",
      "Epoch 35/150\n",
      "81/81 [==============================] - 0s 998us/step - loss: 0.2045 - accuracy: 0.9143\n",
      "Epoch 36/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.2304 - accuracy: 0.9036\n",
      "Epoch 37/150\n",
      "81/81 [==============================] - 0s 997us/step - loss: 0.1977 - accuracy: 0.9130\n",
      "Epoch 38/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.2031 - accuracy: 0.9178\n",
      "Epoch 39/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.1824 - accuracy: 0.9235\n",
      "Epoch 40/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.1676 - accuracy: 0.9340\n",
      "Epoch 41/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.1840 - accuracy: 0.9172\n",
      "Epoch 42/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.1732 - accuracy: 0.9225\n",
      "Epoch 43/150\n",
      "81/81 [==============================] - 0s 972us/step - loss: 0.1521 - accuracy: 0.9355\n",
      "Epoch 44/150\n",
      "81/81 [==============================] - 0s 980us/step - loss: 0.1560 - accuracy: 0.9421\n",
      "Epoch 45/150\n",
      "81/81 [==============================] - 0s 993us/step - loss: 0.1502 - accuracy: 0.9481\n",
      "Epoch 46/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.1597 - accuracy: 0.9310\n",
      "Epoch 47/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.1389 - accuracy: 0.9468\n",
      "Epoch 48/150\n",
      "81/81 [==============================] - 0s 997us/step - loss: 0.1355 - accuracy: 0.9532\n",
      "Epoch 49/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.1465 - accuracy: 0.9423\n",
      "Epoch 50/150\n",
      "81/81 [==============================] - 0s 997us/step - loss: 0.1623 - accuracy: 0.9366\n",
      "Epoch 51/150\n",
      "81/81 [==============================] - 0s 985us/step - loss: 0.1423 - accuracy: 0.9433\n",
      "Epoch 52/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.1622 - accuracy: 0.9386\n",
      "Epoch 53/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.1146 - accuracy: 0.9562\n",
      "Epoch 54/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.1003 - accuracy: 0.9600\n",
      "Epoch 55/150\n",
      "81/81 [==============================] - 0s 998us/step - loss: 0.0986 - accuracy: 0.9622\n",
      "Epoch 56/150\n",
      "81/81 [==============================] - 0s 998us/step - loss: 0.0881 - accuracy: 0.9692\n",
      "Epoch 57/150\n",
      "81/81 [==============================] - 0s 985us/step - loss: 0.0895 - accuracy: 0.9690\n",
      "Epoch 58/150\n",
      "81/81 [==============================] - 0s 972us/step - loss: 0.0830 - accuracy: 0.9673\n",
      "Epoch 59/150\n",
      "81/81 [==============================] - 0s 970us/step - loss: 0.0798 - accuracy: 0.9711\n",
      "Epoch 60/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0906 - accuracy: 0.9743\n",
      "Epoch 61/150\n",
      "81/81 [==============================] - 0s 997us/step - loss: 0.0770 - accuracy: 0.9749\n",
      "Epoch 62/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0743 - accuracy: 0.9715\n",
      "Epoch 63/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0693 - accuracy: 0.9733\n",
      "Epoch 64/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0702 - accuracy: 0.9768\n",
      "Epoch 65/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0633 - accuracy: 0.9778\n",
      "Epoch 66/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0713 - accuracy: 0.9745\n",
      "Epoch 67/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0682 - accuracy: 0.9769\n",
      "Epoch 68/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0548 - accuracy: 0.9864\n",
      "Epoch 69/150\n",
      "81/81 [==============================] - 0s 973us/step - loss: 0.0557 - accuracy: 0.9811\n",
      "Epoch 70/150\n",
      "81/81 [==============================] - 0s 985us/step - loss: 0.0498 - accuracy: 0.9844\n",
      "Epoch 71/150\n",
      "81/81 [==============================] - 0s 997us/step - loss: 0.0783 - accuracy: 0.9748\n",
      "Epoch 72/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.1181 - accuracy: 0.9650\n",
      "Epoch 73/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0756 - accuracy: 0.9712\n",
      "Epoch 74/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0649 - accuracy: 0.9746\n",
      "Epoch 75/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0584 - accuracy: 0.9794\n",
      "Epoch 76/150\n",
      "81/81 [==============================] - 0s 983us/step - loss: 0.0534 - accuracy: 0.9822\n",
      "Epoch 77/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0528 - accuracy: 0.9824\n",
      "Epoch 78/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0512 - accuracy: 0.9826\n",
      "Epoch 79/150\n",
      "81/81 [==============================] - 0s 996us/step - loss: 0.0656 - accuracy: 0.9743\n",
      "Epoch 80/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0487 - accuracy: 0.9825\n",
      "Epoch 81/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0348 - accuracy: 0.9899\n",
      "Epoch 82/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0366 - accuracy: 0.9883\n",
      "Epoch 83/150\n",
      "81/81 [==============================] - 0s 997us/step - loss: 0.0691 - accuracy: 0.9771\n",
      "Epoch 84/150\n",
      "81/81 [==============================] - 0s 985us/step - loss: 0.0483 - accuracy: 0.9846\n",
      "Epoch 85/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0380 - accuracy: 0.9848\n",
      "Epoch 86/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0569 - accuracy: 0.9844\n",
      "Epoch 87/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0737 - accuracy: 0.9750\n",
      "Epoch 88/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0631 - accuracy: 0.9758\n",
      "Epoch 89/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0732 - accuracy: 0.9710\n",
      "Epoch 90/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0526 - accuracy: 0.9794\n",
      "Epoch 91/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0509 - accuracy: 0.9793\n",
      "Epoch 92/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0247 - accuracy: 0.9931\n",
      "Epoch 93/150\n",
      "81/81 [==============================] - 0s 960us/step - loss: 0.0233 - accuracy: 0.9950\n",
      "Epoch 94/150\n",
      "81/81 [==============================] - 0s 984us/step - loss: 0.0211 - accuracy: 0.9950\n",
      "Epoch 95/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0199 - accuracy: 0.9952\n",
      "Epoch 96/150\n",
      "81/81 [==============================] - 0s 955us/step - loss: 0.0201 - accuracy: 0.9956\n",
      "Epoch 97/150\n",
      "81/81 [==============================] - 0s 969us/step - loss: 0.0236 - accuracy: 0.9950\n",
      "Epoch 98/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0298 - accuracy: 0.9888\n",
      "Epoch 99/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0326 - accuracy: 0.9870\n",
      "Epoch 100/150\n",
      "81/81 [==============================] - 0s 960us/step - loss: 0.0425 - accuracy: 0.9858\n",
      "Epoch 101/150\n",
      "81/81 [==============================] - 0s 948us/step - loss: 0.0415 - accuracy: 0.9881\n",
      "Epoch 102/150\n",
      "81/81 [==============================] - 0s 972us/step - loss: 0.0286 - accuracy: 0.9881\n",
      "Epoch 103/150\n",
      "81/81 [==============================] - 0s 947us/step - loss: 0.0315 - accuracy: 0.9909\n",
      "Epoch 104/150\n",
      "81/81 [==============================] - 0s 935us/step - loss: 0.0281 - accuracy: 0.9927\n",
      "Epoch 105/150\n",
      "81/81 [==============================] - 0s 948us/step - loss: 0.0311 - accuracy: 0.9906\n",
      "Epoch 106/150\n",
      "81/81 [==============================] - 0s 985us/step - loss: 0.0350 - accuracy: 0.9877\n",
      "Epoch 107/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0306 - accuracy: 0.9900\n",
      "Epoch 108/150\n",
      "81/81 [==============================] - 0s 973us/step - loss: 0.0247 - accuracy: 0.9938\n",
      "Epoch 109/150\n",
      "81/81 [==============================] - 0s 973us/step - loss: 0.0197 - accuracy: 0.9936\n",
      "Epoch 110/150\n",
      "81/81 [==============================] - 0s 935us/step - loss: 0.0216 - accuracy: 0.9938\n",
      "Epoch 111/150\n",
      "81/81 [==============================] - 0s 960us/step - loss: 0.0311 - accuracy: 0.9866\n",
      "Epoch 112/150\n",
      "81/81 [==============================] - 0s 947us/step - loss: 0.0393 - accuracy: 0.9843\n",
      "Epoch 113/150\n",
      "81/81 [==============================] - 0s 935us/step - loss: 0.0494 - accuracy: 0.9811\n",
      "Epoch 114/150\n",
      "81/81 [==============================] - 0s 960us/step - loss: 0.0160 - accuracy: 0.9944\n",
      "Epoch 115/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0248 - accuracy: 0.9914\n",
      "Epoch 116/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0430 - accuracy: 0.9875\n",
      "Epoch 117/150\n",
      "81/81 [==============================] - 0s 998us/step - loss: 0.1649 - accuracy: 0.9632\n",
      "Epoch 118/150\n",
      "81/81 [==============================] - 0s 947us/step - loss: 0.0768 - accuracy: 0.9792\n",
      "Epoch 119/150\n",
      "81/81 [==============================] - 0s 935us/step - loss: 0.0329 - accuracy: 0.9924\n",
      "Epoch 120/150\n",
      "81/81 [==============================] - 0s 948us/step - loss: 0.0176 - accuracy: 0.9965\n",
      "Epoch 121/150\n",
      "81/81 [==============================] - 0s 997us/step - loss: 0.0138 - accuracy: 0.9962\n",
      "Epoch 122/150\n",
      "81/81 [==============================] - 0s 972us/step - loss: 0.0111 - accuracy: 0.9989\n",
      "Epoch 123/150\n",
      "81/81 [==============================] - 0s 948us/step - loss: 0.0186 - accuracy: 0.9936\n",
      "Epoch 124/150\n",
      "81/81 [==============================] - 0s 935us/step - loss: 0.0165 - accuracy: 0.9962\n",
      "Epoch 125/150\n",
      "81/81 [==============================] - 0s 984us/step - loss: 0.0199 - accuracy: 0.9936\n",
      "Epoch 126/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0218 - accuracy: 0.9929\n",
      "Epoch 127/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0342 - accuracy: 0.9856\n",
      "Epoch 128/150\n",
      "81/81 [==============================] - 0s 960us/step - loss: 0.0196 - accuracy: 0.9934\n",
      "Epoch 129/150\n",
      "81/81 [==============================] - 0s 956us/step - loss: 0.0179 - accuracy: 0.9945\n",
      "Epoch 130/150\n",
      "81/81 [==============================] - 0s 959us/step - loss: 0.0142 - accuracy: 0.9977\n",
      "Epoch 131/150\n",
      "81/81 [==============================] - 0s 947us/step - loss: 0.0096 - accuracy: 0.9987\n",
      "Epoch 132/150\n",
      "81/81 [==============================] - 0s 967us/step - loss: 0.0120 - accuracy: 0.9978\n",
      "Epoch 133/150\n",
      "81/81 [==============================] - 0s 946us/step - loss: 0.0179 - accuracy: 0.9964\n",
      "Epoch 134/150\n",
      "81/81 [==============================] - 0s 973us/step - loss: 0.0236 - accuracy: 0.9893\n",
      "Epoch 135/150\n",
      "81/81 [==============================] - 0s 935us/step - loss: 0.0290 - accuracy: 0.9892\n",
      "Epoch 136/150\n",
      "81/81 [==============================] - 0s 960us/step - loss: 0.0210 - accuracy: 0.9963\n",
      "Epoch 137/150\n",
      "81/81 [==============================] - 0s 973us/step - loss: 0.0263 - accuracy: 0.9909\n",
      "Epoch 138/150\n",
      "81/81 [==============================] - 0s 947us/step - loss: 0.0206 - accuracy: 0.9926\n",
      "Epoch 139/150\n",
      "81/81 [==============================] - 0s 947us/step - loss: 0.0133 - accuracy: 0.9972\n",
      "Epoch 140/150\n",
      "81/81 [==============================] - 0s 960us/step - loss: 0.0136 - accuracy: 0.9965\n",
      "Epoch 141/150\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.0201 - accuracy: 0.9948\n",
      "Epoch 142/150\n",
      "81/81 [==============================] - 0s 997us/step - loss: 0.0346 - accuracy: 0.9872\n",
      "Epoch 143/150\n",
      "81/81 [==============================] - 0s 936us/step - loss: 0.0762 - accuracy: 0.9766\n",
      "Epoch 144/150\n",
      "81/81 [==============================] - 0s 910us/step - loss: 0.0671 - accuracy: 0.9787\n",
      "Epoch 145/150\n",
      "81/81 [==============================] - 0s 960us/step - loss: 0.0267 - accuracy: 0.9874\n",
      "Epoch 146/150\n",
      "81/81 [==============================] - 0s 959us/step - loss: 0.0191 - accuracy: 0.9947\n",
      "Epoch 147/150\n",
      "81/81 [==============================] - 0s 972us/step - loss: 0.0122 - accuracy: 0.9972\n",
      "Epoch 148/150\n",
      "81/81 [==============================] - 0s 936us/step - loss: 0.0127 - accuracy: 0.9966\n",
      "Epoch 149/150\n",
      "81/81 [==============================] - 0s 947us/step - loss: 0.0211 - accuracy: 0.9927\n",
      "Epoch 150/150\n",
      "81/81 [==============================] - 0s 959us/step - loss: 0.0136 - accuracy: 0.9966\n",
      "0.856805 (0.038843) with: {'batch_size': 32, 'epochs': 50, 'optimizer': 'RMSprop'}\n",
      "0.876158 (0.030422) with: {'batch_size': 32, 'epochs': 50, 'optimizer': 'Adam'}\n",
      "0.445672 (0.095140) with: {'batch_size': 32, 'epochs': 50, 'optimizer': 'SGD'}\n",
      "0.895141 (0.029719) with: {'batch_size': 32, 'epochs': 100, 'optimizer': 'RMSprop'}\n",
      "0.898222 (0.041873) with: {'batch_size': 32, 'epochs': 100, 'optimizer': 'Adam'}\n",
      "0.551871 (0.025446) with: {'batch_size': 32, 'epochs': 100, 'optimizer': 'SGD'}\n",
      "0.893963 (0.024900) with: {'batch_size': 32, 'epochs': 150, 'optimizer': 'RMSprop'}\n",
      "0.905196 (0.016573) with: {'batch_size': 32, 'epochs': 150, 'optimizer': 'Adam'}\n",
      "0.605651 (0.036093) with: {'batch_size': 32, 'epochs': 150, 'optimizer': 'SGD'}\n",
      "0.843237 (0.057237) with: {'batch_size': 50, 'epochs': 50, 'optimizer': 'RMSprop'}\n",
      "0.853716 (0.027578) with: {'batch_size': 50, 'epochs': 50, 'optimizer': 'Adam'}\n",
      "0.320071 (0.025089) with: {'batch_size': 50, 'epochs': 50, 'optimizer': 'SGD'}\n",
      "0.885797 (0.030040) with: {'batch_size': 50, 'epochs': 100, 'optimizer': 'RMSprop'}\n",
      "0.870372 (0.032932) with: {'batch_size': 50, 'epochs': 100, 'optimizer': 'Adam'}\n",
      "0.547218 (0.037927) with: {'batch_size': 50, 'epochs': 100, 'optimizer': 'SGD'}\n",
      "0.896305 (0.024707) with: {'batch_size': 50, 'epochs': 150, 'optimizer': 'RMSprop'}\n",
      "0.894369 (0.026719) with: {'batch_size': 50, 'epochs': 150, 'optimizer': 'Adam'}\n",
      "0.567352 (0.025312) with: {'batch_size': 50, 'epochs': 150, 'optimizer': 'SGD'}\n",
      "0.797573 (0.046783) with: {'batch_size': 100, 'epochs': 50, 'optimizer': 'RMSprop'}\n",
      "0.791021 (0.049327) with: {'batch_size': 100, 'epochs': 50, 'optimizer': 'Adam'}\n",
      "0.320071 (0.025089) with: {'batch_size': 100, 'epochs': 50, 'optimizer': 'SGD'}\n",
      "0.848686 (0.056826) with: {'batch_size': 100, 'epochs': 100, 'optimizer': 'RMSprop'}\n",
      "0.871889 (0.036685) with: {'batch_size': 100, 'epochs': 100, 'optimizer': 'Adam'}\n",
      "0.320071 (0.025089) with: {'batch_size': 100, 'epochs': 100, 'optimizer': 'SGD'}\n",
      "0.858355 (0.034671) with: {'batch_size': 100, 'epochs': 150, 'optimizer': 'RMSprop'}\n",
      "0.879620 (0.030481) with: {'batch_size': 100, 'epochs': 150, 'optimizer': 'Adam'}\n",
      "0.399322 (0.094912) with: {'batch_size': 100, 'epochs': 150, 'optimizer': 'SGD'}\n",
      "Best: 0.905196 using {'batch_size': 32, 'epochs': 150, 'optimizer': 'Adam'}\n"
     ]
    }
   ],
   "source": [
    "# Grid Search Cross Validation\n",
    "# GridSearch Cross Validation Parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def build_classifier(optimizer):\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units = 50, kernel_initializer = 'uniform', activation = 'relu', input_dim = 32))\n",
    "    classifier.add(Dense(units = 50, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(units = 50, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'softmax'))\n",
    "    classifier.compile(optimizer = optimizer, loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier\n",
    "\n",
    "param_grid = {\n",
    "   \n",
    "    'epochs': [50,100,150], \n",
    "    'batch_size':[32,50,100],\n",
    "    'optimizer':['RMSprop', 'Adam','SGD'],\n",
    "    \n",
    "}\n",
    "\n",
    "# create model\n",
    "\n",
    "# Creating Model Object with KerasClassifier\n",
    "# create_model = model\n",
    "model_cv = KerasClassifier(build_fn = build_classifier, verbose=1)\n",
    "\n",
    "\n",
    "grid = GridSearchCV(estimator = model_cv,  \n",
    "                    n_jobs = -1, \n",
    "                    verbose = 1,\n",
    "                    cv = 10,\n",
    "                    param_grid = param_grid)\n",
    "\n",
    "grid_cv_model = grid.fit(X_train, y_train) # Fitting the GridSearch Object on the Train Set\n",
    "\n",
    "\n",
    "means = grid_cv_model.cv_results_['mean_test_score'] # Mean of test scores\n",
    "stds = grid_cv_model.cv_results_['std_test_score'] # standard deviations of test scores\n",
    "params = grid_cv_model.cv_results_['params'] # parameters used\n",
    "# to print all scores, standard deviations and parameters used\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "# Printing the Best Parameters as a Result of Grid Search Cross Validation on the Screen\n",
    "print(\"Best: %f using %s\" % (grid_cv_model.best_score_, grid_cv_model.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d1bec42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 1.3836 - accuracy: 0.3922\n",
      "Epoch 2/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 1.3243 - accuracy: 0.5578\n",
      "Epoch 3/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.9771 - accuracy: 0.5514\n",
      "Epoch 4/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.7353 - accuracy: 0.5701\n",
      "Epoch 5/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.6877 - accuracy: 0.5877\n",
      "Epoch 6/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6809 - accuracy: 0.5489\n",
      "Epoch 7/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6774 - accuracy: 0.5547\n",
      "Epoch 8/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6708 - accuracy: 0.5860\n",
      "Epoch 9/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6701 - accuracy: 0.5832\n",
      "Epoch 10/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6868 - accuracy: 0.5691\n",
      "Epoch 11/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6674 - accuracy: 0.6026\n",
      "Epoch 12/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6673 - accuracy: 0.6023\n",
      "Epoch 13/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6584 - accuracy: 0.5884\n",
      "Epoch 14/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.6597 - accuracy: 0.6143\n",
      "Epoch 15/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6617 - accuracy: 0.5912\n",
      "Epoch 16/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6423 - accuracy: 0.6384\n",
      "Epoch 17/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6315 - accuracy: 0.6376\n",
      "Epoch 18/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6243 - accuracy: 0.6357\n",
      "Epoch 19/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6088 - accuracy: 0.6437\n",
      "Epoch 20/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6172 - accuracy: 0.6334\n",
      "Epoch 21/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6064 - accuracy: 0.6307\n",
      "Epoch 22/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5842 - accuracy: 0.6625\n",
      "Epoch 23/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6020 - accuracy: 0.6552\n",
      "Epoch 24/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5908 - accuracy: 0.6579\n",
      "Epoch 25/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5623 - accuracy: 0.6809\n",
      "Epoch 26/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5429 - accuracy: 0.7086\n",
      "Epoch 27/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5706 - accuracy: 0.6857\n",
      "Epoch 28/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5456 - accuracy: 0.6896\n",
      "Epoch 29/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5361 - accuracy: 0.7397\n",
      "Epoch 30/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5282 - accuracy: 0.7114\n",
      "Epoch 31/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5141 - accuracy: 0.7352\n",
      "Epoch 32/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5034 - accuracy: 0.7456\n",
      "Epoch 33/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4910 - accuracy: 0.7400\n",
      "Epoch 34/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4891 - accuracy: 0.7620\n",
      "Epoch 35/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4724 - accuracy: 0.7632\n",
      "Epoch 36/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4698 - accuracy: 0.7618\n",
      "Epoch 37/150\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.4360 - accuracy: 0.75 - 0s 1ms/step - loss: 0.4512 - accuracy: 0.7650\n",
      "Epoch 38/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4013 - accuracy: 0.8154\n",
      "Epoch 39/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4154 - accuracy: 0.7994\n",
      "Epoch 40/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4133 - accuracy: 0.7873\n",
      "Epoch 41/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3903 - accuracy: 0.8386\n",
      "Epoch 42/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3906 - accuracy: 0.8155\n",
      "Epoch 43/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3429 - accuracy: 0.8428\n",
      "Epoch 44/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3245 - accuracy: 0.8593\n",
      "Epoch 45/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3515 - accuracy: 0.8314\n",
      "Epoch 46/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3443 - accuracy: 0.8398\n",
      "Epoch 47/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3278 - accuracy: 0.8553\n",
      "Epoch 48/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.3247 - accuracy: 0.8715\n",
      "Epoch 49/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.3203 - accuracy: 0.8446\n",
      "Epoch 50/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.3130 - accuracy: 0.8534\n",
      "Epoch 51/150\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.3184 - accuracy: 0.8456\n",
      "Epoch 52/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.2981 - accuracy: 0.8737\n",
      "Epoch 53/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.3018 - accuracy: 0.8581\n",
      "Epoch 54/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.2854 - accuracy: 0.8644\n",
      "Epoch 55/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.2551 - accuracy: 0.8938\n",
      "Epoch 56/150\n",
      "19/19 [==============================] - 0s 886us/step - loss: 0.3244 - accuracy: 0.8440\n",
      "Epoch 57/150\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.2613 - accuracy: 0.8897\n",
      "Epoch 58/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2511 - accuracy: 0.8909\n",
      "Epoch 59/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.2870 - accuracy: 0.8454\n",
      "Epoch 60/150\n",
      "19/19 [==============================] - 0s 943us/step - loss: 0.2574 - accuracy: 0.8860\n",
      "Epoch 61/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.2865 - accuracy: 0.8563\n",
      "Epoch 62/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2544 - accuracy: 0.8744\n",
      "Epoch 63/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2829 - accuracy: 0.8538\n",
      "Epoch 64/150\n",
      "19/19 [==============================] - 0s 941us/step - loss: 0.2618 - accuracy: 0.8766\n",
      "Epoch 65/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2204 - accuracy: 0.9103\n",
      "Epoch 66/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.2519 - accuracy: 0.8832\n",
      "Epoch 67/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.2686 - accuracy: 0.8686\n",
      "Epoch 68/150\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.2521 - accuracy: 0.8720\n",
      "Epoch 69/150\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.2544 - accuracy: 0.8670\n",
      "Epoch 70/150\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.2406 - accuracy: 0.8867\n",
      "Epoch 71/150\n",
      "19/19 [==============================] - 0s 886us/step - loss: 0.2290 - accuracy: 0.9063\n",
      "Epoch 72/150\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.2371 - accuracy: 0.8793\n",
      "Epoch 73/150\n",
      "19/19 [==============================] - 0s 831us/step - loss: 0.2428 - accuracy: 0.8741\n",
      "Epoch 74/150\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.2563 - accuracy: 0.8785\n",
      "Epoch 75/150\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.2106 - accuracy: 0.9073\n",
      "Epoch 76/150\n",
      "19/19 [==============================] - 0s 886us/step - loss: 0.2241 - accuracy: 0.8888\n",
      "Epoch 77/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.2146 - accuracy: 0.8926\n",
      "Epoch 78/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2100 - accuracy: 0.9014\n",
      "Epoch 79/150\n",
      "19/19 [==============================] - 0s 943us/step - loss: 0.2213 - accuracy: 0.8881\n",
      "Epoch 80/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.2154 - accuracy: 0.9071\n",
      "Epoch 81/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 831us/step - loss: 0.2242 - accuracy: 0.8823\n",
      "Epoch 82/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.2008 - accuracy: 0.9075\n",
      "Epoch 83/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2010 - accuracy: 0.8980\n",
      "Epoch 84/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2359 - accuracy: 0.8672\n",
      "Epoch 85/150\n",
      "19/19 [==============================] - 0s 886us/step - loss: 0.2000 - accuracy: 0.8996\n",
      "Epoch 86/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.2089 - accuracy: 0.8919\n",
      "Epoch 87/150\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.1719 - accuracy: 0.9248\n",
      "Epoch 88/150\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.2149 - accuracy: 0.8883\n",
      "Epoch 89/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.1898 - accuracy: 0.9015\n",
      "Epoch 90/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.2041 - accuracy: 0.8956\n",
      "Epoch 91/150\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.1970 - accuracy: 0.9075\n",
      "Epoch 92/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1584 - accuracy: 0.9172\n",
      "Epoch 93/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1736 - accuracy: 0.9153\n",
      "Epoch 94/150\n",
      "19/19 [==============================] - 0s 943us/step - loss: 0.1991 - accuracy: 0.8908\n",
      "Epoch 95/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1684 - accuracy: 0.9102\n",
      "Epoch 96/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1721 - accuracy: 0.9204\n",
      "Epoch 97/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1730 - accuracy: 0.9177\n",
      "Epoch 98/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1558 - accuracy: 0.9280\n",
      "Epoch 99/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2002 - accuracy: 0.9017\n",
      "Epoch 100/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1841 - accuracy: 0.9114\n",
      "Epoch 101/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1746 - accuracy: 0.9036\n",
      "Epoch 102/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2077 - accuracy: 0.9153\n",
      "Epoch 103/150\n",
      "19/19 [==============================] - 0s 886us/step - loss: 0.1769 - accuracy: 0.9126\n",
      "Epoch 104/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1746 - accuracy: 0.9173\n",
      "Epoch 105/150\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.1766 - accuracy: 0.9117\n",
      "Epoch 106/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1431 - accuracy: 0.9453\n",
      "Epoch 107/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.2097 - accuracy: 0.9040\n",
      "Epoch 108/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1763 - accuracy: 0.9210\n",
      "Epoch 109/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1926 - accuracy: 0.9227\n",
      "Epoch 110/150\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.1822 - accuracy: 0.9156\n",
      "Epoch 111/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1529 - accuracy: 0.9263\n",
      "Epoch 112/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1353 - accuracy: 0.9401\n",
      "Epoch 113/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1336 - accuracy: 0.9280\n",
      "Epoch 114/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1526 - accuracy: 0.9379\n",
      "Epoch 115/150\n",
      "19/19 [==============================] - 0s 941us/step - loss: 0.1480 - accuracy: 0.9360\n",
      "Epoch 116/150\n",
      "19/19 [==============================] - 0s 943us/step - loss: 0.1534 - accuracy: 0.9281\n",
      "Epoch 117/150\n",
      "19/19 [==============================] - 0s 939us/step - loss: 0.1238 - accuracy: 0.9449\n",
      "Epoch 118/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.1340 - accuracy: 0.9446\n",
      "Epoch 119/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1543 - accuracy: 0.9274\n",
      "Epoch 120/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1543 - accuracy: 0.9426\n",
      "Epoch 121/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1320 - accuracy: 0.9334\n",
      "Epoch 122/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1161 - accuracy: 0.9439\n",
      "Epoch 123/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1212 - accuracy: 0.9521\n",
      "Epoch 124/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1320 - accuracy: 0.9421\n",
      "Epoch 125/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1160 - accuracy: 0.9441\n",
      "Epoch 126/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1253 - accuracy: 0.9453\n",
      "Epoch 127/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1123 - accuracy: 0.9466\n",
      "Epoch 128/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0973 - accuracy: 0.9695\n",
      "Epoch 129/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1041 - accuracy: 0.9508\n",
      "Epoch 130/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.1121 - accuracy: 0.9537\n",
      "Epoch 131/150\n",
      "19/19 [==============================] - 0s 886us/step - loss: 0.1195 - accuracy: 0.9461\n",
      "Epoch 132/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0981 - accuracy: 0.9612\n",
      "Epoch 133/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1120 - accuracy: 0.9414\n",
      "Epoch 134/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0888 - accuracy: 0.9614\n",
      "Epoch 135/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1049 - accuracy: 0.9441\n",
      "Epoch 136/150\n",
      "19/19 [==============================] - 0s 943us/step - loss: 0.0965 - accuracy: 0.9621\n",
      "Epoch 137/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1104 - accuracy: 0.9431\n",
      "Epoch 138/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0954 - accuracy: 0.9641\n",
      "Epoch 139/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0769 - accuracy: 0.9729\n",
      "Epoch 140/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0792 - accuracy: 0.9694\n",
      "Epoch 141/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.1042 - accuracy: 0.9590\n",
      "Epoch 142/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0814 - accuracy: 0.9713\n",
      "Epoch 143/150\n",
      "19/19 [==============================] - 0s 941us/step - loss: 0.0820 - accuracy: 0.9679\n",
      "Epoch 144/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0850 - accuracy: 0.9568\n",
      "Epoch 145/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0606 - accuracy: 0.9874\n",
      "Epoch 146/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.0799 - accuracy: 0.9715\n",
      "Epoch 147/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0691 - accuracy: 0.9767\n",
      "Epoch 148/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0705 - accuracy: 0.9763\n",
      "Epoch 149/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0576 - accuracy: 0.9846\n",
      "Epoch 150/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0636 - accuracy: 0.9759\n",
      "3/3 [==============================] - 0s 997us/step\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\position\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 1ms/step - loss: 1.3843 - accuracy: 0.3099\n",
      "Epoch 2/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 1.3329 - accuracy: 0.5754\n",
      "Epoch 3/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.9998 - accuracy: 0.5874\n",
      "Epoch 4/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.7309 - accuracy: 0.5607\n",
      "Epoch 5/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6978 - accuracy: 0.5874\n",
      "Epoch 6/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.6847 - accuracy: 0.5719\n",
      "Epoch 7/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6742 - accuracy: 0.5928\n",
      "Epoch 8/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6759 - accuracy: 0.5908\n",
      "Epoch 9/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6633 - accuracy: 0.6021\n",
      "Epoch 10/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.6748 - accuracy: 0.5714\n",
      "Epoch 11/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6634 - accuracy: 0.5848\n",
      "Epoch 12/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6552 - accuracy: 0.5977\n",
      "Epoch 13/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6473 - accuracy: 0.6376\n",
      "Epoch 14/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.6478 - accuracy: 0.6184\n",
      "Epoch 15/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6079 - accuracy: 0.6577\n",
      "Epoch 16/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6038 - accuracy: 0.6799\n",
      "Epoch 17/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6251 - accuracy: 0.6501\n",
      "Epoch 18/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5951 - accuracy: 0.6814\n",
      "Epoch 19/150\n",
      "19/19 [==============================] - 0s 995us/step - loss: 0.5908 - accuracy: 0.6604\n",
      "Epoch 20/150\n",
      "19/19 [==============================] - 0s 1000us/step - loss: 0.5883 - accuracy: 0.6863\n",
      "Epoch 21/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5679 - accuracy: 0.6631\n",
      "Epoch 22/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5423 - accuracy: 0.7198\n",
      "Epoch 23/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.5217 - accuracy: 0.7460\n",
      "Epoch 24/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5316 - accuracy: 0.7419\n",
      "Epoch 25/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5129 - accuracy: 0.7473\n",
      "Epoch 26/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4988 - accuracy: 0.7638\n",
      "Epoch 27/150\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.4109 - accuracy: 0.93 - 0s 1ms/step - loss: 0.4663 - accuracy: 0.7910\n",
      "Epoch 28/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4651 - accuracy: 0.7764\n",
      "Epoch 29/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4698 - accuracy: 0.7666\n",
      "Epoch 30/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4430 - accuracy: 0.7961\n",
      "Epoch 31/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4404 - accuracy: 0.8114\n",
      "Epoch 32/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.4110 - accuracy: 0.8126\n",
      "Epoch 33/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.4363 - accuracy: 0.7806\n",
      "Epoch 34/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4076 - accuracy: 0.7956\n",
      "Epoch 35/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3664 - accuracy: 0.8315\n",
      "Epoch 36/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4006 - accuracy: 0.7901\n",
      "Epoch 37/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3413 - accuracy: 0.8285\n",
      "Epoch 38/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3385 - accuracy: 0.8560\n",
      "Epoch 39/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3338 - accuracy: 0.8266\n",
      "Epoch 40/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.3787 - accuracy: 0.8095\n",
      "Epoch 41/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.3298 - accuracy: 0.8491\n",
      "Epoch 42/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3125 - accuracy: 0.8574\n",
      "Epoch 43/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.3318 - accuracy: 0.8444\n",
      "Epoch 44/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2952 - accuracy: 0.8596\n",
      "Epoch 45/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3054 - accuracy: 0.8543\n",
      "Epoch 46/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3043 - accuracy: 0.8498\n",
      "Epoch 47/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2524 - accuracy: 0.8806\n",
      "Epoch 48/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2756 - accuracy: 0.8723\n",
      "Epoch 49/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2396 - accuracy: 0.9037\n",
      "Epoch 50/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2310 - accuracy: 0.9019\n",
      "Epoch 51/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2306 - accuracy: 0.9013\n",
      "Epoch 52/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2417 - accuracy: 0.9000\n",
      "Epoch 53/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2164 - accuracy: 0.9140\n",
      "Epoch 54/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1921 - accuracy: 0.9299\n",
      "Epoch 55/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2170 - accuracy: 0.9007\n",
      "Epoch 56/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2027 - accuracy: 0.9151\n",
      "Epoch 57/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1964 - accuracy: 0.9180\n",
      "Epoch 58/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1745 - accuracy: 0.9240\n",
      "Epoch 59/150\n",
      "19/19 [==============================] - 0s 1000us/step - loss: 0.1760 - accuracy: 0.9307\n",
      "Epoch 60/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1583 - accuracy: 0.9456\n",
      "Epoch 61/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1509 - accuracy: 0.9517\n",
      "Epoch 62/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1510 - accuracy: 0.9459\n",
      "Epoch 63/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1816 - accuracy: 0.9378\n",
      "Epoch 64/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1298 - accuracy: 0.9557\n",
      "Epoch 65/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1593 - accuracy: 0.9387\n",
      "Epoch 66/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1447 - accuracy: 0.9547\n",
      "Epoch 67/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1458 - accuracy: 0.9459\n",
      "Epoch 68/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1355 - accuracy: 0.9556\n",
      "Epoch 69/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1268 - accuracy: 0.9583\n",
      "Epoch 70/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1079 - accuracy: 0.9732\n",
      "Epoch 71/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1587 - accuracy: 0.9298\n",
      "Epoch 72/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1207 - accuracy: 0.9586\n",
      "Epoch 73/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1209 - accuracy: 0.9542\n",
      "Epoch 74/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1276 - accuracy: 0.9581\n",
      "Epoch 75/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1223 - accuracy: 0.9580\n",
      "Epoch 76/150\n",
      "19/19 [==============================] - 0s 991us/step - loss: 0.1153 - accuracy: 0.9619\n",
      "Epoch 77/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1042 - accuracy: 0.9675\n",
      "Epoch 78/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1057 - accuracy: 0.9622\n",
      "Epoch 79/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1055 - accuracy: 0.9580\n",
      "Epoch 80/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1098 - accuracy: 0.9591\n",
      "Epoch 81/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0959 - accuracy: 0.9674\n",
      "Epoch 82/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 942us/step - loss: 0.0886 - accuracy: 0.9723\n",
      "Epoch 83/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1006 - accuracy: 0.9664\n",
      "Epoch 84/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0858 - accuracy: 0.9693\n",
      "Epoch 85/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0808 - accuracy: 0.9738\n",
      "Epoch 86/150\n",
      "19/19 [==============================] - 0s 921us/step - loss: 0.0868 - accuracy: 0.9749\n",
      "Epoch 87/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0929 - accuracy: 0.9756\n",
      "Epoch 88/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0904 - accuracy: 0.9718\n",
      "Epoch 89/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1190 - accuracy: 0.9581\n",
      "Epoch 90/150\n",
      "19/19 [==============================] - 0s 996us/step - loss: 0.0978 - accuracy: 0.9634\n",
      "Epoch 91/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0741 - accuracy: 0.9700\n",
      "Epoch 92/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1046 - accuracy: 0.9608\n",
      "Epoch 93/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0961 - accuracy: 0.9645\n",
      "Epoch 94/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0897 - accuracy: 0.9707\n",
      "Epoch 95/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0715 - accuracy: 0.9776\n",
      "Epoch 96/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0635 - accuracy: 0.9793\n",
      "Epoch 97/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0889 - accuracy: 0.9708\n",
      "Epoch 98/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0807 - accuracy: 0.9688\n",
      "Epoch 99/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.0775 - accuracy: 0.9706\n",
      "Epoch 100/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0713 - accuracy: 0.9804\n",
      "Epoch 101/150\n",
      "19/19 [==============================] - 0s 975us/step - loss: 0.0732 - accuracy: 0.9774\n",
      "Epoch 102/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0849 - accuracy: 0.9651\n",
      "Epoch 103/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0818 - accuracy: 0.9633\n",
      "Epoch 104/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0664 - accuracy: 0.9745\n",
      "Epoch 105/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0874 - accuracy: 0.9676\n",
      "Epoch 106/150\n",
      "19/19 [==============================] - 0s 947us/step - loss: 0.0545 - accuracy: 0.9804\n",
      "Epoch 107/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0655 - accuracy: 0.9808\n",
      "Epoch 108/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0788 - accuracy: 0.9751\n",
      "Epoch 109/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0699 - accuracy: 0.9781\n",
      "Epoch 110/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1979 - accuracy: 0.9257\n",
      "Epoch 111/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0743 - accuracy: 0.9692\n",
      "Epoch 112/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0748 - accuracy: 0.9790\n",
      "Epoch 113/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0868 - accuracy: 0.9675\n",
      "Epoch 114/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0721 - accuracy: 0.9684\n",
      "Epoch 115/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0773 - accuracy: 0.9630\n",
      "Epoch 116/150\n",
      "19/19 [==============================] - 0s 996us/step - loss: 0.0589 - accuracy: 0.9718\n",
      "Epoch 117/150\n",
      "19/19 [==============================] - 0s 1000us/step - loss: 0.0515 - accuracy: 0.9786\n",
      "Epoch 118/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0671 - accuracy: 0.9790\n",
      "Epoch 119/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0576 - accuracy: 0.9787\n",
      "Epoch 120/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0409 - accuracy: 0.9863\n",
      "Epoch 121/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0580 - accuracy: 0.9737\n",
      "Epoch 122/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0518 - accuracy: 0.9754\n",
      "Epoch 123/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.0453 - accuracy: 0.9815\n",
      "Epoch 124/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0460 - accuracy: 0.9822\n",
      "Epoch 125/150\n",
      "19/19 [==============================] - 0s 943us/step - loss: 0.0350 - accuracy: 0.9896\n",
      "Epoch 126/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0481 - accuracy: 0.9818\n",
      "Epoch 127/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0531 - accuracy: 0.9776\n",
      "Epoch 128/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0522 - accuracy: 0.9801\n",
      "Epoch 129/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0564 - accuracy: 0.9835\n",
      "Epoch 130/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0487 - accuracy: 0.9791\n",
      "Epoch 131/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0394 - accuracy: 0.9814\n",
      "Epoch 132/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.0458 - accuracy: 0.9803\n",
      "Epoch 133/150\n",
      "19/19 [==============================] - 0s 1000us/step - loss: 0.0536 - accuracy: 0.9759\n",
      "Epoch 134/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0461 - accuracy: 0.9866\n",
      "Epoch 135/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0503 - accuracy: 0.9822\n",
      "Epoch 136/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0643 - accuracy: 0.9723\n",
      "Epoch 137/150\n",
      "19/19 [==============================] - 0s 995us/step - loss: 0.0326 - accuracy: 0.9856\n",
      "Epoch 138/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0485 - accuracy: 0.9827\n",
      "Epoch 139/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0453 - accuracy: 0.9818\n",
      "Epoch 140/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.0455 - accuracy: 0.9830\n",
      "Epoch 141/150\n",
      "19/19 [==============================] - 0s 1000us/step - loss: 0.0353 - accuracy: 0.9848\n",
      "Epoch 142/150\n",
      "19/19 [==============================] - 0s 941us/step - loss: 0.0287 - accuracy: 0.9875\n",
      "Epoch 143/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.0305 - accuracy: 0.9929\n",
      "Epoch 144/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0497 - accuracy: 0.9800\n",
      "Epoch 145/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0593 - accuracy: 0.9790\n",
      "Epoch 146/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0445 - accuracy: 0.9815\n",
      "Epoch 147/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0394 - accuracy: 0.9855\n",
      "Epoch 148/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0486 - accuracy: 0.9785\n",
      "Epoch 149/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0292 - accuracy: 0.9883\n",
      "Epoch 150/150\n",
      "19/19 [==============================] - 0s 944us/step - loss: 0.0373 - accuracy: 0.9818\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\position\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 1ms/step - loss: 1.3844 - accuracy: 0.3462\n",
      "Epoch 2/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 1.3294 - accuracy: 0.5195\n",
      "Epoch 3/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 1.0158 - accuracy: 0.5221\n",
      "Epoch 4/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.7563 - accuracy: 0.5555\n",
      "Epoch 5/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6901 - accuracy: 0.5613\n",
      "Epoch 6/150\n",
      "19/19 [==============================] - 0s 994us/step - loss: 0.6862 - accuracy: 0.5796\n",
      "Epoch 7/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6813 - accuracy: 0.5250\n",
      "Epoch 8/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6879 - accuracy: 0.5598\n",
      "Epoch 9/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6920 - accuracy: 0.5267\n",
      "Epoch 10/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6819 - accuracy: 0.5515\n",
      "Epoch 11/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6783 - accuracy: 0.5402\n",
      "Epoch 12/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6840 - accuracy: 0.5030\n",
      "Epoch 13/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6725 - accuracy: 0.5745\n",
      "Epoch 14/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6747 - accuracy: 0.5437\n",
      "Epoch 15/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6686 - accuracy: 0.5679\n",
      "Epoch 16/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6707 - accuracy: 0.5904\n",
      "Epoch 17/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6825 - accuracy: 0.5349\n",
      "Epoch 18/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6605 - accuracy: 0.5651\n",
      "Epoch 19/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6678 - accuracy: 0.5779\n",
      "Epoch 20/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6785 - accuracy: 0.5601\n",
      "Epoch 21/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6683 - accuracy: 0.5798\n",
      "Epoch 22/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6543 - accuracy: 0.5958\n",
      "Epoch 23/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6477 - accuracy: 0.6386\n",
      "Epoch 24/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6410 - accuracy: 0.6469\n",
      "Epoch 25/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6256 - accuracy: 0.6625\n",
      "Epoch 26/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6172 - accuracy: 0.6739\n",
      "Epoch 27/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5971 - accuracy: 0.6975\n",
      "Epoch 28/150\n",
      "19/19 [==============================] - 0s 996us/step - loss: 0.5923 - accuracy: 0.6746\n",
      "Epoch 29/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5558 - accuracy: 0.7054\n",
      "Epoch 30/150\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.5127 - accuracy: 0.7555\n",
      "Epoch 31/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.5389 - accuracy: 0.7288\n",
      "Epoch 32/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.4994 - accuracy: 0.7632\n",
      "Epoch 33/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.5105 - accuracy: 0.7263\n",
      "Epoch 34/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.4676 - accuracy: 0.7766\n",
      "Epoch 35/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.4914 - accuracy: 0.7717\n",
      "Epoch 36/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.4402 - accuracy: 0.7672\n",
      "Epoch 37/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.4324 - accuracy: 0.7998\n",
      "Epoch 38/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.4313 - accuracy: 0.8065\n",
      "Epoch 39/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.4223 - accuracy: 0.8002\n",
      "Epoch 40/150\n",
      "19/19 [==============================] - 0s 886us/step - loss: 0.4030 - accuracy: 0.8121\n",
      "Epoch 41/150\n",
      "19/19 [==============================] - 0s 943us/step - loss: 0.3685 - accuracy: 0.8257\n",
      "Epoch 42/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.3803 - accuracy: 0.8357\n",
      "Epoch 43/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3231 - accuracy: 0.8563\n",
      "Epoch 44/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.3353 - accuracy: 0.8633\n",
      "Epoch 45/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.3366 - accuracy: 0.8609\n",
      "Epoch 46/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.3110 - accuracy: 0.8640\n",
      "Epoch 47/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2849 - accuracy: 0.8821\n",
      "Epoch 48/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.2918 - accuracy: 0.8899\n",
      "Epoch 49/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.3024 - accuracy: 0.8754\n",
      "Epoch 50/150\n",
      "19/19 [==============================] - 0s 996us/step - loss: 0.2578 - accuracy: 0.9016\n",
      "Epoch 51/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.3256 - accuracy: 0.8479\n",
      "Epoch 52/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2751 - accuracy: 0.9028\n",
      "Epoch 53/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2376 - accuracy: 0.8991\n",
      "Epoch 54/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2814 - accuracy: 0.8841\n",
      "Epoch 55/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2239 - accuracy: 0.9240\n",
      "Epoch 56/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.2082 - accuracy: 0.9247\n",
      "Epoch 57/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1962 - accuracy: 0.9381\n",
      "Epoch 58/150\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.2220 - accuracy: 0.9153\n",
      "Epoch 59/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.2311 - accuracy: 0.9072\n",
      "Epoch 60/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2376 - accuracy: 0.9163\n",
      "Epoch 61/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1916 - accuracy: 0.9295\n",
      "Epoch 62/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1670 - accuracy: 0.9377\n",
      "Epoch 63/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1993 - accuracy: 0.9263\n",
      "Epoch 64/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1835 - accuracy: 0.9355\n",
      "Epoch 65/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2082 - accuracy: 0.9207\n",
      "Epoch 66/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.1460 - accuracy: 0.9552\n",
      "Epoch 67/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1623 - accuracy: 0.9384\n",
      "Epoch 68/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1610 - accuracy: 0.9492\n",
      "Epoch 69/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1525 - accuracy: 0.9486\n",
      "Epoch 70/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1920 - accuracy: 0.9314\n",
      "Epoch 71/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1763 - accuracy: 0.9335\n",
      "Epoch 72/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1331 - accuracy: 0.9559\n",
      "Epoch 73/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1546 - accuracy: 0.9481\n",
      "Epoch 74/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1272 - accuracy: 0.9602\n",
      "Epoch 75/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1242 - accuracy: 0.9649\n",
      "Epoch 76/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1198 - accuracy: 0.9703\n",
      "Epoch 77/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1283 - accuracy: 0.9606\n",
      "Epoch 78/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1357 - accuracy: 0.9525\n",
      "Epoch 79/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1316 - accuracy: 0.9515\n",
      "Epoch 80/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1248 - accuracy: 0.9575\n",
      "Epoch 81/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1411 - accuracy: 0.9547\n",
      "Epoch 82/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 998us/step - loss: 0.1304 - accuracy: 0.9578\n",
      "Epoch 83/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1085 - accuracy: 0.9599\n",
      "Epoch 84/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1390 - accuracy: 0.9427\n",
      "Epoch 85/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1661 - accuracy: 0.9434\n",
      "Epoch 86/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1576 - accuracy: 0.9369\n",
      "Epoch 87/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1434 - accuracy: 0.9471\n",
      "Epoch 88/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1356 - accuracy: 0.9531\n",
      "Epoch 89/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1459 - accuracy: 0.9530\n",
      "Epoch 90/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1364 - accuracy: 0.9533\n",
      "Epoch 91/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1207 - accuracy: 0.9567\n",
      "Epoch 92/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0975 - accuracy: 0.9663\n",
      "Epoch 93/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.0952 - accuracy: 0.9698\n",
      "Epoch 94/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1224 - accuracy: 0.9485\n",
      "Epoch 95/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1071 - accuracy: 0.9586\n",
      "Epoch 96/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1248 - accuracy: 0.9564\n",
      "Epoch 97/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1267 - accuracy: 0.9454\n",
      "Epoch 98/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1123 - accuracy: 0.9604\n",
      "Epoch 99/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1148 - accuracy: 0.9578\n",
      "Epoch 100/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0992 - accuracy: 0.9681\n",
      "Epoch 101/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1036 - accuracy: 0.9631\n",
      "Epoch 102/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1016 - accuracy: 0.9672\n",
      "Epoch 103/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0954 - accuracy: 0.9674\n",
      "Epoch 104/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1255 - accuracy: 0.9460\n",
      "Epoch 105/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0950 - accuracy: 0.9664\n",
      "Epoch 106/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0957 - accuracy: 0.9649\n",
      "Epoch 107/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0903 - accuracy: 0.9615\n",
      "Epoch 108/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1200 - accuracy: 0.9480\n",
      "Epoch 109/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0905 - accuracy: 0.9653\n",
      "Epoch 110/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1069 - accuracy: 0.9568\n",
      "Epoch 111/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0880 - accuracy: 0.9648\n",
      "Epoch 112/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0759 - accuracy: 0.9756\n",
      "Epoch 113/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1017 - accuracy: 0.9529\n",
      "Epoch 114/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1277 - accuracy: 0.9475\n",
      "Epoch 115/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0863 - accuracy: 0.9680\n",
      "Epoch 116/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0750 - accuracy: 0.9713\n",
      "Epoch 117/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0858 - accuracy: 0.9698\n",
      "Epoch 118/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0821 - accuracy: 0.9651\n",
      "Epoch 119/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0948 - accuracy: 0.9644\n",
      "Epoch 120/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0867 - accuracy: 0.9690\n",
      "Epoch 121/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0840 - accuracy: 0.9692\n",
      "Epoch 122/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0840 - accuracy: 0.9681\n",
      "Epoch 123/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1065 - accuracy: 0.9552\n",
      "Epoch 124/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0818 - accuracy: 0.9649\n",
      "Epoch 125/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0785 - accuracy: 0.9690\n",
      "Epoch 126/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1015 - accuracy: 0.9536\n",
      "Epoch 127/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0934 - accuracy: 0.9647\n",
      "Epoch 128/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0905 - accuracy: 0.9648\n",
      "Epoch 129/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0890 - accuracy: 0.9612\n",
      "Epoch 130/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0844 - accuracy: 0.9700\n",
      "Epoch 131/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0835 - accuracy: 0.9618\n",
      "Epoch 132/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0786 - accuracy: 0.9648\n",
      "Epoch 133/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0925 - accuracy: 0.9591\n",
      "Epoch 134/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0898 - accuracy: 0.9620\n",
      "Epoch 135/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1019 - accuracy: 0.9482\n",
      "Epoch 136/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0860 - accuracy: 0.9589\n",
      "Epoch 137/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0603 - accuracy: 0.9748\n",
      "Epoch 138/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0682 - accuracy: 0.9771\n",
      "Epoch 139/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0690 - accuracy: 0.9725\n",
      "Epoch 140/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0626 - accuracy: 0.9769\n",
      "Epoch 141/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0843 - accuracy: 0.9677\n",
      "Epoch 142/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0817 - accuracy: 0.9629\n",
      "Epoch 143/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1068 - accuracy: 0.9506\n",
      "Epoch 144/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0807 - accuracy: 0.9609\n",
      "Epoch 145/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0545 - accuracy: 0.9795\n",
      "Epoch 146/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0614 - accuracy: 0.9754\n",
      "Epoch 147/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0763 - accuracy: 0.9666\n",
      "Epoch 148/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0667 - accuracy: 0.9732\n",
      "Epoch 149/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0632 - accuracy: 0.9717\n",
      "Epoch 150/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.0693 - accuracy: 0.9741\n",
      "3/3 [==============================] - 0s 997us/step\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\position\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 1ms/step - loss: 1.3843 - accuracy: 0.3669\n",
      "Epoch 2/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 1.3333 - accuracy: 0.5387\n",
      "Epoch 3/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 1.0034 - accuracy: 0.5417\n",
      "Epoch 4/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.7469 - accuracy: 0.5365\n",
      "Epoch 5/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6939 - accuracy: 0.5587\n",
      "Epoch 6/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6728 - accuracy: 0.6121\n",
      "Epoch 7/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6949 - accuracy: 0.5074\n",
      "Epoch 8/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6852 - accuracy: 0.5658\n",
      "Epoch 9/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6786 - accuracy: 0.5831\n",
      "Epoch 10/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6745 - accuracy: 0.5903\n",
      "Epoch 11/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6896 - accuracy: 0.5206\n",
      "Epoch 12/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6678 - accuracy: 0.5952\n",
      "Epoch 13/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6684 - accuracy: 0.6007\n",
      "Epoch 14/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6782 - accuracy: 0.5391\n",
      "Epoch 15/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6784 - accuracy: 0.5683\n",
      "Epoch 16/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6604 - accuracy: 0.5968\n",
      "Epoch 17/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6565 - accuracy: 0.6111\n",
      "Epoch 18/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6663 - accuracy: 0.5973\n",
      "Epoch 19/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6649 - accuracy: 0.5821\n",
      "Epoch 20/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6563 - accuracy: 0.6083\n",
      "Epoch 21/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6530 - accuracy: 0.5956\n",
      "Epoch 22/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6415 - accuracy: 0.6282\n",
      "Epoch 23/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5952 - accuracy: 0.6769\n",
      "Epoch 24/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6106 - accuracy: 0.6607\n",
      "Epoch 25/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6019 - accuracy: 0.6601\n",
      "Epoch 26/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5924 - accuracy: 0.6923\n",
      "Epoch 27/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5590 - accuracy: 0.7196\n",
      "Epoch 28/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5787 - accuracy: 0.6856\n",
      "Epoch 29/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5512 - accuracy: 0.7097\n",
      "Epoch 30/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5231 - accuracy: 0.7402\n",
      "Epoch 31/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5318 - accuracy: 0.7131\n",
      "Epoch 32/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5155 - accuracy: 0.7236\n",
      "Epoch 33/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4819 - accuracy: 0.7374\n",
      "Epoch 34/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4516 - accuracy: 0.7783\n",
      "Epoch 35/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5394 - accuracy: 0.6997\n",
      "Epoch 36/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4830 - accuracy: 0.7292\n",
      "Epoch 37/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4465 - accuracy: 0.7877\n",
      "Epoch 38/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4649 - accuracy: 0.7615\n",
      "Epoch 39/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4455 - accuracy: 0.7813\n",
      "Epoch 40/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4453 - accuracy: 0.7620\n",
      "Epoch 41/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4126 - accuracy: 0.7709\n",
      "Epoch 42/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4328 - accuracy: 0.7611\n",
      "Epoch 43/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4060 - accuracy: 0.8103\n",
      "Epoch 44/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4360 - accuracy: 0.7503\n",
      "Epoch 45/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4052 - accuracy: 0.7944\n",
      "Epoch 46/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4001 - accuracy: 0.8071\n",
      "Epoch 47/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3894 - accuracy: 0.8110\n",
      "Epoch 48/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3457 - accuracy: 0.8371\n",
      "Epoch 49/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3797 - accuracy: 0.7999\n",
      "Epoch 50/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3524 - accuracy: 0.8313\n",
      "Epoch 51/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3482 - accuracy: 0.8062\n",
      "Epoch 52/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3501 - accuracy: 0.8259\n",
      "Epoch 53/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3366 - accuracy: 0.8389\n",
      "Epoch 54/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3230 - accuracy: 0.8484\n",
      "Epoch 55/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3330 - accuracy: 0.8328\n",
      "Epoch 56/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3073 - accuracy: 0.8275\n",
      "Epoch 57/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3273 - accuracy: 0.8431\n",
      "Epoch 58/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3159 - accuracy: 0.8263\n",
      "Epoch 59/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2944 - accuracy: 0.8590\n",
      "Epoch 60/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3092 - accuracy: 0.8310\n",
      "Epoch 61/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3182 - accuracy: 0.8281\n",
      "Epoch 62/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2949 - accuracy: 0.8534\n",
      "Epoch 63/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3142 - accuracy: 0.8473\n",
      "Epoch 64/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2788 - accuracy: 0.8626\n",
      "Epoch 65/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2786 - accuracy: 0.8581\n",
      "Epoch 66/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2546 - accuracy: 0.8809\n",
      "Epoch 67/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2381 - accuracy: 0.9007\n",
      "Epoch 68/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2888 - accuracy: 0.8553\n",
      "Epoch 69/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2824 - accuracy: 0.8493\n",
      "Epoch 70/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2506 - accuracy: 0.8808\n",
      "Epoch 71/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2537 - accuracy: 0.8820\n",
      "Epoch 72/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2407 - accuracy: 0.8878\n",
      "Epoch 73/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2489 - accuracy: 0.8715\n",
      "Epoch 74/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2765 - accuracy: 0.8535\n",
      "Epoch 75/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2542 - accuracy: 0.8683\n",
      "Epoch 76/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2726 - accuracy: 0.8513\n",
      "Epoch 77/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2171 - accuracy: 0.9171\n",
      "Epoch 78/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2090 - accuracy: 0.8992\n",
      "Epoch 79/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2497 - accuracy: 0.8843\n",
      "Epoch 80/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2252 - accuracy: 0.9090\n",
      "Epoch 81/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2181 - accuracy: 0.8956\n",
      "Epoch 82/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2205 - accuracy: 0.8872\n",
      "Epoch 83/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2157 - accuracy: 0.9150\n",
      "Epoch 84/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2187 - accuracy: 0.9094\n",
      "Epoch 85/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2145 - accuracy: 0.8962\n",
      "Epoch 86/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2094 - accuracy: 0.8975\n",
      "Epoch 87/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2176 - accuracy: 0.8876\n",
      "Epoch 88/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2282 - accuracy: 0.8852\n",
      "Epoch 89/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1941 - accuracy: 0.9098\n",
      "Epoch 90/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2028 - accuracy: 0.9071\n",
      "Epoch 91/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2197 - accuracy: 0.8986\n",
      "Epoch 92/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1839 - accuracy: 0.9114\n",
      "Epoch 93/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1957 - accuracy: 0.8848\n",
      "Epoch 94/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2117 - accuracy: 0.9091\n",
      "Epoch 95/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1894 - accuracy: 0.9173\n",
      "Epoch 96/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1805 - accuracy: 0.9329\n",
      "Epoch 97/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1894 - accuracy: 0.9025\n",
      "Epoch 98/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1619 - accuracy: 0.9372\n",
      "Epoch 99/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1594 - accuracy: 0.9300\n",
      "Epoch 100/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1633 - accuracy: 0.9420\n",
      "Epoch 101/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1817 - accuracy: 0.9104\n",
      "Epoch 102/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1548 - accuracy: 0.9379\n",
      "Epoch 103/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1502 - accuracy: 0.9365\n",
      "Epoch 104/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1761 - accuracy: 0.9313\n",
      "Epoch 105/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1347 - accuracy: 0.9513\n",
      "Epoch 106/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1414 - accuracy: 0.9534\n",
      "Epoch 107/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1234 - accuracy: 0.9552\n",
      "Epoch 108/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1232 - accuracy: 0.9647\n",
      "Epoch 109/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1424 - accuracy: 0.9365\n",
      "Epoch 110/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1378 - accuracy: 0.9526\n",
      "Epoch 111/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1417 - accuracy: 0.9436\n",
      "Epoch 112/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1418 - accuracy: 0.9452\n",
      "Epoch 113/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1278 - accuracy: 0.9598\n",
      "Epoch 114/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1530 - accuracy: 0.9416\n",
      "Epoch 115/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1378 - accuracy: 0.9560\n",
      "Epoch 116/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0965 - accuracy: 0.9782\n",
      "Epoch 117/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0924 - accuracy: 0.9781\n",
      "Epoch 118/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1265 - accuracy: 0.9495\n",
      "Epoch 119/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1046 - accuracy: 0.9606\n",
      "Epoch 120/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1007 - accuracy: 0.9605\n",
      "Epoch 121/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0970 - accuracy: 0.9530\n",
      "Epoch 122/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0999 - accuracy: 0.9636\n",
      "Epoch 123/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0812 - accuracy: 0.9725\n",
      "Epoch 124/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1053 - accuracy: 0.9542\n",
      "Epoch 125/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1075 - accuracy: 0.9741\n",
      "Epoch 126/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1222 - accuracy: 0.9594\n",
      "Epoch 127/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0819 - accuracy: 0.9739\n",
      "Epoch 128/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0891 - accuracy: 0.9635\n",
      "Epoch 129/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0935 - accuracy: 0.9595\n",
      "Epoch 130/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0758 - accuracy: 0.9750\n",
      "Epoch 131/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0817 - accuracy: 0.9629\n",
      "Epoch 132/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.9790\n",
      "Epoch 133/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0933 - accuracy: 0.9595\n",
      "Epoch 134/150\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 1.00 - 0s 1ms/step - loss: 0.0685 - accuracy: 0.9724\n",
      "Epoch 135/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0797 - accuracy: 0.9680\n",
      "Epoch 136/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0908 - accuracy: 0.9640\n",
      "Epoch 137/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0839 - accuracy: 0.9641\n",
      "Epoch 138/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0724 - accuracy: 0.9710\n",
      "Epoch 139/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0649 - accuracy: 0.9807\n",
      "Epoch 140/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0888 - accuracy: 0.9583\n",
      "Epoch 141/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1072 - accuracy: 0.9472\n",
      "Epoch 142/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0630 - accuracy: 0.9797\n",
      "Epoch 143/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0970 - accuracy: 0.9569\n",
      "Epoch 144/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0896 - accuracy: 0.9549\n",
      "Epoch 145/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0701 - accuracy: 0.9673\n",
      "Epoch 146/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0865 - accuracy: 0.9635\n",
      "Epoch 147/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0774 - accuracy: 0.9675\n",
      "Epoch 148/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0777 - accuracy: 0.9647\n",
      "Epoch 149/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0822 - accuracy: 0.9636\n",
      "Epoch 150/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0595 - accuracy: 0.9829\n",
      "WARNING:tensorflow:5 out of the last 31 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000281D6A25670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 972us/step\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\position\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 1ms/step - loss: 1.3842 - accuracy: 0.3257\n",
      "Epoch 2/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 1.3352 - accuracy: 0.5899\n",
      "Epoch 3/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 1.0261 - accuracy: 0.5375\n",
      "Epoch 4/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.7375 - accuracy: 0.5798\n",
      "Epoch 5/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.7144 - accuracy: 0.5427\n",
      "Epoch 6/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.7072 - accuracy: 0.5569\n",
      "Epoch 7/150\n",
      "19/19 [==============================] - 0s 999us/step - loss: 0.6896 - accuracy: 0.5561\n",
      "Epoch 8/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6699 - accuracy: 0.5887\n",
      "Epoch 9/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6826 - accuracy: 0.5875\n",
      "Epoch 10/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6840 - accuracy: 0.5541\n",
      "Epoch 11/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6634 - accuracy: 0.5781\n",
      "Epoch 12/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6677 - accuracy: 0.5946\n",
      "Epoch 13/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6753 - accuracy: 0.5872\n",
      "Epoch 14/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6549 - accuracy: 0.6081\n",
      "Epoch 15/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6659 - accuracy: 0.6007\n",
      "Epoch 16/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6776 - accuracy: 0.5783\n",
      "Epoch 17/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6463 - accuracy: 0.6039\n",
      "Epoch 18/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6327 - accuracy: 0.6515\n",
      "Epoch 19/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6470 - accuracy: 0.6153\n",
      "Epoch 20/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6184 - accuracy: 0.6362\n",
      "Epoch 21/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6258 - accuracy: 0.6282\n",
      "Epoch 22/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6134 - accuracy: 0.6425\n",
      "Epoch 23/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5807 - accuracy: 0.6783\n",
      "Epoch 24/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5811 - accuracy: 0.6811\n",
      "Epoch 25/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5724 - accuracy: 0.6818\n",
      "Epoch 26/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5692 - accuracy: 0.6977\n",
      "Epoch 27/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5272 - accuracy: 0.7291\n",
      "Epoch 28/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5405 - accuracy: 0.7074\n",
      "Epoch 29/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5297 - accuracy: 0.7289\n",
      "Epoch 30/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5247 - accuracy: 0.7237\n",
      "Epoch 31/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5185 - accuracy: 0.7142\n",
      "Epoch 32/150\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4942 - accuracy: 0.7498\n",
      "Epoch 33/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4557 - accuracy: 0.7806\n",
      "Epoch 34/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4526 - accuracy: 0.7879\n",
      "Epoch 35/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.4740 - accuracy: 0.7725\n",
      "Epoch 36/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.4429 - accuracy: 0.7601\n",
      "Epoch 37/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.4042 - accuracy: 0.8068\n",
      "Epoch 38/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.3835 - accuracy: 0.8304\n",
      "Epoch 39/150\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.4013 - accuracy: 0.8338\n",
      "Epoch 40/150\n",
      "19/19 [==============================] - 0s 831us/step - loss: 0.3584 - accuracy: 0.8348\n",
      "Epoch 41/150\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.3616 - accuracy: 0.8478\n",
      "Epoch 42/150\n",
      "19/19 [==============================] - 0s 831us/step - loss: 0.2921 - accuracy: 0.8764\n",
      "Epoch 43/150\n",
      "19/19 [==============================] - 0s 941us/step - loss: 0.3402 - accuracy: 0.8444\n",
      "Epoch 44/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.2993 - accuracy: 0.8743\n",
      "Epoch 45/150\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.2980 - accuracy: 0.8719\n",
      "Epoch 46/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2896 - accuracy: 0.8635\n",
      "Epoch 47/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.2655 - accuracy: 0.9043\n",
      "Epoch 48/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2728 - accuracy: 0.8738\n",
      "Epoch 49/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.2420 - accuracy: 0.8908\n",
      "Epoch 50/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2372 - accuracy: 0.9018\n",
      "Epoch 51/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2407 - accuracy: 0.9037\n",
      "Epoch 52/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2017 - accuracy: 0.9308\n",
      "Epoch 53/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2368 - accuracy: 0.9076\n",
      "Epoch 54/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1779 - accuracy: 0.9326\n",
      "Epoch 55/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.2079 - accuracy: 0.9205\n",
      "Epoch 56/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.2574 - accuracy: 0.8983\n",
      "Epoch 57/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1735 - accuracy: 0.9372\n",
      "Epoch 58/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1867 - accuracy: 0.9230\n",
      "Epoch 59/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.2026 - accuracy: 0.9188\n",
      "Epoch 60/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1974 - accuracy: 0.9303\n",
      "Epoch 61/150\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.1658 - accuracy: 0.9331\n",
      "Epoch 62/150\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.1627 - accuracy: 0.9412\n",
      "Epoch 63/150\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.1440 - accuracy: 0.9496\n",
      "Epoch 64/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1346 - accuracy: 0.9603\n",
      "Epoch 65/150\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.1291 - accuracy: 0.9610\n",
      "Epoch 66/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1340 - accuracy: 0.9390\n",
      "Epoch 67/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1262 - accuracy: 0.9531\n",
      "Epoch 68/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1252 - accuracy: 0.9531\n",
      "Epoch 69/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1331 - accuracy: 0.9463\n",
      "Epoch 70/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1431 - accuracy: 0.9411\n",
      "Epoch 71/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1531 - accuracy: 0.9368\n",
      "Epoch 72/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1555 - accuracy: 0.9466\n",
      "Epoch 73/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1190 - accuracy: 0.9555\n",
      "Epoch 74/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1244 - accuracy: 0.9518\n",
      "Epoch 75/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1083 - accuracy: 0.9590\n",
      "Epoch 76/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1424 - accuracy: 0.9320\n",
      "Epoch 77/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1348 - accuracy: 0.9370\n",
      "Epoch 78/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1371 - accuracy: 0.9372\n",
      "Epoch 79/150\n",
      "19/19 [==============================] - 0s 971us/step - loss: 0.1116 - accuracy: 0.9550\n",
      "Epoch 80/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0988 - accuracy: 0.9651\n",
      "Epoch 81/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1436 - accuracy: 0.9211\n",
      "Epoch 82/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 942us/step - loss: 0.1092 - accuracy: 0.9475\n",
      "Epoch 83/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1245 - accuracy: 0.9518\n",
      "Epoch 84/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1055 - accuracy: 0.9535\n",
      "Epoch 85/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.1275 - accuracy: 0.9339\n",
      "Epoch 86/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0984 - accuracy: 0.9644\n",
      "Epoch 87/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0908 - accuracy: 0.9697\n",
      "Epoch 88/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0898 - accuracy: 0.9570\n",
      "Epoch 89/150\n",
      "19/19 [==============================] - 0s 943us/step - loss: 0.0976 - accuracy: 0.9602\n",
      "Epoch 90/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0948 - accuracy: 0.9694\n",
      "Epoch 91/150\n",
      "19/19 [==============================] - 0s 996us/step - loss: 0.0786 - accuracy: 0.9695\n",
      "Epoch 92/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0763 - accuracy: 0.9714\n",
      "Epoch 93/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0818 - accuracy: 0.9580\n",
      "Epoch 94/150\n",
      "19/19 [==============================] - 0s 975us/step - loss: 0.0891 - accuracy: 0.9631\n",
      "Epoch 95/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0659 - accuracy: 0.9769\n",
      "Epoch 96/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0842 - accuracy: 0.9646\n",
      "Epoch 97/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1276 - accuracy: 0.9423\n",
      "Epoch 98/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0714 - accuracy: 0.9683\n",
      "Epoch 99/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0737 - accuracy: 0.9672\n",
      "Epoch 100/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0696 - accuracy: 0.9735\n",
      "Epoch 101/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0734 - accuracy: 0.9692\n",
      "Epoch 102/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0645 - accuracy: 0.9763\n",
      "Epoch 103/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0624 - accuracy: 0.9791\n",
      "Epoch 104/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0686 - accuracy: 0.9708\n",
      "Epoch 105/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0661 - accuracy: 0.9817\n",
      "Epoch 106/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0695 - accuracy: 0.9673\n",
      "Epoch 107/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0630 - accuracy: 0.9773\n",
      "Epoch 108/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0570 - accuracy: 0.9802\n",
      "Epoch 109/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.0591 - accuracy: 0.9784\n",
      "Epoch 110/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.0663 - accuracy: 0.9684\n",
      "Epoch 111/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0615 - accuracy: 0.9878\n",
      "Epoch 112/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.0483 - accuracy: 0.9864\n",
      "Epoch 113/150\n",
      "19/19 [==============================] - 0s 978us/step - loss: 0.0590 - accuracy: 0.9802\n",
      "Epoch 114/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0593 - accuracy: 0.9797\n",
      "Epoch 115/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0558 - accuracy: 0.9837\n",
      "Epoch 116/150\n",
      "19/19 [==============================] - 0s 941us/step - loss: 0.0425 - accuracy: 0.9903\n",
      "Epoch 117/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0473 - accuracy: 0.9876\n",
      "Epoch 118/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0476 - accuracy: 0.9835\n",
      "Epoch 119/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0464 - accuracy: 0.9877\n",
      "Epoch 120/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0467 - accuracy: 0.9857\n",
      "Epoch 121/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0469 - accuracy: 0.9805\n",
      "Epoch 122/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1108 - accuracy: 0.9576\n",
      "Epoch 123/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0584 - accuracy: 0.9758\n",
      "Epoch 124/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0502 - accuracy: 0.9857\n",
      "Epoch 125/150\n",
      "19/19 [==============================] - 0s 996us/step - loss: 0.0503 - accuracy: 0.9804\n",
      "Epoch 126/150\n",
      "19/19 [==============================] - 0s 996us/step - loss: 0.0425 - accuracy: 0.9868\n",
      "Epoch 127/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0373 - accuracy: 0.9872\n",
      "Epoch 128/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0498 - accuracy: 0.9832\n",
      "Epoch 129/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0439 - accuracy: 0.9889\n",
      "Epoch 130/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0743 - accuracy: 0.9588\n",
      "Epoch 131/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0505 - accuracy: 0.9710\n",
      "Epoch 132/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0352 - accuracy: 0.9907\n",
      "Epoch 133/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0429 - accuracy: 0.9858\n",
      "Epoch 134/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0365 - accuracy: 0.9916\n",
      "Epoch 135/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0491 - accuracy: 0.9805\n",
      "Epoch 136/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0482 - accuracy: 0.9806\n",
      "Epoch 137/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0424 - accuracy: 0.9916\n",
      "Epoch 138/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0407 - accuracy: 0.9899\n",
      "Epoch 139/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0308 - accuracy: 0.9875\n",
      "Epoch 140/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0320 - accuracy: 0.9893\n",
      "Epoch 141/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0399 - accuracy: 0.9836\n",
      "Epoch 142/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0387 - accuracy: 0.9886\n",
      "Epoch 143/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0249 - accuracy: 0.9958\n",
      "Epoch 144/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0273 - accuracy: 0.9890\n",
      "Epoch 145/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0287 - accuracy: 0.9885\n",
      "Epoch 146/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0262 - accuracy: 0.9925\n",
      "Epoch 147/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0265 - accuracy: 0.9899\n",
      "Epoch 148/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0312 - accuracy: 0.9955\n",
      "Epoch 149/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0300 - accuracy: 0.9888\n",
      "Epoch 150/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0294 - accuracy: 0.9921\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000281D6B34700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\position\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 1ms/step - loss: 1.3846 - accuracy: 0.3650\n",
      "Epoch 2/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 1.3416 - accuracy: 0.5801\n",
      "Epoch 3/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 1.1065 - accuracy: 0.5577\n",
      "Epoch 4/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.8339 - accuracy: 0.5661\n",
      "Epoch 5/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.7473 - accuracy: 0.5466\n",
      "Epoch 6/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6915 - accuracy: 0.5747\n",
      "Epoch 7/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6776 - accuracy: 0.5901\n",
      "Epoch 8/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6767 - accuracy: 0.5673\n",
      "Epoch 9/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6715 - accuracy: 0.6022\n",
      "Epoch 10/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6702 - accuracy: 0.5900\n",
      "Epoch 11/150\n",
      "19/19 [==============================] - 0s 996us/step - loss: 0.6600 - accuracy: 0.6349\n",
      "Epoch 12/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6414 - accuracy: 0.6391\n",
      "Epoch 13/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.6566 - accuracy: 0.5976\n",
      "Epoch 14/150\n",
      "19/19 [==============================] - 0s 1000us/step - loss: 0.6225 - accuracy: 0.6628\n",
      "Epoch 15/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.6320 - accuracy: 0.6409\n",
      "Epoch 16/150\n",
      "19/19 [==============================] - 0s 1000us/step - loss: 0.6194 - accuracy: 0.6415\n",
      "Epoch 17/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6355 - accuracy: 0.6037\n",
      "Epoch 18/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5935 - accuracy: 0.6478\n",
      "Epoch 19/150\n",
      "19/19 [==============================] - 0s 1000us/step - loss: 0.6005 - accuracy: 0.6588\n",
      "Epoch 20/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5820 - accuracy: 0.6758\n",
      "Epoch 21/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.5666 - accuracy: 0.6874\n",
      "Epoch 22/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.5444 - accuracy: 0.7046\n",
      "Epoch 23/150\n",
      "19/19 [==============================] - 0s 992us/step - loss: 0.5402 - accuracy: 0.7220\n",
      "Epoch 24/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5122 - accuracy: 0.7399\n",
      "Epoch 25/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.5213 - accuracy: 0.7419\n",
      "Epoch 26/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.5355 - accuracy: 0.7301\n",
      "Epoch 27/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5056 - accuracy: 0.7346\n",
      "Epoch 28/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5070 - accuracy: 0.7272\n",
      "Epoch 29/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4908 - accuracy: 0.7508\n",
      "Epoch 30/150\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.5210 - accuracy: 0.75 - 0s 942us/step - loss: 0.4946 - accuracy: 0.7458\n",
      "Epoch 31/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4766 - accuracy: 0.7576\n",
      "Epoch 32/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5057 - accuracy: 0.7289\n",
      "Epoch 33/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4544 - accuracy: 0.7836\n",
      "Epoch 34/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.4393 - accuracy: 0.8033\n",
      "Epoch 35/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4351 - accuracy: 0.7668\n",
      "Epoch 36/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4365 - accuracy: 0.7674\n",
      "Epoch 37/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4016 - accuracy: 0.8112\n",
      "Epoch 38/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4290 - accuracy: 0.7928\n",
      "Epoch 39/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4243 - accuracy: 0.7921\n",
      "Epoch 40/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3866 - accuracy: 0.8290\n",
      "Epoch 41/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3998 - accuracy: 0.8016\n",
      "Epoch 42/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3930 - accuracy: 0.8154\n",
      "Epoch 43/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.3790 - accuracy: 0.8194\n",
      "Epoch 44/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3436 - accuracy: 0.8470\n",
      "Epoch 45/150\n",
      "19/19 [==============================] - 0s 999us/step - loss: 0.3658 - accuracy: 0.8253\n",
      "Epoch 46/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3460 - accuracy: 0.8314\n",
      "Epoch 47/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.3571 - accuracy: 0.8352\n",
      "Epoch 48/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3537 - accuracy: 0.8280\n",
      "Epoch 49/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3483 - accuracy: 0.8332\n",
      "Epoch 50/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3453 - accuracy: 0.8396\n",
      "Epoch 51/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.3682 - accuracy: 0.8253\n",
      "Epoch 52/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3420 - accuracy: 0.8421\n",
      "Epoch 53/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.3268 - accuracy: 0.8547\n",
      "Epoch 54/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8621\n",
      "Epoch 55/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.3062 - accuracy: 0.8586\n",
      "Epoch 56/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3226 - accuracy: 0.8565\n",
      "Epoch 57/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3172 - accuracy: 0.8601\n",
      "Epoch 58/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3338 - accuracy: 0.8485\n",
      "Epoch 59/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3228 - accuracy: 0.8436\n",
      "Epoch 60/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2933 - accuracy: 0.8727\n",
      "Epoch 61/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2639 - accuracy: 0.8884\n",
      "Epoch 62/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2842 - accuracy: 0.8773\n",
      "Epoch 63/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2843 - accuracy: 0.8878\n",
      "Epoch 64/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2826 - accuracy: 0.8724\n",
      "Epoch 65/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2772 - accuracy: 0.8694\n",
      "Epoch 66/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2842 - accuracy: 0.8811\n",
      "Epoch 67/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2800 - accuracy: 0.8789\n",
      "Epoch 68/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2302 - accuracy: 0.9042\n",
      "Epoch 69/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2615 - accuracy: 0.8930\n",
      "Epoch 70/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2331 - accuracy: 0.9148\n",
      "Epoch 71/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2804 - accuracy: 0.8775\n",
      "Epoch 72/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2853 - accuracy: 0.8803\n",
      "Epoch 73/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2651 - accuracy: 0.8885\n",
      "Epoch 74/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2716 - accuracy: 0.8750\n",
      "Epoch 75/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2566 - accuracy: 0.8804\n",
      "Epoch 76/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2580 - accuracy: 0.8882\n",
      "Epoch 77/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2521 - accuracy: 0.8955\n",
      "Epoch 78/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2419 - accuracy: 0.8950\n",
      "Epoch 79/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2537 - accuracy: 0.8801\n",
      "Epoch 80/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2298 - accuracy: 0.9042\n",
      "Epoch 81/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2288 - accuracy: 0.9025\n",
      "Epoch 82/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2433 - accuracy: 0.8837\n",
      "Epoch 83/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2026 - accuracy: 0.9163\n",
      "Epoch 84/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2261 - accuracy: 0.9012\n",
      "Epoch 85/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2517 - accuracy: 0.8786\n",
      "Epoch 86/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2261 - accuracy: 0.9058\n",
      "Epoch 87/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2303 - accuracy: 0.8973\n",
      "Epoch 88/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2308 - accuracy: 0.8853\n",
      "Epoch 89/150\n",
      "19/19 [==============================] - 0s 1000us/step - loss: 0.2244 - accuracy: 0.8795\n",
      "Epoch 90/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2277 - accuracy: 0.8869\n",
      "Epoch 91/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1980 - accuracy: 0.9269\n",
      "Epoch 92/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2042 - accuracy: 0.8966\n",
      "Epoch 93/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2041 - accuracy: 0.9072\n",
      "Epoch 94/150\n",
      "19/19 [==============================] - 0s 999us/step - loss: 0.1883 - accuracy: 0.9167\n",
      "Epoch 95/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1861 - accuracy: 0.9272\n",
      "Epoch 96/150\n",
      "19/19 [==============================] - 0s 999us/step - loss: 0.1960 - accuracy: 0.9276\n",
      "Epoch 97/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1811 - accuracy: 0.9261\n",
      "Epoch 98/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2057 - accuracy: 0.9067\n",
      "Epoch 99/150\n",
      "19/19 [==============================] - 0s 943us/step - loss: 0.1896 - accuracy: 0.9157\n",
      "Epoch 100/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2431 - accuracy: 0.8803\n",
      "Epoch 101/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1995 - accuracy: 0.9123\n",
      "Epoch 102/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1918 - accuracy: 0.9155\n",
      "Epoch 103/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1896 - accuracy: 0.9176\n",
      "Epoch 104/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1823 - accuracy: 0.9308\n",
      "Epoch 105/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1690 - accuracy: 0.9279\n",
      "Epoch 106/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1529 - accuracy: 0.9401\n",
      "Epoch 107/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1527 - accuracy: 0.9361\n",
      "Epoch 108/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1606 - accuracy: 0.9194\n",
      "Epoch 109/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1362 - accuracy: 0.9482\n",
      "Epoch 110/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1183 - accuracy: 0.9574\n",
      "Epoch 111/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1544 - accuracy: 0.9392\n",
      "Epoch 112/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1472 - accuracy: 0.9327\n",
      "Epoch 113/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1331 - accuracy: 0.9360\n",
      "Epoch 114/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1498 - accuracy: 0.9414\n",
      "Epoch 115/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1336 - accuracy: 0.9369\n",
      "Epoch 116/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1313 - accuracy: 0.9416\n",
      "Epoch 117/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1184 - accuracy: 0.9548\n",
      "Epoch 118/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1217 - accuracy: 0.9519\n",
      "Epoch 119/150\n",
      "19/19 [==============================] - 0s 999us/step - loss: 0.1088 - accuracy: 0.9616\n",
      "Epoch 120/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1066 - accuracy: 0.9473\n",
      "Epoch 121/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1550 - accuracy: 0.9312\n",
      "Epoch 122/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1685 - accuracy: 0.9304\n",
      "Epoch 123/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1105 - accuracy: 0.9515\n",
      "Epoch 124/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1074 - accuracy: 0.9587\n",
      "Epoch 125/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1084 - accuracy: 0.9534\n",
      "Epoch 126/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1026 - accuracy: 0.9575\n",
      "Epoch 127/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1204 - accuracy: 0.9414\n",
      "Epoch 128/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1245 - accuracy: 0.9508\n",
      "Epoch 129/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1186 - accuracy: 0.9484\n",
      "Epoch 130/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0971 - accuracy: 0.9602\n",
      "Epoch 131/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1015 - accuracy: 0.9602\n",
      "Epoch 132/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1074 - accuracy: 0.9604\n",
      "Epoch 133/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0750 - accuracy: 0.9708\n",
      "Epoch 134/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1047 - accuracy: 0.9452\n",
      "Epoch 135/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0885 - accuracy: 0.9608\n",
      "Epoch 136/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0932 - accuracy: 0.9517\n",
      "Epoch 137/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0818 - accuracy: 0.9648\n",
      "Epoch 138/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0912 - accuracy: 0.9713\n",
      "Epoch 139/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1041 - accuracy: 0.9676\n",
      "Epoch 140/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1046 - accuracy: 0.9628\n",
      "Epoch 141/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0791 - accuracy: 0.9727\n",
      "Epoch 142/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0833 - accuracy: 0.9661\n",
      "Epoch 143/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0762 - accuracy: 0.9626\n",
      "Epoch 144/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0866 - accuracy: 0.9688\n",
      "Epoch 145/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1100 - accuracy: 0.9582\n",
      "Epoch 146/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1200 - accuracy: 0.9643\n",
      "Epoch 147/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1009 - accuracy: 0.9537\n",
      "Epoch 148/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1481 - accuracy: 0.9381\n",
      "Epoch 149/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0805 - accuracy: 0.9672\n",
      "Epoch 150/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0759 - accuracy: 0.9638\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000281D7C06160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 999us/step\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\position\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 1ms/step - loss: 1.3849 - accuracy: 0.4219\n",
      "Epoch 2/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 1.3437 - accuracy: 0.5629\n",
      "Epoch 3/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 1.0455 - accuracy: 0.5524\n",
      "Epoch 4/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.7407 - accuracy: 0.5642\n",
      "Epoch 5/150\n",
      "19/19 [==============================] - 0s 996us/step - loss: 0.6961 - accuracy: 0.5792\n",
      "Epoch 6/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.6851 - accuracy: 0.5452\n",
      "Epoch 7/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.6784 - accuracy: 0.5723\n",
      "Epoch 8/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.6836 - accuracy: 0.5531\n",
      "Epoch 9/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.6836 - accuracy: 0.5781\n",
      "Epoch 10/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.6801 - accuracy: 0.5522\n",
      "Epoch 11/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6769 - accuracy: 0.5632\n",
      "Epoch 12/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6673 - accuracy: 0.6007\n",
      "Epoch 13/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6559 - accuracy: 0.6144\n",
      "Epoch 14/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6500 - accuracy: 0.6379\n",
      "Epoch 15/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6480 - accuracy: 0.6378\n",
      "Epoch 16/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.6435 - accuracy: 0.6106\n",
      "Epoch 17/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6131 - accuracy: 0.6508\n",
      "Epoch 18/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.6162 - accuracy: 0.6447\n",
      "Epoch 19/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6150 - accuracy: 0.6484\n",
      "Epoch 20/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.5953 - accuracy: 0.6768\n",
      "Epoch 21/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.5960 - accuracy: 0.6510\n",
      "Epoch 22/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.5912 - accuracy: 0.6626\n",
      "Epoch 23/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.5818 - accuracy: 0.6460\n",
      "Epoch 24/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.5349 - accuracy: 0.6995\n",
      "Epoch 25/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.5306 - accuracy: 0.7008\n",
      "Epoch 26/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5202 - accuracy: 0.7022\n",
      "Epoch 27/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.5114 - accuracy: 0.7142\n",
      "Epoch 28/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5028 - accuracy: 0.7400\n",
      "Epoch 29/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5155 - accuracy: 0.7200\n",
      "Epoch 30/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4810 - accuracy: 0.7249\n",
      "Epoch 31/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.4575 - accuracy: 0.7712\n",
      "Epoch 32/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.4466 - accuracy: 0.7635\n",
      "Epoch 33/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4588 - accuracy: 0.7616\n",
      "Epoch 34/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.4573 - accuracy: 0.7396\n",
      "Epoch 35/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4593 - accuracy: 0.7604\n",
      "Epoch 36/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4052 - accuracy: 0.8047\n",
      "Epoch 37/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.4357 - accuracy: 0.7620\n",
      "Epoch 38/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4196 - accuracy: 0.7804\n",
      "Epoch 39/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4018 - accuracy: 0.8022\n",
      "Epoch 40/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.3686 - accuracy: 0.8122\n",
      "Epoch 41/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4022 - accuracy: 0.7922\n",
      "Epoch 42/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.3513 - accuracy: 0.8107\n",
      "Epoch 43/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3643 - accuracy: 0.8118\n",
      "Epoch 44/150\n",
      "19/19 [==============================] - 0s 1000us/step - loss: 0.3575 - accuracy: 0.8138\n",
      "Epoch 45/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3604 - accuracy: 0.7981\n",
      "Epoch 46/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3456 - accuracy: 0.8128\n",
      "Epoch 47/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3467 - accuracy: 0.8118\n",
      "Epoch 48/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3338 - accuracy: 0.8376\n",
      "Epoch 49/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3304 - accuracy: 0.8294\n",
      "Epoch 50/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3538 - accuracy: 0.7912\n",
      "Epoch 51/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3063 - accuracy: 0.8408\n",
      "Epoch 52/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3150 - accuracy: 0.8293\n",
      "Epoch 53/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3134 - accuracy: 0.8279\n",
      "Epoch 54/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2976 - accuracy: 0.8573\n",
      "Epoch 55/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2798 - accuracy: 0.8673\n",
      "Epoch 56/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2770 - accuracy: 0.8531\n",
      "Epoch 57/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3032 - accuracy: 0.8518\n",
      "Epoch 58/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2861 - accuracy: 0.8584\n",
      "Epoch 59/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2770 - accuracy: 0.8506\n",
      "Epoch 60/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2568 - accuracy: 0.8672\n",
      "Epoch 61/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2356 - accuracy: 0.8894\n",
      "Epoch 62/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2868 - accuracy: 0.8573\n",
      "Epoch 63/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2592 - accuracy: 0.8739\n",
      "Epoch 64/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2775 - accuracy: 0.8526\n",
      "Epoch 65/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2434 - accuracy: 0.8596\n",
      "Epoch 66/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2386 - accuracy: 0.8969\n",
      "Epoch 67/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2268 - accuracy: 0.8856\n",
      "Epoch 68/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2656 - accuracy: 0.8563\n",
      "Epoch 69/150\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2445 - accuracy: 0.8731\n",
      "Epoch 70/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2235 - accuracy: 0.8902\n",
      "Epoch 71/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2142 - accuracy: 0.9128\n",
      "Epoch 72/150\n",
      "19/19 [==============================] - 0s 995us/step - loss: 0.2202 - accuracy: 0.8953\n",
      "Epoch 73/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2386 - accuracy: 0.8895\n",
      "Epoch 74/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1893 - accuracy: 0.9045\n",
      "Epoch 75/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2040 - accuracy: 0.9134\n",
      "Epoch 76/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2182 - accuracy: 0.8999\n",
      "Epoch 77/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1965 - accuracy: 0.9182\n",
      "Epoch 78/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1994 - accuracy: 0.9100\n",
      "Epoch 79/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2123 - accuracy: 0.9065\n",
      "Epoch 80/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1889 - accuracy: 0.9028\n",
      "Epoch 81/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1899 - accuracy: 0.9033\n",
      "Epoch 82/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2003 - accuracy: 0.9098\n",
      "Epoch 83/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1914 - accuracy: 0.9203\n",
      "Epoch 84/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1766 - accuracy: 0.9289\n",
      "Epoch 85/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2095 - accuracy: 0.8812\n",
      "Epoch 86/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1693 - accuracy: 0.9356\n",
      "Epoch 87/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1555 - accuracy: 0.9294\n",
      "Epoch 88/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1731 - accuracy: 0.9334\n",
      "Epoch 89/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2016 - accuracy: 0.9121\n",
      "Epoch 90/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1781 - accuracy: 0.9270\n",
      "Epoch 91/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1693 - accuracy: 0.9185\n",
      "Epoch 92/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1825 - accuracy: 0.9359\n",
      "Epoch 93/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1572 - accuracy: 0.9206\n",
      "Epoch 94/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1632 - accuracy: 0.9164\n",
      "Epoch 95/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1749 - accuracy: 0.9176\n",
      "Epoch 96/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1764 - accuracy: 0.9232\n",
      "Epoch 97/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1516 - accuracy: 0.9283\n",
      "Epoch 98/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1432 - accuracy: 0.9489\n",
      "Epoch 99/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1473 - accuracy: 0.9504\n",
      "Epoch 100/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1764 - accuracy: 0.9365\n",
      "Epoch 101/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1560 - accuracy: 0.9331\n",
      "Epoch 102/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1355 - accuracy: 0.9470\n",
      "Epoch 103/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1604 - accuracy: 0.9304\n",
      "Epoch 104/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1455 - accuracy: 0.9278\n",
      "Epoch 105/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1383 - accuracy: 0.9412\n",
      "Epoch 106/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1290 - accuracy: 0.9596\n",
      "Epoch 107/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1221 - accuracy: 0.9568\n",
      "Epoch 108/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1341 - accuracy: 0.9531\n",
      "Epoch 109/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1488 - accuracy: 0.9350\n",
      "Epoch 110/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1240 - accuracy: 0.9531\n",
      "Epoch 111/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1319 - accuracy: 0.9441\n",
      "Epoch 112/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1449 - accuracy: 0.9361\n",
      "Epoch 113/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1242 - accuracy: 0.9487\n",
      "Epoch 114/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1244 - accuracy: 0.9538\n",
      "Epoch 115/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1317 - accuracy: 0.9441\n",
      "Epoch 116/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1559 - accuracy: 0.9353\n",
      "Epoch 117/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1240 - accuracy: 0.9428\n",
      "Epoch 118/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1572 - accuracy: 0.9300\n",
      "Epoch 119/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1219 - accuracy: 0.9567\n",
      "Epoch 120/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1386 - accuracy: 0.9473\n",
      "Epoch 121/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1144 - accuracy: 0.9675\n",
      "Epoch 122/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1208 - accuracy: 0.9535\n",
      "Epoch 123/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1185 - accuracy: 0.9522\n",
      "Epoch 124/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1117 - accuracy: 0.9559\n",
      "Epoch 125/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.1095 - accuracy: 0.9448\n",
      "Epoch 126/150\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.1226 - accuracy: 0.9486\n",
      "Epoch 127/150\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.1097 - accuracy: 0.9543\n",
      "Epoch 128/150\n",
      "19/19 [==============================] - 0s 886us/step - loss: 0.1052 - accuracy: 0.9518\n",
      "Epoch 129/150\n",
      "19/19 [==============================] - 0s 953us/step - loss: 0.1020 - accuracy: 0.9594\n",
      "Epoch 130/150\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.1032 - accuracy: 0.9540\n",
      "Epoch 131/150\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.0950 - accuracy: 0.9630\n",
      "Epoch 132/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1166 - accuracy: 0.9616\n",
      "Epoch 133/150\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.1137 - accuracy: 0.9505\n",
      "Epoch 134/150\n",
      "19/19 [==============================] - 0s 945us/step - loss: 0.1087 - accuracy: 0.9545\n",
      "Epoch 135/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1054 - accuracy: 0.9640\n",
      "Epoch 136/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1221 - accuracy: 0.9455\n",
      "Epoch 137/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1084 - accuracy: 0.9542\n",
      "Epoch 138/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1211 - accuracy: 0.9393\n",
      "Epoch 139/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0952 - accuracy: 0.9573\n",
      "Epoch 140/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0987 - accuracy: 0.9624\n",
      "Epoch 141/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0908 - accuracy: 0.9655\n",
      "Epoch 142/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0999 - accuracy: 0.9629\n",
      "Epoch 143/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0996 - accuracy: 0.9633\n",
      "Epoch 144/150\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.0787 - accuracy: 0.9664\n",
      "Epoch 145/150\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.0927 - accuracy: 0.9659\n",
      "Epoch 146/150\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.0848 - accuracy: 0.9661\n",
      "Epoch 147/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0925 - accuracy: 0.9668\n",
      "Epoch 148/150\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.0779 - accuracy: 0.9743\n",
      "Epoch 149/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0949 - accuracy: 0.9678\n",
      "Epoch 150/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0721 - accuracy: 0.9768\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000281D7CB3E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 998us/step\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\position\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 1ms/step - loss: 1.3837 - accuracy: 0.4395\n",
      "Epoch 2/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 1.3207 - accuracy: 0.5712\n",
      "Epoch 3/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 1.0359 - accuracy: 0.5791\n",
      "Epoch 4/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.7946 - accuracy: 0.5360\n",
      "Epoch 5/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6966 - accuracy: 0.5583\n",
      "Epoch 6/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6875 - accuracy: 0.5767\n",
      "Epoch 7/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6795 - accuracy: 0.5801\n",
      "Epoch 8/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6848 - accuracy: 0.5733\n",
      "Epoch 9/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6693 - accuracy: 0.5985\n",
      "Epoch 10/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6788 - accuracy: 0.5766\n",
      "Epoch 11/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6835 - accuracy: 0.5492\n",
      "Epoch 12/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6771 - accuracy: 0.5723\n",
      "Epoch 13/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6807 - accuracy: 0.5608\n",
      "Epoch 14/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6592 - accuracy: 0.5814\n",
      "Epoch 15/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6571 - accuracy: 0.5920\n",
      "Epoch 16/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6497 - accuracy: 0.5847\n",
      "Epoch 17/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6281 - accuracy: 0.6509\n",
      "Epoch 18/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6158 - accuracy: 0.6670\n",
      "Epoch 19/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5960 - accuracy: 0.6363\n",
      "Epoch 20/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6193 - accuracy: 0.6402\n",
      "Epoch 21/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5747 - accuracy: 0.6754\n",
      "Epoch 22/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5627 - accuracy: 0.6824\n",
      "Epoch 23/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5590 - accuracy: 0.7044\n",
      "Epoch 24/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5465 - accuracy: 0.7155\n",
      "Epoch 25/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5381 - accuracy: 0.6986\n",
      "Epoch 26/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5529 - accuracy: 0.7061\n",
      "Epoch 27/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5566 - accuracy: 0.6819\n",
      "Epoch 28/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5083 - accuracy: 0.7289\n",
      "Epoch 29/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5499 - accuracy: 0.7155\n",
      "Epoch 30/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4764 - accuracy: 0.7679\n",
      "Epoch 31/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4871 - accuracy: 0.7354\n",
      "Epoch 32/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4805 - accuracy: 0.7381\n",
      "Epoch 33/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.4643 - accuracy: 0.7921\n",
      "Epoch 34/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4690 - accuracy: 0.7567\n",
      "Epoch 35/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4680 - accuracy: 0.7690\n",
      "Epoch 36/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4289 - accuracy: 0.7964\n",
      "Epoch 37/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4409 - accuracy: 0.7939\n",
      "Epoch 38/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3950 - accuracy: 0.8107\n",
      "Epoch 39/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4269 - accuracy: 0.7964\n",
      "Epoch 40/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3955 - accuracy: 0.8159\n",
      "Epoch 41/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3991 - accuracy: 0.8006\n",
      "Epoch 42/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3778 - accuracy: 0.8362\n",
      "Epoch 43/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4100 - accuracy: 0.7862\n",
      "Epoch 44/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.3804 - accuracy: 0.8338\n",
      "Epoch 45/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3478 - accuracy: 0.8360\n",
      "Epoch 46/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3739 - accuracy: 0.8276\n",
      "Epoch 47/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.3641 - accuracy: 0.8402\n",
      "Epoch 48/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.3493 - accuracy: 0.8244\n",
      "Epoch 49/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3580 - accuracy: 0.8242\n",
      "Epoch 50/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3331 - accuracy: 0.8353\n",
      "Epoch 51/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3394 - accuracy: 0.8364\n",
      "Epoch 52/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3208 - accuracy: 0.8265\n",
      "Epoch 53/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3005 - accuracy: 0.8596\n",
      "Epoch 54/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3600 - accuracy: 0.8401\n",
      "Epoch 55/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3221 - accuracy: 0.8385\n",
      "Epoch 56/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3106 - accuracy: 0.8605\n",
      "Epoch 57/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3037 - accuracy: 0.8567\n",
      "Epoch 58/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8525\n",
      "Epoch 59/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2920 - accuracy: 0.8690\n",
      "Epoch 60/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2813 - accuracy: 0.8667\n",
      "Epoch 61/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3142 - accuracy: 0.8456\n",
      "Epoch 62/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3183 - accuracy: 0.8529\n",
      "Epoch 63/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2852 - accuracy: 0.8565\n",
      "Epoch 64/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2955 - accuracy: 0.8485\n",
      "Epoch 65/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2779 - accuracy: 0.8638\n",
      "Epoch 66/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2815 - accuracy: 0.8528\n",
      "Epoch 67/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2637 - accuracy: 0.8668\n",
      "Epoch 68/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2638 - accuracy: 0.8807\n",
      "Epoch 69/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2752 - accuracy: 0.8620\n",
      "Epoch 70/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2477 - accuracy: 0.8942\n",
      "Epoch 71/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2651 - accuracy: 0.8787\n",
      "Epoch 72/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2646 - accuracy: 0.8811\n",
      "Epoch 73/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2604 - accuracy: 0.8759\n",
      "Epoch 74/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2721 - accuracy: 0.8675\n",
      "Epoch 75/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2376 - accuracy: 0.8909\n",
      "Epoch 76/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2485 - accuracy: 0.8829\n",
      "Epoch 77/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2109 - accuracy: 0.9079\n",
      "Epoch 78/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2403 - accuracy: 0.9021\n",
      "Epoch 79/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2072 - accuracy: 0.9020\n",
      "Epoch 80/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2367 - accuracy: 0.8885\n",
      "Epoch 81/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2043 - accuracy: 0.9206\n",
      "Epoch 82/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2265 - accuracy: 0.8920\n",
      "Epoch 83/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2093 - accuracy: 0.9013\n",
      "Epoch 84/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2033 - accuracy: 0.9078\n",
      "Epoch 85/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1919 - accuracy: 0.9058\n",
      "Epoch 86/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2167 - accuracy: 0.8987\n",
      "Epoch 87/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2181 - accuracy: 0.8897\n",
      "Epoch 88/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.2106 - accuracy: 0.9085\n",
      "Epoch 89/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1727 - accuracy: 0.9114\n",
      "Epoch 90/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1955 - accuracy: 0.9168\n",
      "Epoch 91/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1914 - accuracy: 0.9055\n",
      "Epoch 92/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1929 - accuracy: 0.9054\n",
      "Epoch 93/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2112 - accuracy: 0.9131\n",
      "Epoch 94/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1720 - accuracy: 0.9321\n",
      "Epoch 95/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1739 - accuracy: 0.9264\n",
      "Epoch 96/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.1834 - accuracy: 0.9169\n",
      "Epoch 97/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1824 - accuracy: 0.9120\n",
      "Epoch 98/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1807 - accuracy: 0.9045\n",
      "Epoch 99/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2031 - accuracy: 0.9069\n",
      "Epoch 100/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1700 - accuracy: 0.9215\n",
      "Epoch 101/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1519 - accuracy: 0.9251\n",
      "Epoch 102/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1647 - accuracy: 0.9346\n",
      "Epoch 103/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1864 - accuracy: 0.9179\n",
      "Epoch 104/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1685 - accuracy: 0.9238\n",
      "Epoch 105/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1577 - accuracy: 0.9242\n",
      "Epoch 106/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1498 - accuracy: 0.9525\n",
      "Epoch 107/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1676 - accuracy: 0.9421\n",
      "Epoch 108/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1423 - accuracy: 0.9375\n",
      "Epoch 109/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1574 - accuracy: 0.9333\n",
      "Epoch 110/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1341 - accuracy: 0.9431\n",
      "Epoch 111/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1592 - accuracy: 0.9338\n",
      "Epoch 112/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1307 - accuracy: 0.9424\n",
      "Epoch 113/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1642 - accuracy: 0.9282\n",
      "Epoch 114/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1267 - accuracy: 0.9510\n",
      "Epoch 115/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1346 - accuracy: 0.9316\n",
      "Epoch 116/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1287 - accuracy: 0.9409\n",
      "Epoch 117/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1345 - accuracy: 0.9302\n",
      "Epoch 118/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1405 - accuracy: 0.9435\n",
      "Epoch 119/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1191 - accuracy: 0.9469\n",
      "Epoch 120/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1298 - accuracy: 0.9481\n",
      "Epoch 121/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.1024 - accuracy: 0.9635\n",
      "Epoch 122/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1504 - accuracy: 0.9457\n",
      "Epoch 123/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1222 - accuracy: 0.9384\n",
      "Epoch 124/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1102 - accuracy: 0.9524\n",
      "Epoch 125/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1725 - accuracy: 0.9199\n",
      "Epoch 126/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1008 - accuracy: 0.9612\n",
      "Epoch 127/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1221 - accuracy: 0.9530\n",
      "Epoch 128/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1127 - accuracy: 0.9482\n",
      "Epoch 129/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1079 - accuracy: 0.9615\n",
      "Epoch 130/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0894 - accuracy: 0.9755\n",
      "Epoch 131/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1114 - accuracy: 0.9497\n",
      "Epoch 132/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0826 - accuracy: 0.9723\n",
      "Epoch 133/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0909 - accuracy: 0.9661\n",
      "Epoch 134/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0857 - accuracy: 0.9602\n",
      "Epoch 135/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0854 - accuracy: 0.9750\n",
      "Epoch 136/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0743 - accuracy: 0.9786\n",
      "Epoch 137/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0716 - accuracy: 0.9802\n",
      "Epoch 138/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0967 - accuracy: 0.9505\n",
      "Epoch 139/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0677 - accuracy: 0.9770\n",
      "Epoch 140/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0696 - accuracy: 0.9749\n",
      "Epoch 141/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0793 - accuracy: 0.9702\n",
      "Epoch 142/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0668 - accuracy: 0.9757\n",
      "Epoch 143/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0583 - accuracy: 0.9794\n",
      "Epoch 144/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0687 - accuracy: 0.9751\n",
      "Epoch 145/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0635 - accuracy: 0.9737\n",
      "Epoch 146/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0677 - accuracy: 0.9738\n",
      "Epoch 147/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0713 - accuracy: 0.9689\n",
      "Epoch 148/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0496 - accuracy: 0.9837\n",
      "Epoch 149/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0554 - accuracy: 0.9823\n",
      "Epoch 150/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0731 - accuracy: 0.9740\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000281D8DF9310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 959us/step\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\position\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 1ms/step - loss: 1.3837 - accuracy: 0.4257\n",
      "Epoch 2/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 1.3220 - accuracy: 0.5857\n",
      "Epoch 3/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.9876 - accuracy: 0.5334\n",
      "Epoch 4/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.7418 - accuracy: 0.5617\n",
      "Epoch 5/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6820 - accuracy: 0.5897\n",
      "Epoch 6/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6812 - accuracy: 0.5703\n",
      "Epoch 7/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6868 - accuracy: 0.5843\n",
      "Epoch 8/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6900 - accuracy: 0.5591\n",
      "Epoch 9/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6704 - accuracy: 0.5936\n",
      "Epoch 10/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6781 - accuracy: 0.5847\n",
      "Epoch 11/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6689 - accuracy: 0.5854\n",
      "Epoch 12/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6585 - accuracy: 0.6226\n",
      "Epoch 13/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6533 - accuracy: 0.6118\n",
      "Epoch 14/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6672 - accuracy: 0.5868\n",
      "Epoch 15/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6248 - accuracy: 0.6497\n",
      "Epoch 16/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6394 - accuracy: 0.6368\n",
      "Epoch 17/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6121 - accuracy: 0.6511\n",
      "Epoch 18/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6028 - accuracy: 0.6516\n",
      "Epoch 19/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5912 - accuracy: 0.6776\n",
      "Epoch 20/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6088 - accuracy: 0.6477\n",
      "Epoch 21/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5879 - accuracy: 0.6773\n",
      "Epoch 22/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5686 - accuracy: 0.6894\n",
      "Epoch 23/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5566 - accuracy: 0.7023\n",
      "Epoch 24/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5568 - accuracy: 0.7048\n",
      "Epoch 25/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5455 - accuracy: 0.7121\n",
      "Epoch 26/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5142 - accuracy: 0.7279\n",
      "Epoch 27/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5106 - accuracy: 0.7259\n",
      "Epoch 28/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4980 - accuracy: 0.7381\n",
      "Epoch 29/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4739 - accuracy: 0.7629\n",
      "Epoch 30/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4852 - accuracy: 0.7370\n",
      "Epoch 31/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5008 - accuracy: 0.7228\n",
      "Epoch 32/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4585 - accuracy: 0.7722\n",
      "Epoch 33/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4648 - accuracy: 0.7841\n",
      "Epoch 34/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4584 - accuracy: 0.7704\n",
      "Epoch 35/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4344 - accuracy: 0.7982\n",
      "Epoch 36/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4225 - accuracy: 0.7892\n",
      "Epoch 37/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4285 - accuracy: 0.7961\n",
      "Epoch 38/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.3886 - accuracy: 0.8074\n",
      "Epoch 39/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3708 - accuracy: 0.8053\n",
      "Epoch 40/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.4351 - accuracy: 0.7718\n",
      "Epoch 41/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3836 - accuracy: 0.8140\n",
      "Epoch 42/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3333 - accuracy: 0.8287\n",
      "Epoch 43/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.3678 - accuracy: 0.8186\n",
      "Epoch 44/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.3429 - accuracy: 0.8252\n",
      "Epoch 45/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.3103 - accuracy: 0.8541\n",
      "Epoch 46/150\n",
      "19/19 [==============================] - 0s 994us/step - loss: 0.3268 - accuracy: 0.8472\n",
      "Epoch 47/150\n",
      "19/19 [==============================] - 0s 999us/step - loss: 0.3480 - accuracy: 0.8208\n",
      "Epoch 48/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3036 - accuracy: 0.8424\n",
      "Epoch 49/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.2942 - accuracy: 0.8382\n",
      "Epoch 50/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2954 - accuracy: 0.8734\n",
      "Epoch 51/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2956 - accuracy: 0.8601\n",
      "Epoch 52/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2844 - accuracy: 0.8560\n",
      "Epoch 53/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2642 - accuracy: 0.8607\n",
      "Epoch 54/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2585 - accuracy: 0.8790\n",
      "Epoch 55/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2742 - accuracy: 0.8555\n",
      "Epoch 56/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2793 - accuracy: 0.8561\n",
      "Epoch 57/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2942 - accuracy: 0.8591\n",
      "Epoch 58/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2494 - accuracy: 0.8748\n",
      "Epoch 59/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2317 - accuracy: 0.8877\n",
      "Epoch 60/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2529 - accuracy: 0.8741\n",
      "Epoch 61/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2558 - accuracy: 0.8778\n",
      "Epoch 62/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2636 - accuracy: 0.8615\n",
      "Epoch 63/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2432 - accuracy: 0.8784\n",
      "Epoch 64/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2300 - accuracy: 0.8918\n",
      "Epoch 65/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2279 - accuracy: 0.8999\n",
      "Epoch 66/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2145 - accuracy: 0.8960\n",
      "Epoch 67/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2145 - accuracy: 0.8960\n",
      "Epoch 68/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2732 - accuracy: 0.8435\n",
      "Epoch 69/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2309 - accuracy: 0.8823\n",
      "Epoch 70/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2364 - accuracy: 0.8791\n",
      "Epoch 71/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2439 - accuracy: 0.8733\n",
      "Epoch 72/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2702 - accuracy: 0.8672\n",
      "Epoch 73/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2204 - accuracy: 0.8979\n",
      "Epoch 74/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1877 - accuracy: 0.9251\n",
      "Epoch 75/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1887 - accuracy: 0.9267\n",
      "Epoch 76/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.2097 - accuracy: 0.9017\n",
      "Epoch 77/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2157 - accuracy: 0.8923\n",
      "Epoch 78/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2247 - accuracy: 0.8962\n",
      "Epoch 79/150\n",
      "19/19 [==============================] - 0s 1000us/step - loss: 0.2133 - accuracy: 0.8926\n",
      "Epoch 80/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1933 - accuracy: 0.9131\n",
      "Epoch 81/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2116 - accuracy: 0.9153\n",
      "Epoch 82/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 997us/step - loss: 0.1947 - accuracy: 0.9056\n",
      "Epoch 83/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1874 - accuracy: 0.9243\n",
      "Epoch 84/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1994 - accuracy: 0.9120\n",
      "Epoch 85/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1943 - accuracy: 0.9223\n",
      "Epoch 86/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.2196 - accuracy: 0.9019\n",
      "Epoch 87/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1795 - accuracy: 0.9245\n",
      "Epoch 88/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1750 - accuracy: 0.9145\n",
      "Epoch 89/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1902 - accuracy: 0.9085\n",
      "Epoch 90/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2002 - accuracy: 0.9026\n",
      "Epoch 91/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1847 - accuracy: 0.8997\n",
      "Epoch 92/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1953 - accuracy: 0.9266\n",
      "Epoch 93/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2003 - accuracy: 0.9013\n",
      "Epoch 94/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1874 - accuracy: 0.9011\n",
      "Epoch 95/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1657 - accuracy: 0.9380\n",
      "Epoch 96/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1617 - accuracy: 0.9382\n",
      "Epoch 97/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1533 - accuracy: 0.9172\n",
      "Epoch 98/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1904 - accuracy: 0.9088\n",
      "Epoch 99/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1645 - accuracy: 0.9327\n",
      "Epoch 100/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1811 - accuracy: 0.9106\n",
      "Epoch 101/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1719 - accuracy: 0.9193\n",
      "Epoch 102/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1737 - accuracy: 0.9121\n",
      "Epoch 103/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1954 - accuracy: 0.9148\n",
      "Epoch 104/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1611 - accuracy: 0.9397\n",
      "Epoch 105/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1642 - accuracy: 0.9314\n",
      "Epoch 106/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1781 - accuracy: 0.9002\n",
      "Epoch 107/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1621 - accuracy: 0.9332\n",
      "Epoch 108/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1573 - accuracy: 0.9237\n",
      "Epoch 109/150\n",
      "19/19 [==============================] - 0s 1000us/step - loss: 0.1362 - accuracy: 0.9442\n",
      "Epoch 110/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1458 - accuracy: 0.9218\n",
      "Epoch 111/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1379 - accuracy: 0.9382\n",
      "Epoch 112/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1327 - accuracy: 0.9452\n",
      "Epoch 113/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1546 - accuracy: 0.9244\n",
      "Epoch 114/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1280 - accuracy: 0.9503\n",
      "Epoch 115/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1856 - accuracy: 0.9095\n",
      "Epoch 116/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1422 - accuracy: 0.9273\n",
      "Epoch 117/150\n",
      "19/19 [==============================] - 0s 1000us/step - loss: 0.1299 - accuracy: 0.9482\n",
      "Epoch 118/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1388 - accuracy: 0.9393\n",
      "Epoch 119/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1666 - accuracy: 0.9293\n",
      "Epoch 120/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1690 - accuracy: 0.9346\n",
      "Epoch 121/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1274 - accuracy: 0.9408\n",
      "Epoch 122/150\n",
      "19/19 [==============================] - 0s 1000us/step - loss: 0.1359 - accuracy: 0.9432\n",
      "Epoch 123/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1533 - accuracy: 0.9359\n",
      "Epoch 124/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1229 - accuracy: 0.9489\n",
      "Epoch 125/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1173 - accuracy: 0.9424\n",
      "Epoch 126/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1343 - accuracy: 0.9489\n",
      "Epoch 127/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1280 - accuracy: 0.9481\n",
      "Epoch 128/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1216 - accuracy: 0.9600\n",
      "Epoch 129/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.9467\n",
      "Epoch 130/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1018 - accuracy: 0.9652\n",
      "Epoch 131/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1078 - accuracy: 0.9649\n",
      "Epoch 132/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1843 - accuracy: 0.9233\n",
      "Epoch 133/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1092 - accuracy: 0.9569\n",
      "Epoch 134/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1155 - accuracy: 0.9479\n",
      "Epoch 135/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1102 - accuracy: 0.9616\n",
      "Epoch 136/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1371 - accuracy: 0.9494\n",
      "Epoch 137/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1461 - accuracy: 0.9385\n",
      "Epoch 138/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1363 - accuracy: 0.9560\n",
      "Epoch 139/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1326 - accuracy: 0.9419\n",
      "Epoch 140/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.1167 - accuracy: 0.9640\n",
      "Epoch 141/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1419 - accuracy: 0.9551\n",
      "Epoch 142/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1180 - accuracy: 0.9563\n",
      "Epoch 143/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1280 - accuracy: 0.9483\n",
      "Epoch 144/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0916 - accuracy: 0.9756\n",
      "Epoch 145/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0954 - accuracy: 0.9621\n",
      "Epoch 146/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0925 - accuracy: 0.9763\n",
      "Epoch 147/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0769 - accuracy: 0.9783\n",
      "Epoch 148/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1047 - accuracy: 0.9662\n",
      "Epoch 149/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1046 - accuracy: 0.9639\n",
      "Epoch 150/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1079 - accuracy: 0.9607\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000281D8EDF3A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\position\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 1.3846 - accuracy: 0.3270\n",
      "Epoch 2/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 1.3409 - accuracy: 0.5110\n",
      "Epoch 3/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 1.0444 - accuracy: 0.5489\n",
      "Epoch 4/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.7409 - accuracy: 0.5856\n",
      "Epoch 5/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6968 - accuracy: 0.5583\n",
      "Epoch 6/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6868 - accuracy: 0.5691\n",
      "Epoch 7/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.7015 - accuracy: 0.5357\n",
      "Epoch 8/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6896 - accuracy: 0.5610\n",
      "Epoch 9/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5307\n",
      "Epoch 10/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6853 - accuracy: 0.5531\n",
      "Epoch 11/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6515 - accuracy: 0.5986\n",
      "Epoch 12/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6646 - accuracy: 0.6082\n",
      "Epoch 13/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6634 - accuracy: 0.6103\n",
      "Epoch 14/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6774 - accuracy: 0.5735\n",
      "Epoch 15/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6675 - accuracy: 0.5835\n",
      "Epoch 16/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6646 - accuracy: 0.6171\n",
      "Epoch 17/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6459 - accuracy: 0.6348\n",
      "Epoch 18/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6504 - accuracy: 0.6122\n",
      "Epoch 19/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6345 - accuracy: 0.6355\n",
      "Epoch 20/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6244 - accuracy: 0.6361\n",
      "Epoch 21/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6161 - accuracy: 0.6380\n",
      "Epoch 22/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5939 - accuracy: 0.6637\n",
      "Epoch 23/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5848 - accuracy: 0.6726\n",
      "Epoch 24/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5963 - accuracy: 0.6627\n",
      "Epoch 25/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5661 - accuracy: 0.6813\n",
      "Epoch 26/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5393 - accuracy: 0.6947\n",
      "Epoch 27/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5365 - accuracy: 0.7058\n",
      "Epoch 28/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5106 - accuracy: 0.7532\n",
      "Epoch 29/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4900 - accuracy: 0.7448\n",
      "Epoch 30/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4875 - accuracy: 0.7587\n",
      "Epoch 31/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5103 - accuracy: 0.7153\n",
      "Epoch 32/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4592 - accuracy: 0.7553\n",
      "Epoch 33/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4777 - accuracy: 0.7318\n",
      "Epoch 34/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4356 - accuracy: 0.7627\n",
      "Epoch 35/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4113 - accuracy: 0.8022\n",
      "Epoch 36/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4268 - accuracy: 0.7611\n",
      "Epoch 37/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4378 - accuracy: 0.7497\n",
      "Epoch 38/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3727 - accuracy: 0.8019\n",
      "Epoch 39/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4252 - accuracy: 0.7864\n",
      "Epoch 40/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4059 - accuracy: 0.7791\n",
      "Epoch 41/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3900 - accuracy: 0.8130\n",
      "Epoch 42/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3668 - accuracy: 0.7989\n",
      "Epoch 43/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3702 - accuracy: 0.8092\n",
      "Epoch 44/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3506 - accuracy: 0.8128\n",
      "Epoch 45/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3708 - accuracy: 0.8062\n",
      "Epoch 46/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3890 - accuracy: 0.7904\n",
      "Epoch 47/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3428 - accuracy: 0.8288\n",
      "Epoch 48/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3700 - accuracy: 0.8228\n",
      "Epoch 49/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3494 - accuracy: 0.8072\n",
      "Epoch 50/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3423 - accuracy: 0.8435\n",
      "Epoch 51/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3137 - accuracy: 0.8391\n",
      "Epoch 52/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3312 - accuracy: 0.8351\n",
      "Epoch 53/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3398 - accuracy: 0.8362\n",
      "Epoch 54/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.3084 - accuracy: 0.8532\n",
      "Epoch 55/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3207 - accuracy: 0.8270\n",
      "Epoch 56/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2920 - accuracy: 0.8808\n",
      "Epoch 57/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3014 - accuracy: 0.8663\n",
      "Epoch 58/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2966 - accuracy: 0.8311\n",
      "Epoch 59/150\n",
      "19/19 [==============================] - 0s 999us/step - loss: 0.2901 - accuracy: 0.8609\n",
      "Epoch 60/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2845 - accuracy: 0.8584\n",
      "Epoch 61/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2970 - accuracy: 0.8698\n",
      "Epoch 62/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2967 - accuracy: 0.8436\n",
      "Epoch 63/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2631 - accuracy: 0.8727\n",
      "Epoch 64/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2696 - accuracy: 0.8910\n",
      "Epoch 65/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2727 - accuracy: 0.8749\n",
      "Epoch 66/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2792 - accuracy: 0.8573\n",
      "Epoch 67/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2501 - accuracy: 0.9000\n",
      "Epoch 68/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2542 - accuracy: 0.8843\n",
      "Epoch 69/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2326 - accuracy: 0.9120\n",
      "Epoch 70/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2591 - accuracy: 0.8793\n",
      "Epoch 71/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2605 - accuracy: 0.8819\n",
      "Epoch 72/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2428 - accuracy: 0.9004\n",
      "Epoch 73/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2282 - accuracy: 0.9086\n",
      "Epoch 74/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2416 - accuracy: 0.8964\n",
      "Epoch 75/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2402 - accuracy: 0.8891\n",
      "Epoch 76/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2415 - accuracy: 0.8855\n",
      "Epoch 77/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.2459 - accuracy: 0.8982\n",
      "Epoch 78/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2080 - accuracy: 0.9138\n",
      "Epoch 79/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2326 - accuracy: 0.8939\n",
      "Epoch 80/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2456 - accuracy: 0.8928\n",
      "Epoch 81/150\n",
      "19/19 [==============================] - 0s 998us/step - loss: 0.2187 - accuracy: 0.8964\n",
      "Epoch 82/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2372 - accuracy: 0.8920\n",
      "Epoch 83/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2089 - accuracy: 0.9155\n",
      "Epoch 84/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2003 - accuracy: 0.9253\n",
      "Epoch 85/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2149 - accuracy: 0.9130\n",
      "Epoch 86/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1979 - accuracy: 0.9211\n",
      "Epoch 87/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1832 - accuracy: 0.9271\n",
      "Epoch 88/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1878 - accuracy: 0.9297\n",
      "Epoch 89/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1972 - accuracy: 0.9246\n",
      "Epoch 90/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1801 - accuracy: 0.9292\n",
      "Epoch 91/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1801 - accuracy: 0.9183\n",
      "Epoch 92/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1832 - accuracy: 0.9242\n",
      "Epoch 93/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1594 - accuracy: 0.9301\n",
      "Epoch 94/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1677 - accuracy: 0.9360\n",
      "Epoch 95/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1754 - accuracy: 0.9271\n",
      "Epoch 96/150\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.1665 - accuracy: 0.9264\n",
      "Epoch 97/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1423 - accuracy: 0.9437\n",
      "Epoch 98/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1465 - accuracy: 0.9511\n",
      "Epoch 99/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1430 - accuracy: 0.9477\n",
      "Epoch 100/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1544 - accuracy: 0.9379\n",
      "Epoch 101/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1310 - accuracy: 0.9538\n",
      "Epoch 102/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1409 - accuracy: 0.9525\n",
      "Epoch 103/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.1147 - accuracy: 0.9644\n",
      "Epoch 104/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1279 - accuracy: 0.9618\n",
      "Epoch 105/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1253 - accuracy: 0.9542\n",
      "Epoch 106/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1218 - accuracy: 0.9630\n",
      "Epoch 107/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1288 - accuracy: 0.9609\n",
      "Epoch 108/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0986 - accuracy: 0.9740\n",
      "Epoch 109/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1218 - accuracy: 0.9557\n",
      "Epoch 110/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1050 - accuracy: 0.9596\n",
      "Epoch 111/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1010 - accuracy: 0.9655\n",
      "Epoch 112/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1263 - accuracy: 0.9502\n",
      "Epoch 113/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1160 - accuracy: 0.9554\n",
      "Epoch 114/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0951 - accuracy: 0.9757\n",
      "Epoch 115/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0931 - accuracy: 0.9714\n",
      "Epoch 116/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0858 - accuracy: 0.9759\n",
      "Epoch 117/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1019 - accuracy: 0.9642\n",
      "Epoch 118/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0800 - accuracy: 0.9743\n",
      "Epoch 119/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0855 - accuracy: 0.9709\n",
      "Epoch 120/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0819 - accuracy: 0.9800\n",
      "Epoch 121/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0845 - accuracy: 0.9740\n",
      "Epoch 122/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0923 - accuracy: 0.9705\n",
      "Epoch 123/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0663 - accuracy: 0.9855\n",
      "Epoch 124/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0874 - accuracy: 0.9770\n",
      "Epoch 125/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0810 - accuracy: 0.9753\n",
      "Epoch 126/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0748 - accuracy: 0.9801\n",
      "Epoch 127/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0680 - accuracy: 0.9780\n",
      "Epoch 128/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0821 - accuracy: 0.9718\n",
      "Epoch 129/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0708 - accuracy: 0.9800\n",
      "Epoch 130/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0903 - accuracy: 0.9691\n",
      "Epoch 131/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0731 - accuracy: 0.9795\n",
      "Epoch 132/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0644 - accuracy: 0.9735\n",
      "Epoch 133/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0828 - accuracy: 0.9723\n",
      "Epoch 134/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0726 - accuracy: 0.9807\n",
      "Epoch 135/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0721 - accuracy: 0.9738\n",
      "Epoch 136/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0602 - accuracy: 0.9819\n",
      "Epoch 137/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0601 - accuracy: 0.9781\n",
      "Epoch 138/150\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0754 - accuracy: 0.9760\n",
      "Epoch 139/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0594 - accuracy: 0.9870\n",
      "Epoch 140/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0534 - accuracy: 0.9884\n",
      "Epoch 141/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0632 - accuracy: 0.9826\n",
      "Epoch 142/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0590 - accuracy: 0.9828\n",
      "Epoch 143/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0547 - accuracy: 0.9857\n",
      "Epoch 144/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0421 - accuracy: 0.9840\n",
      "Epoch 145/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0555 - accuracy: 0.9849\n",
      "Epoch 146/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0510 - accuracy: 0.9901\n",
      "Epoch 147/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0546 - accuracy: 0.9824\n",
      "Epoch 148/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0497 - accuracy: 0.9887\n",
      "Epoch 149/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0376 - accuracy: 0.9916\n",
      "Epoch 150/150\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0549 - accuracy: 0.9869\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000281D4776D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "K-fold Cross Validation Accuracy Results:  [0.64615385 0.69230769 0.69230769 0.70769231 0.75384615 0.61538462\n",
      " 0.66153846 0.578125   0.609375   0.734375  ]\n",
      "K-fold Cross Validation Accuracy Results Mean:  0.669110576923077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\position\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "cv_model = grid_cv_model.best_estimator_\n",
    "\n",
    "#%% K-FOLD\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# K-fold accuracy scores\n",
    "\n",
    "kfold = KFold(n_splits = 10, shuffle=True)\n",
    "# results = cross_val_score(cv_model, X_test, np.argmax(y_test, axis=1), cv=kfold,scoring= 'accuracy')\n",
    "results = cross_val_score(cv_model, X_test, y_test, cv=kfold,scoring= 'accuracy')\n",
    "\n",
    "print('K-fold Cross Validation Accuracy Results: ', results)\n",
    "print('K-fold Cross Validation Accuracy Results Mean: ', results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d79f4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.87      0.91       134\n",
      "           1       0.90      0.96      0.93       154\n",
      "           2       0.88      0.93      0.90       141\n",
      "           3       0.95      0.92      0.93       218\n",
      "\n",
      "    accuracy                           0.92       647\n",
      "   macro avg       0.92      0.92      0.92       647\n",
      "weighted avg       0.92      0.92      0.92       647\n",
      "\n",
      "[[117  17   0   0]\n",
      " [  6 148   0   0]\n",
      " [  0   0 131  10]\n",
      " [  0   0  18 200]]\n"
     ]
    }
   ],
   "source": [
    "#%% Confusion Matrix and Classification Report\n",
    "from sklearn.metrics import confusion_matrix, classification_report \n",
    "\n",
    "# Classification Report\n",
    "model_report = classification_report(y_test, y_pred)\n",
    "print(model_report)\n",
    "\n",
    "# Confusion Matrix\n",
    "model_conf = confusion_matrix(y_test, y_pred)\n",
    "print(model_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65dfd0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = pd.read_csv(\"C:/Users/gold/Desktop/models/experiment1/PCAtransform.csv\")\n",
    "# C:\\Users\\gold\\Desktop\\models\\experiment1\n",
    "# y2 = data['level']\n",
    "# C:/Users/gold/Desktop/models/newexperiment/4Classes-ROC-and-AUC-of-9 ML~AAPTM\n",
    "y2 = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9987c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data into training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X2_train, X2_test, Y2_train, Y2_test = train_test_split(x2, y2, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "091fce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X2_train = sc.fit_transform(X2_train)\n",
    "X2_test = sc.transform(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4c94510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout,Activation\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "input_size = 11\n",
    "output_size = 4\n",
    "hidden_layer_size = 50\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    #Input layer\n",
    "    tf.keras.layers.Dense(input_size),\n",
    "    \n",
    "    #Hidden layer 1\n",
    "    tf.keras.layers.Dense(hidden_layer_size,activation='relu'),\n",
    "    #Hidden layer 2\n",
    "    tf.keras.layers.Dense(hidden_layer_size,activation='relu'),\n",
    "    #Hidden layer 3\n",
    "    tf.keras.layers.Dense(hidden_layer_size,activation='relu'),\n",
    "    \n",
    "    #Output layer\n",
    "    tf.keras.layers.Dense(output_size,activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f427b816",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "845e072e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "26/26 - 1s - loss: 1.2924 - accuracy: 0.4478 - val_loss: 1.1154 - val_accuracy: 0.5560\n",
      "Epoch 2/50\n",
      "26/26 - 0s - loss: 0.9256 - accuracy: 0.5801 - val_loss: 0.7529 - val_accuracy: 0.5920\n",
      "Epoch 3/50\n",
      "26/26 - 0s - loss: 0.6957 - accuracy: 0.6053 - val_loss: 0.6556 - val_accuracy: 0.6360\n",
      "Epoch 4/50\n",
      "26/26 - 0s - loss: 0.6437 - accuracy: 0.6413 - val_loss: 0.6369 - val_accuracy: 0.6540\n",
      "Epoch 5/50\n",
      "26/26 - 0s - loss: 0.6215 - accuracy: 0.6563 - val_loss: 0.6077 - val_accuracy: 0.6690\n",
      "Epoch 6/50\n",
      "26/26 - 0s - loss: 0.6028 - accuracy: 0.6629 - val_loss: 0.5930 - val_accuracy: 0.6780\n",
      "Epoch 7/50\n",
      "26/26 - 0s - loss: 0.5942 - accuracy: 0.6707 - val_loss: 0.5812 - val_accuracy: 0.6780\n",
      "Epoch 8/50\n",
      "26/26 - 0s - loss: 0.5790 - accuracy: 0.6854 - val_loss: 0.5737 - val_accuracy: 0.6820\n",
      "Epoch 9/50\n",
      "26/26 - 0s - loss: 0.5667 - accuracy: 0.7001 - val_loss: 0.5571 - val_accuracy: 0.7050\n",
      "Epoch 10/50\n",
      "26/26 - 0s - loss: 0.5544 - accuracy: 0.7117 - val_loss: 0.5530 - val_accuracy: 0.7110\n",
      "Epoch 11/50\n",
      "26/26 - 0s - loss: 0.5455 - accuracy: 0.7217 - val_loss: 0.5394 - val_accuracy: 0.7330\n",
      "Epoch 12/50\n",
      "26/26 - 0s - loss: 0.5289 - accuracy: 0.7415 - val_loss: 0.5279 - val_accuracy: 0.7360\n",
      "Epoch 13/50\n",
      "26/26 - 0s - loss: 0.5133 - accuracy: 0.7562 - val_loss: 0.5080 - val_accuracy: 0.7590\n",
      "Epoch 14/50\n",
      "26/26 - 0s - loss: 0.4990 - accuracy: 0.7728 - val_loss: 0.4910 - val_accuracy: 0.7850\n",
      "Epoch 15/50\n",
      "26/26 - 0s - loss: 0.4871 - accuracy: 0.7844 - val_loss: 0.4787 - val_accuracy: 0.7810\n",
      "Epoch 16/50\n",
      "26/26 - 0s - loss: 0.4739 - accuracy: 0.7829 - val_loss: 0.4691 - val_accuracy: 0.8020\n",
      "Epoch 17/50\n",
      "26/26 - 0s - loss: 0.4546 - accuracy: 0.8061 - val_loss: 0.4499 - val_accuracy: 0.7970\n",
      "Epoch 18/50\n",
      "26/26 - 0s - loss: 0.4470 - accuracy: 0.8069 - val_loss: 0.4379 - val_accuracy: 0.8040\n",
      "Epoch 19/50\n",
      "26/26 - 0s - loss: 0.4330 - accuracy: 0.8046 - val_loss: 0.4273 - val_accuracy: 0.8100\n",
      "Epoch 20/50\n",
      "26/26 - 0s - loss: 0.4228 - accuracy: 0.8092 - val_loss: 0.4240 - val_accuracy: 0.8080\n",
      "Epoch 21/50\n",
      "26/26 - 0s - loss: 0.4159 - accuracy: 0.8135 - val_loss: 0.4094 - val_accuracy: 0.8120\n",
      "Epoch 22/50\n",
      "26/26 - 0s - loss: 0.4035 - accuracy: 0.8170 - val_loss: 0.3955 - val_accuracy: 0.8230\n",
      "Epoch 23/50\n",
      "26/26 - 0s - loss: 0.4034 - accuracy: 0.8154 - val_loss: 0.3988 - val_accuracy: 0.8250\n",
      "Epoch 24/50\n",
      "26/26 - 0s - loss: 0.3900 - accuracy: 0.8278 - val_loss: 0.3941 - val_accuracy: 0.8230\n",
      "Epoch 25/50\n",
      "26/26 - 0s - loss: 0.3903 - accuracy: 0.8251 - val_loss: 0.3740 - val_accuracy: 0.8280\n",
      "Epoch 26/50\n",
      "26/26 - 0s - loss: 0.3758 - accuracy: 0.8282 - val_loss: 0.3744 - val_accuracy: 0.8250\n",
      "Epoch 27/50\n",
      "26/26 - 0s - loss: 0.3676 - accuracy: 0.8305 - val_loss: 0.3623 - val_accuracy: 0.8370\n",
      "Epoch 28/50\n",
      "26/26 - 0s - loss: 0.3591 - accuracy: 0.8406 - val_loss: 0.3541 - val_accuracy: 0.8460\n",
      "Epoch 29/50\n",
      "26/26 - 0s - loss: 0.3605 - accuracy: 0.8382 - val_loss: 0.3500 - val_accuracy: 0.8380\n",
      "Epoch 30/50\n",
      "26/26 - 0s - loss: 0.3456 - accuracy: 0.8413 - val_loss: 0.3422 - val_accuracy: 0.8510\n",
      "Epoch 31/50\n",
      "26/26 - 0s - loss: 0.3403 - accuracy: 0.8487 - val_loss: 0.3410 - val_accuracy: 0.8560\n",
      "Epoch 32/50\n",
      "26/26 - 0s - loss: 0.3391 - accuracy: 0.8487 - val_loss: 0.3328 - val_accuracy: 0.8490\n",
      "Epoch 33/50\n",
      "26/26 - 0s - loss: 0.3299 - accuracy: 0.8522 - val_loss: 0.3143 - val_accuracy: 0.8600\n",
      "Epoch 34/50\n",
      "26/26 - 0s - loss: 0.3230 - accuracy: 0.8587 - val_loss: 0.3166 - val_accuracy: 0.8550\n",
      "Epoch 35/50\n",
      "26/26 - 0s - loss: 0.3076 - accuracy: 0.8707 - val_loss: 0.3068 - val_accuracy: 0.8630\n",
      "Epoch 36/50\n",
      "26/26 - 0s - loss: 0.3034 - accuracy: 0.8765 - val_loss: 0.3193 - val_accuracy: 0.8550\n",
      "Epoch 37/50\n",
      "26/26 - 0s - loss: 0.3015 - accuracy: 0.8711 - val_loss: 0.2906 - val_accuracy: 0.8740\n",
      "Epoch 38/50\n",
      "26/26 - 0s - loss: 0.2942 - accuracy: 0.8769 - val_loss: 0.2877 - val_accuracy: 0.8800\n",
      "Epoch 39/50\n",
      "26/26 - 0s - loss: 0.2880 - accuracy: 0.8769 - val_loss: 0.2833 - val_accuracy: 0.8790\n",
      "Epoch 40/50\n",
      "26/26 - 0s - loss: 0.2819 - accuracy: 0.8866 - val_loss: 0.2668 - val_accuracy: 0.8950\n",
      "Epoch 41/50\n",
      "26/26 - 0s - loss: 0.2769 - accuracy: 0.8831 - val_loss: 0.2648 - val_accuracy: 0.8890\n",
      "Epoch 42/50\n",
      "26/26 - 0s - loss: 0.2697 - accuracy: 0.8940 - val_loss: 0.2602 - val_accuracy: 0.8900\n",
      "Epoch 43/50\n",
      "26/26 - 0s - loss: 0.2611 - accuracy: 0.8943 - val_loss: 0.2480 - val_accuracy: 0.9010\n",
      "Epoch 44/50\n",
      "26/26 - 0s - loss: 0.2559 - accuracy: 0.9009 - val_loss: 0.2503 - val_accuracy: 0.8970\n",
      "Epoch 45/50\n",
      "26/26 - 0s - loss: 0.2568 - accuracy: 0.8967 - val_loss: 0.2428 - val_accuracy: 0.9090\n",
      "Epoch 46/50\n",
      "26/26 - 0s - loss: 0.2416 - accuracy: 0.9071 - val_loss: 0.2456 - val_accuracy: 0.9010\n",
      "Epoch 47/50\n",
      "26/26 - 0s - loss: 0.2424 - accuracy: 0.9025 - val_loss: 0.2394 - val_accuracy: 0.9010\n",
      "Epoch 48/50\n",
      "26/26 - 0s - loss: 0.2396 - accuracy: 0.9017 - val_loss: 0.2207 - val_accuracy: 0.9150\n",
      "Epoch 49/50\n",
      "26/26 - 0s - loss: 0.2288 - accuracy: 0.9091 - val_loss: 0.2241 - val_accuracy: 0.9160\n",
      "Epoch 50/50\n",
      "26/26 - 0s - loss: 0.2236 - accuracy: 0.9172 - val_loss: 0.2143 - val_accuracy: 0.9180\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "early_stopping=tf.keras.callbacks.EarlyStopping(patience=20)\n",
    "\n",
    "results2 = model.fit(X2_train, Y2_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=NUM_EPOCHS,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(X2_train, Y2_train),\n",
    "          verbose=2,validation_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f835d100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.292438</td>\n",
       "      <td>0.447755</td>\n",
       "      <td>1.115381</td>\n",
       "      <td>0.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.925643</td>\n",
       "      <td>0.580108</td>\n",
       "      <td>0.752940</td>\n",
       "      <td>0.592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.695656</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.655583</td>\n",
       "      <td>0.636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.643733</td>\n",
       "      <td>0.641254</td>\n",
       "      <td>0.636923</td>\n",
       "      <td>0.654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.621521</td>\n",
       "      <td>0.656347</td>\n",
       "      <td>0.607684</td>\n",
       "      <td>0.669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.602818</td>\n",
       "      <td>0.662926</td>\n",
       "      <td>0.592985</td>\n",
       "      <td>0.678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.594174</td>\n",
       "      <td>0.670666</td>\n",
       "      <td>0.581211</td>\n",
       "      <td>0.678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.578962</td>\n",
       "      <td>0.685372</td>\n",
       "      <td>0.573651</td>\n",
       "      <td>0.682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.566673</td>\n",
       "      <td>0.700077</td>\n",
       "      <td>0.557065</td>\n",
       "      <td>0.705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.554377</td>\n",
       "      <td>0.711687</td>\n",
       "      <td>0.553004</td>\n",
       "      <td>0.711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.545536</td>\n",
       "      <td>0.721749</td>\n",
       "      <td>0.539445</td>\n",
       "      <td>0.733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.528927</td>\n",
       "      <td>0.741486</td>\n",
       "      <td>0.527927</td>\n",
       "      <td>0.736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.513290</td>\n",
       "      <td>0.756192</td>\n",
       "      <td>0.508019</td>\n",
       "      <td>0.759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.499031</td>\n",
       "      <td>0.772833</td>\n",
       "      <td>0.490965</td>\n",
       "      <td>0.785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.487077</td>\n",
       "      <td>0.784443</td>\n",
       "      <td>0.478660</td>\n",
       "      <td>0.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.473893</td>\n",
       "      <td>0.782895</td>\n",
       "      <td>0.469070</td>\n",
       "      <td>0.802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.454645</td>\n",
       "      <td>0.806115</td>\n",
       "      <td>0.449851</td>\n",
       "      <td>0.797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.446992</td>\n",
       "      <td>0.806889</td>\n",
       "      <td>0.437854</td>\n",
       "      <td>0.804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.433018</td>\n",
       "      <td>0.804567</td>\n",
       "      <td>0.427303</td>\n",
       "      <td>0.810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.422777</td>\n",
       "      <td>0.809211</td>\n",
       "      <td>0.423975</td>\n",
       "      <td>0.808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.415913</td>\n",
       "      <td>0.813468</td>\n",
       "      <td>0.409403</td>\n",
       "      <td>0.812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.403521</td>\n",
       "      <td>0.816950</td>\n",
       "      <td>0.395546</td>\n",
       "      <td>0.823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.403424</td>\n",
       "      <td>0.815402</td>\n",
       "      <td>0.398803</td>\n",
       "      <td>0.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.390034</td>\n",
       "      <td>0.827786</td>\n",
       "      <td>0.394099</td>\n",
       "      <td>0.823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.390313</td>\n",
       "      <td>0.825077</td>\n",
       "      <td>0.374001</td>\n",
       "      <td>0.828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.375789</td>\n",
       "      <td>0.828173</td>\n",
       "      <td>0.374423</td>\n",
       "      <td>0.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.367588</td>\n",
       "      <td>0.830495</td>\n",
       "      <td>0.362296</td>\n",
       "      <td>0.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.359110</td>\n",
       "      <td>0.840557</td>\n",
       "      <td>0.354073</td>\n",
       "      <td>0.846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.360547</td>\n",
       "      <td>0.838235</td>\n",
       "      <td>0.349983</td>\n",
       "      <td>0.838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.345588</td>\n",
       "      <td>0.841331</td>\n",
       "      <td>0.342184</td>\n",
       "      <td>0.851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.340340</td>\n",
       "      <td>0.848684</td>\n",
       "      <td>0.340974</td>\n",
       "      <td>0.856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.339072</td>\n",
       "      <td>0.848684</td>\n",
       "      <td>0.332774</td>\n",
       "      <td>0.849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.329917</td>\n",
       "      <td>0.852167</td>\n",
       "      <td>0.314327</td>\n",
       "      <td>0.860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.323010</td>\n",
       "      <td>0.858746</td>\n",
       "      <td>0.316645</td>\n",
       "      <td>0.855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.307630</td>\n",
       "      <td>0.870743</td>\n",
       "      <td>0.306840</td>\n",
       "      <td>0.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.303413</td>\n",
       "      <td>0.876548</td>\n",
       "      <td>0.319332</td>\n",
       "      <td>0.855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.301474</td>\n",
       "      <td>0.871130</td>\n",
       "      <td>0.290586</td>\n",
       "      <td>0.874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.294248</td>\n",
       "      <td>0.876935</td>\n",
       "      <td>0.287691</td>\n",
       "      <td>0.880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.287990</td>\n",
       "      <td>0.876935</td>\n",
       "      <td>0.283328</td>\n",
       "      <td>0.879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.281900</td>\n",
       "      <td>0.886610</td>\n",
       "      <td>0.266805</td>\n",
       "      <td>0.895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.276899</td>\n",
       "      <td>0.883127</td>\n",
       "      <td>0.264815</td>\n",
       "      <td>0.889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.269738</td>\n",
       "      <td>0.893963</td>\n",
       "      <td>0.260182</td>\n",
       "      <td>0.890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.261090</td>\n",
       "      <td>0.894350</td>\n",
       "      <td>0.247996</td>\n",
       "      <td>0.901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.255949</td>\n",
       "      <td>0.900929</td>\n",
       "      <td>0.250264</td>\n",
       "      <td>0.897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.256767</td>\n",
       "      <td>0.896672</td>\n",
       "      <td>0.242775</td>\n",
       "      <td>0.909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.241564</td>\n",
       "      <td>0.907121</td>\n",
       "      <td>0.245623</td>\n",
       "      <td>0.901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.242377</td>\n",
       "      <td>0.902477</td>\n",
       "      <td>0.239365</td>\n",
       "      <td>0.901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.239626</td>\n",
       "      <td>0.901703</td>\n",
       "      <td>0.220731</td>\n",
       "      <td>0.915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.228766</td>\n",
       "      <td>0.909056</td>\n",
       "      <td>0.224139</td>\n",
       "      <td>0.916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.223649</td>\n",
       "      <td>0.917183</td>\n",
       "      <td>0.214259</td>\n",
       "      <td>0.918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  accuracy  val_loss  val_accuracy\n",
       "0   1.292438  0.447755  1.115381         0.556\n",
       "1   0.925643  0.580108  0.752940         0.592\n",
       "2   0.695656  0.605263  0.655583         0.636\n",
       "3   0.643733  0.641254  0.636923         0.654\n",
       "4   0.621521  0.656347  0.607684         0.669\n",
       "5   0.602818  0.662926  0.592985         0.678\n",
       "6   0.594174  0.670666  0.581211         0.678\n",
       "7   0.578962  0.685372  0.573651         0.682\n",
       "8   0.566673  0.700077  0.557065         0.705\n",
       "9   0.554377  0.711687  0.553004         0.711\n",
       "10  0.545536  0.721749  0.539445         0.733\n",
       "11  0.528927  0.741486  0.527927         0.736\n",
       "12  0.513290  0.756192  0.508019         0.759\n",
       "13  0.499031  0.772833  0.490965         0.785\n",
       "14  0.487077  0.784443  0.478660         0.781\n",
       "15  0.473893  0.782895  0.469070         0.802\n",
       "16  0.454645  0.806115  0.449851         0.797\n",
       "17  0.446992  0.806889  0.437854         0.804\n",
       "18  0.433018  0.804567  0.427303         0.810\n",
       "19  0.422777  0.809211  0.423975         0.808\n",
       "20  0.415913  0.813468  0.409403         0.812\n",
       "21  0.403521  0.816950  0.395546         0.823\n",
       "22  0.403424  0.815402  0.398803         0.825\n",
       "23  0.390034  0.827786  0.394099         0.823\n",
       "24  0.390313  0.825077  0.374001         0.828\n",
       "25  0.375789  0.828173  0.374423         0.825\n",
       "26  0.367588  0.830495  0.362296         0.837\n",
       "27  0.359110  0.840557  0.354073         0.846\n",
       "28  0.360547  0.838235  0.349983         0.838\n",
       "29  0.345588  0.841331  0.342184         0.851\n",
       "30  0.340340  0.848684  0.340974         0.856\n",
       "31  0.339072  0.848684  0.332774         0.849\n",
       "32  0.329917  0.852167  0.314327         0.860\n",
       "33  0.323010  0.858746  0.316645         0.855\n",
       "34  0.307630  0.870743  0.306840         0.863\n",
       "35  0.303413  0.876548  0.319332         0.855\n",
       "36  0.301474  0.871130  0.290586         0.874\n",
       "37  0.294248  0.876935  0.287691         0.880\n",
       "38  0.287990  0.876935  0.283328         0.879\n",
       "39  0.281900  0.886610  0.266805         0.895\n",
       "40  0.276899  0.883127  0.264815         0.889\n",
       "41  0.269738  0.893963  0.260182         0.890\n",
       "42  0.261090  0.894350  0.247996         0.901\n",
       "43  0.255949  0.900929  0.250264         0.897\n",
       "44  0.256767  0.896672  0.242775         0.909\n",
       "45  0.241564  0.907121  0.245623         0.901\n",
       "46  0.242377  0.902477  0.239365         0.901\n",
       "47  0.239626  0.901703  0.220731         0.915\n",
       "48  0.228766  0.909056  0.224139         0.916\n",
       "49  0.223649  0.917183  0.214259         0.918"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results2.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af0ce75f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(results2.history)[['accuracy', 'val_accuracy','loss', 'val_loss']].plot()\n",
    "plt.savefig('C:\\\\Users\\\\gold\\\\Desktop\\\\models\\\\experiment2\\\\thesisfigures\\\\Learning Curve2.png',dpi = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c3e9d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 798us/step - loss: 0.3912 - accuracy: 0.8532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.39122331142425537, 0.8531684875488281]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X2_test, Y2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc35faf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = pd.DataFrame(results2.history,columns = ['accuracy', 'val_accuracy','loss', 'val_loss'])\n",
    "# save.to_csv('C:\\\\Users\\\\gold\\\\Desktop\\\\models\\\\results.csv',index = False,header = False)\n",
    "save.to_csv('C:\\\\Users\\\\gold\\\\Desktop\\\\models\\\\experiment2\\\\thesisfigures\\\\results2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1c03aae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import csv  # 导入csv模块\n",
    "import matplotlib.pyplot as plt\n",
    "path = 'C:\\\\Users\\\\gold\\\\Desktop\\\\models\\\\results2.csv'\n",
    "data = pd.read_csv(path) #读取文件中所有数据\n",
    "# epoch = data[['epoch']]\n",
    "\n",
    "accuracy = data[['accuracy']]\n",
    "val_accuracy = data[['val_accuracy']]\n",
    "loss = data[['loss']]\n",
    "val_loss = data[['val_loss']]\n",
    "\n",
    "#画折线图\n",
    "plt.plot(accuracy,label='accuracy',color='black',linestyle='-')\n",
    "plt.plot(val_accuracy,label='val_accuracy',color='black',linestyle='--')\n",
    "plt.plot(loss,label='loss',color='black',linestyle='-.')\n",
    "plt.plot(val_loss,label='val_loss',color='black',linestyle=':')\n",
    "\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('performance')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend()\n",
    "plt.savefig('C:\\\\Users\\\\gold\\\\Desktop\\\\models\\\\experiment2\\\\thesisfigures\\\\Learning Curve Image.png',dpi = 1000)\n",
    "plt.show()\n",
    "\n",
    "# plt.savefig('C:\\\\Users\\\\gold\\\\Desktop\\\\models\\\\Learning Curve Image',dpi = 600)\n",
    "# plt.savefig('plotLearning Curve Image.png',dpi = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3b22d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "x3 = pd.read_csv(\"C:/Users/gold/Desktop/models/experiment1/PCAtransform2.csv\")\n",
    "# C:\\Users\\gold\\Desktop\\models\\experiment1\n",
    "# y2 = data['level']\n",
    "y3 = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6d93873",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data into training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X3_train, X3_test, Y3_train, Y3_test = train_test_split(x3, y3, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95ac7c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X3_train = sc.fit_transform(X3_train)\n",
    "X3_test = sc.transform(X3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6aa38c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout,Activation\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "input_size = 6\n",
    "output_size = 4\n",
    "hidden_layer_size = 50\n",
    "\n",
    "model=tf.keras.Sequential([\n",
    "    #Input layer\n",
    "    tf.keras.layers.Dense(input_size),\n",
    "    \n",
    "    #Hidden layer 1\n",
    "    tf.keras.layers.Dense(hidden_layer_size,activation='relu'),\n",
    "    #Hidden layer 2\n",
    "    tf.keras.layers.Dense(hidden_layer_size,activation='relu'),\n",
    "    #Hidden layer 3\n",
    "    tf.keras.layers.Dense(hidden_layer_size,activation='relu'),\n",
    "    \n",
    "    #Output layer\n",
    "    tf.keras.layers.Dense(output_size,activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ea6b2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e3a561c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "26/26 - 1s - loss: 1.2280 - accuracy: 0.4435 - val_loss: 1.0086 - val_accuracy: 0.5560\n",
      "Epoch 2/50\n",
      "26/26 - 0s - loss: 0.8628 - accuracy: 0.5755 - val_loss: 0.7501 - val_accuracy: 0.5810\n",
      "Epoch 3/50\n",
      "26/26 - 0s - loss: 0.7032 - accuracy: 0.6080 - val_loss: 0.6744 - val_accuracy: 0.6030\n",
      "Epoch 4/50\n",
      "26/26 - 0s - loss: 0.6531 - accuracy: 0.6440 - val_loss: 0.6461 - val_accuracy: 0.6290\n",
      "Epoch 5/50\n",
      "26/26 - 0s - loss: 0.6296 - accuracy: 0.6873 - val_loss: 0.6179 - val_accuracy: 0.6790\n",
      "Epoch 6/50\n",
      "26/26 - 0s - loss: 0.6076 - accuracy: 0.6970 - val_loss: 0.5964 - val_accuracy: 0.7090\n",
      "Epoch 7/50\n",
      "26/26 - 0s - loss: 0.5822 - accuracy: 0.7276 - val_loss: 0.5968 - val_accuracy: 0.6970\n",
      "Epoch 8/50\n",
      "26/26 - 0s - loss: 0.5719 - accuracy: 0.7330 - val_loss: 0.5733 - val_accuracy: 0.7280\n",
      "Epoch 9/50\n",
      "26/26 - 0s - loss: 0.5458 - accuracy: 0.7566 - val_loss: 0.5409 - val_accuracy: 0.7670\n",
      "Epoch 10/50\n",
      "26/26 - 0s - loss: 0.5207 - accuracy: 0.7686 - val_loss: 0.5175 - val_accuracy: 0.7810\n",
      "Epoch 11/50\n",
      "26/26 - 0s - loss: 0.4979 - accuracy: 0.7833 - val_loss: 0.5035 - val_accuracy: 0.7780\n",
      "Epoch 12/50\n",
      "26/26 - 0s - loss: 0.4873 - accuracy: 0.7848 - val_loss: 0.4882 - val_accuracy: 0.7810\n",
      "Epoch 13/50\n",
      "26/26 - 0s - loss: 0.4688 - accuracy: 0.7879 - val_loss: 0.4755 - val_accuracy: 0.7650\n",
      "Epoch 14/50\n",
      "26/26 - 0s - loss: 0.4560 - accuracy: 0.7910 - val_loss: 0.4621 - val_accuracy: 0.7760\n",
      "Epoch 15/50\n",
      "26/26 - 0s - loss: 0.4465 - accuracy: 0.8019 - val_loss: 0.4440 - val_accuracy: 0.7950\n",
      "Epoch 16/50\n",
      "26/26 - 0s - loss: 0.4322 - accuracy: 0.8069 - val_loss: 0.4361 - val_accuracy: 0.7980\n",
      "Epoch 17/50\n",
      "26/26 - 0s - loss: 0.4242 - accuracy: 0.8019 - val_loss: 0.4262 - val_accuracy: 0.7960\n",
      "Epoch 18/50\n",
      "26/26 - 0s - loss: 0.4198 - accuracy: 0.8108 - val_loss: 0.4174 - val_accuracy: 0.7970\n",
      "Epoch 19/50\n",
      "26/26 - 0s - loss: 0.4173 - accuracy: 0.8069 - val_loss: 0.4146 - val_accuracy: 0.7930\n",
      "Epoch 20/50\n",
      "26/26 - 0s - loss: 0.4115 - accuracy: 0.8092 - val_loss: 0.4187 - val_accuracy: 0.8110\n",
      "Epoch 21/50\n",
      "26/26 - 0s - loss: 0.4023 - accuracy: 0.8208 - val_loss: 0.3994 - val_accuracy: 0.8090\n",
      "Epoch 22/50\n",
      "26/26 - 0s - loss: 0.3935 - accuracy: 0.8208 - val_loss: 0.3969 - val_accuracy: 0.8050\n",
      "Epoch 23/50\n",
      "26/26 - 0s - loss: 0.3919 - accuracy: 0.8235 - val_loss: 0.3863 - val_accuracy: 0.8180\n",
      "Epoch 24/50\n",
      "26/26 - 0s - loss: 0.3862 - accuracy: 0.8193 - val_loss: 0.3945 - val_accuracy: 0.8110\n",
      "Epoch 25/50\n",
      "26/26 - 0s - loss: 0.3804 - accuracy: 0.8228 - val_loss: 0.3813 - val_accuracy: 0.8230\n",
      "Epoch 26/50\n",
      "26/26 - 0s - loss: 0.3742 - accuracy: 0.8305 - val_loss: 0.3730 - val_accuracy: 0.8280\n",
      "Epoch 27/50\n",
      "26/26 - 0s - loss: 0.3698 - accuracy: 0.8297 - val_loss: 0.3746 - val_accuracy: 0.8190\n",
      "Epoch 28/50\n",
      "26/26 - 0s - loss: 0.3673 - accuracy: 0.8348 - val_loss: 0.3746 - val_accuracy: 0.8240\n",
      "Epoch 29/50\n",
      "26/26 - 0s - loss: 0.3632 - accuracy: 0.8309 - val_loss: 0.3629 - val_accuracy: 0.8260\n",
      "Epoch 30/50\n",
      "26/26 - 0s - loss: 0.3630 - accuracy: 0.8355 - val_loss: 0.3583 - val_accuracy: 0.8290\n",
      "Epoch 31/50\n",
      "26/26 - 0s - loss: 0.3575 - accuracy: 0.8351 - val_loss: 0.3553 - val_accuracy: 0.8310\n",
      "Epoch 32/50\n",
      "26/26 - 0s - loss: 0.3537 - accuracy: 0.8328 - val_loss: 0.3504 - val_accuracy: 0.8320\n",
      "Epoch 33/50\n",
      "26/26 - 0s - loss: 0.3488 - accuracy: 0.8390 - val_loss: 0.3498 - val_accuracy: 0.8350\n",
      "Epoch 34/50\n",
      "26/26 - 0s - loss: 0.3417 - accuracy: 0.8491 - val_loss: 0.3471 - val_accuracy: 0.8380\n",
      "Epoch 35/50\n",
      "26/26 - 0s - loss: 0.3444 - accuracy: 0.8417 - val_loss: 0.3543 - val_accuracy: 0.8210\n",
      "Epoch 36/50\n",
      "26/26 - 0s - loss: 0.3395 - accuracy: 0.8475 - val_loss: 0.3380 - val_accuracy: 0.8370\n",
      "Epoch 37/50\n",
      "26/26 - 0s - loss: 0.3361 - accuracy: 0.8440 - val_loss: 0.3466 - val_accuracy: 0.8320\n",
      "Epoch 38/50\n",
      "26/26 - 0s - loss: 0.3344 - accuracy: 0.8390 - val_loss: 0.3421 - val_accuracy: 0.8310\n",
      "Epoch 39/50\n",
      "26/26 - 0s - loss: 0.3287 - accuracy: 0.8487 - val_loss: 0.3301 - val_accuracy: 0.8400\n",
      "Epoch 40/50\n",
      "26/26 - 0s - loss: 0.3325 - accuracy: 0.8409 - val_loss: 0.3287 - val_accuracy: 0.8430\n",
      "Epoch 41/50\n",
      "26/26 - 0s - loss: 0.3224 - accuracy: 0.8529 - val_loss: 0.3419 - val_accuracy: 0.8250\n",
      "Epoch 42/50\n",
      "26/26 - 0s - loss: 0.3181 - accuracy: 0.8518 - val_loss: 0.3301 - val_accuracy: 0.8370\n",
      "Epoch 43/50\n",
      "26/26 - 0s - loss: 0.3260 - accuracy: 0.8471 - val_loss: 0.3351 - val_accuracy: 0.8500\n",
      "Epoch 44/50\n",
      "26/26 - 0s - loss: 0.3176 - accuracy: 0.8514 - val_loss: 0.3227 - val_accuracy: 0.8340\n",
      "Epoch 45/50\n",
      "26/26 - 0s - loss: 0.3150 - accuracy: 0.8502 - val_loss: 0.3255 - val_accuracy: 0.8450\n",
      "Epoch 46/50\n",
      "26/26 - 0s - loss: 0.3103 - accuracy: 0.8595 - val_loss: 0.3172 - val_accuracy: 0.8450\n",
      "Epoch 47/50\n",
      "26/26 - 0s - loss: 0.3122 - accuracy: 0.8495 - val_loss: 0.3216 - val_accuracy: 0.8400\n",
      "Epoch 48/50\n",
      "26/26 - 0s - loss: 0.3104 - accuracy: 0.8587 - val_loss: 0.3195 - val_accuracy: 0.8360\n",
      "Epoch 49/50\n",
      "26/26 - 0s - loss: 0.3052 - accuracy: 0.8595 - val_loss: 0.3115 - val_accuracy: 0.8510\n",
      "Epoch 50/50\n",
      "26/26 - 0s - loss: 0.3066 - accuracy: 0.8615 - val_loss: 0.3097 - val_accuracy: 0.8570\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "early_stopping=tf.keras.callbacks.EarlyStopping(patience=20)\n",
    "\n",
    "results3 = model.fit(X3_train, Y3_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=NUM_EPOCHS,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(X3_train, Y3_train),\n",
    "          verbose=2,validation_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7aba78be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.227964</td>\n",
       "      <td>0.443498</td>\n",
       "      <td>1.008630</td>\n",
       "      <td>0.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.862779</td>\n",
       "      <td>0.575464</td>\n",
       "      <td>0.750149</td>\n",
       "      <td>0.581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.703168</td>\n",
       "      <td>0.607972</td>\n",
       "      <td>0.674373</td>\n",
       "      <td>0.603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.653078</td>\n",
       "      <td>0.643963</td>\n",
       "      <td>0.646056</td>\n",
       "      <td>0.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.629648</td>\n",
       "      <td>0.687307</td>\n",
       "      <td>0.617874</td>\n",
       "      <td>0.679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.607554</td>\n",
       "      <td>0.696981</td>\n",
       "      <td>0.596423</td>\n",
       "      <td>0.709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.582232</td>\n",
       "      <td>0.727554</td>\n",
       "      <td>0.596797</td>\n",
       "      <td>0.697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.571878</td>\n",
       "      <td>0.732972</td>\n",
       "      <td>0.573280</td>\n",
       "      <td>0.728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.545795</td>\n",
       "      <td>0.756579</td>\n",
       "      <td>0.540890</td>\n",
       "      <td>0.767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.520698</td>\n",
       "      <td>0.768576</td>\n",
       "      <td>0.517484</td>\n",
       "      <td>0.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.497937</td>\n",
       "      <td>0.783282</td>\n",
       "      <td>0.503499</td>\n",
       "      <td>0.778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.487290</td>\n",
       "      <td>0.784830</td>\n",
       "      <td>0.488236</td>\n",
       "      <td>0.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.468782</td>\n",
       "      <td>0.787926</td>\n",
       "      <td>0.475516</td>\n",
       "      <td>0.765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.455983</td>\n",
       "      <td>0.791022</td>\n",
       "      <td>0.462103</td>\n",
       "      <td>0.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.446535</td>\n",
       "      <td>0.801858</td>\n",
       "      <td>0.444002</td>\n",
       "      <td>0.795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.432230</td>\n",
       "      <td>0.806889</td>\n",
       "      <td>0.436065</td>\n",
       "      <td>0.798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.424178</td>\n",
       "      <td>0.801858</td>\n",
       "      <td>0.426239</td>\n",
       "      <td>0.796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.419765</td>\n",
       "      <td>0.810759</td>\n",
       "      <td>0.417407</td>\n",
       "      <td>0.797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.417345</td>\n",
       "      <td>0.806889</td>\n",
       "      <td>0.414632</td>\n",
       "      <td>0.793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.411527</td>\n",
       "      <td>0.809211</td>\n",
       "      <td>0.418669</td>\n",
       "      <td>0.811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.402283</td>\n",
       "      <td>0.820820</td>\n",
       "      <td>0.399428</td>\n",
       "      <td>0.809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.393489</td>\n",
       "      <td>0.820820</td>\n",
       "      <td>0.396901</td>\n",
       "      <td>0.805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.391918</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.386273</td>\n",
       "      <td>0.818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.386245</td>\n",
       "      <td>0.819272</td>\n",
       "      <td>0.394519</td>\n",
       "      <td>0.811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.380398</td>\n",
       "      <td>0.822755</td>\n",
       "      <td>0.381260</td>\n",
       "      <td>0.823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.374151</td>\n",
       "      <td>0.830495</td>\n",
       "      <td>0.373005</td>\n",
       "      <td>0.828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.369756</td>\n",
       "      <td>0.829721</td>\n",
       "      <td>0.374599</td>\n",
       "      <td>0.819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.367335</td>\n",
       "      <td>0.834752</td>\n",
       "      <td>0.374603</td>\n",
       "      <td>0.824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.363165</td>\n",
       "      <td>0.830882</td>\n",
       "      <td>0.362936</td>\n",
       "      <td>0.826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.362996</td>\n",
       "      <td>0.835526</td>\n",
       "      <td>0.358336</td>\n",
       "      <td>0.829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.357472</td>\n",
       "      <td>0.835139</td>\n",
       "      <td>0.355295</td>\n",
       "      <td>0.831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.353743</td>\n",
       "      <td>0.832817</td>\n",
       "      <td>0.350391</td>\n",
       "      <td>0.832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.348770</td>\n",
       "      <td>0.839009</td>\n",
       "      <td>0.349750</td>\n",
       "      <td>0.835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.341696</td>\n",
       "      <td>0.849071</td>\n",
       "      <td>0.347084</td>\n",
       "      <td>0.838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.344421</td>\n",
       "      <td>0.841718</td>\n",
       "      <td>0.354334</td>\n",
       "      <td>0.821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.339464</td>\n",
       "      <td>0.847523</td>\n",
       "      <td>0.338041</td>\n",
       "      <td>0.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.336134</td>\n",
       "      <td>0.844040</td>\n",
       "      <td>0.346639</td>\n",
       "      <td>0.832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.334392</td>\n",
       "      <td>0.839009</td>\n",
       "      <td>0.342060</td>\n",
       "      <td>0.831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.328694</td>\n",
       "      <td>0.848684</td>\n",
       "      <td>0.330111</td>\n",
       "      <td>0.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.332466</td>\n",
       "      <td>0.840944</td>\n",
       "      <td>0.328657</td>\n",
       "      <td>0.843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.322386</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.341858</td>\n",
       "      <td>0.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.318147</td>\n",
       "      <td>0.851780</td>\n",
       "      <td>0.330101</td>\n",
       "      <td>0.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.325994</td>\n",
       "      <td>0.847136</td>\n",
       "      <td>0.335144</td>\n",
       "      <td>0.850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.317557</td>\n",
       "      <td>0.851393</td>\n",
       "      <td>0.322653</td>\n",
       "      <td>0.834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.314962</td>\n",
       "      <td>0.850232</td>\n",
       "      <td>0.325521</td>\n",
       "      <td>0.845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.310335</td>\n",
       "      <td>0.859520</td>\n",
       "      <td>0.317206</td>\n",
       "      <td>0.845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.312234</td>\n",
       "      <td>0.849458</td>\n",
       "      <td>0.321598</td>\n",
       "      <td>0.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.310447</td>\n",
       "      <td>0.858746</td>\n",
       "      <td>0.319480</td>\n",
       "      <td>0.836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.305186</td>\n",
       "      <td>0.859520</td>\n",
       "      <td>0.311514</td>\n",
       "      <td>0.851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.306573</td>\n",
       "      <td>0.861455</td>\n",
       "      <td>0.309715</td>\n",
       "      <td>0.857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  accuracy  val_loss  val_accuracy\n",
       "0   1.227964  0.443498  1.008630         0.556\n",
       "1   0.862779  0.575464  0.750149         0.581\n",
       "2   0.703168  0.607972  0.674373         0.603\n",
       "3   0.653078  0.643963  0.646056         0.629\n",
       "4   0.629648  0.687307  0.617874         0.679\n",
       "5   0.607554  0.696981  0.596423         0.709\n",
       "6   0.582232  0.727554  0.596797         0.697\n",
       "7   0.571878  0.732972  0.573280         0.728\n",
       "8   0.545795  0.756579  0.540890         0.767\n",
       "9   0.520698  0.768576  0.517484         0.781\n",
       "10  0.497937  0.783282  0.503499         0.778\n",
       "11  0.487290  0.784830  0.488236         0.781\n",
       "12  0.468782  0.787926  0.475516         0.765\n",
       "13  0.455983  0.791022  0.462103         0.776\n",
       "14  0.446535  0.801858  0.444002         0.795\n",
       "15  0.432230  0.806889  0.436065         0.798\n",
       "16  0.424178  0.801858  0.426239         0.796\n",
       "17  0.419765  0.810759  0.417407         0.797\n",
       "18  0.417345  0.806889  0.414632         0.793\n",
       "19  0.411527  0.809211  0.418669         0.811\n",
       "20  0.402283  0.820820  0.399428         0.809\n",
       "21  0.393489  0.820820  0.396901         0.805\n",
       "22  0.391918  0.823529  0.386273         0.818\n",
       "23  0.386245  0.819272  0.394519         0.811\n",
       "24  0.380398  0.822755  0.381260         0.823\n",
       "25  0.374151  0.830495  0.373005         0.828\n",
       "26  0.369756  0.829721  0.374599         0.819\n",
       "27  0.367335  0.834752  0.374603         0.824\n",
       "28  0.363165  0.830882  0.362936         0.826\n",
       "29  0.362996  0.835526  0.358336         0.829\n",
       "30  0.357472  0.835139  0.355295         0.831\n",
       "31  0.353743  0.832817  0.350391         0.832\n",
       "32  0.348770  0.839009  0.349750         0.835\n",
       "33  0.341696  0.849071  0.347084         0.838\n",
       "34  0.344421  0.841718  0.354334         0.821\n",
       "35  0.339464  0.847523  0.338041         0.837\n",
       "36  0.336134  0.844040  0.346639         0.832\n",
       "37  0.334392  0.839009  0.342060         0.831\n",
       "38  0.328694  0.848684  0.330111         0.840\n",
       "39  0.332466  0.840944  0.328657         0.843\n",
       "40  0.322386  0.852941  0.341858         0.825\n",
       "41  0.318147  0.851780  0.330101         0.837\n",
       "42  0.325994  0.847136  0.335144         0.850\n",
       "43  0.317557  0.851393  0.322653         0.834\n",
       "44  0.314962  0.850232  0.325521         0.845\n",
       "45  0.310335  0.859520  0.317206         0.845\n",
       "46  0.312234  0.849458  0.321598         0.840\n",
       "47  0.310447  0.858746  0.319480         0.836\n",
       "48  0.305186  0.859520  0.311514         0.851\n",
       "49  0.306573  0.861455  0.309715         0.857"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results3.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3deae404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3hUVf7H8feZkknPTHqjQ+g9CIL0FVEpCiIoovBDXdaOq7J2VNx11XVXV1dFFyusIEURFRcEwVVa6BAgQAIhpJdJz2TK/f0xEIoEAiQME76v55knmZlbzh30MyfnnqI0TUMIIYT303m6AEIIIeqHBLoQQjQSEuhCCNFISKALIUQjIYEuhBCNhMFTJw4PD9eaN2/uqdMLIYRX2rx5c76maRFnes9jgd68eXOSkpI8dXohhPBKSqnDtb0nTS5CCNFISKALIUQjIYEuhBCNhMfa0IUQlxe73U5GRgZVVVWeLooAfH19iY+Px2g01nkfCXQhBAAZGRkEBQXRvHlzlFKeLs4VTdM0CgoKyMjIoEWLFnXeT5pchBAAVFVVERYWJmF+GVBKERYWdt5/LUmgCyFqSJhfPi7k38LrAj2lKIW3tryFtcrq6aIIIcRlxesC/UjJET7Y+QHZFdmeLooQQlxWvC7Qzb5mAAqrCj1cEiGEt3I4HJ4uQoPwukC3mCwA0uQiRCN100030bNnTzp27Mjs2bMBWL58OT169KBr164MHToUgLKyMqZMmULnzp3p0qULixYtAiAwMLDmWAsXLmTy5MkATJ48mUcffZTBgwczY8YMNm7cSN++fenevTt9+/Zl3759ADidTh577LGa4/7zn//kxx9/5Oabb6457ooVKxgzZsyl+DjOyzm7LSql5gAjgFxN0zqd4f2JwIxjT8uAP2iatr1eS3kSi6870ItsRQ11CiGueC98s5vkzJJ6PWaH2GCeH9nxnNvNmTOH0NBQKisr6dWrF6NHj+aee+5h7dq1tGjRgsJC91/nL730EiEhIezcuROAoqJzZ0JKSgorV65Er9dTUlLC2rVrMRgMrFy5kqeeeopFixYxe/Zs0tLS2Lp1KwaDgcLCQiwWC/fffz95eXlERETw0UcfMWXKlIv7QBpAXfqhfwy8DXxay/tpwEBN04qUUtcDs4He9VO83wr2CUahKKqSQBeiMXrrrbdYsmQJAEeOHGH27NkMGDCgpj92aGgoACtXruSLL76o2c9isZzz2OPGjUOv1wNQXFzMXXfdxf79+1FKYbfba447bdo0DAbDKeebNGkSn3/+OVOmTGHdunV8+mltkeg55wx0TdPWKqWan+X9X096uh6Iv/hi1U6v0xNiCsFqkyYXIRpKXWrSDeGnn35i5cqVrFu3Dn9/fwYNGkTXrl1rmkNOpmnaGbv2nfza6f24AwICan5/9tlnGTx4MEuWLOHQoUMMGjTorMedMmUKI0eOxNfXl3HjxtUE/uWkvtvQpwLf1/amUupepVSSUiopLy/vgk9i8bVIDV2IRqi4uBiLxYK/vz979+5l/fr12Gw21qxZQ1paGkBNk8uwYcN4++23a/Y93uQSFRXFnj17cLlcNTX92s4VFxcHwMcff1zz+rBhw3jvvfdqbpweP19sbCyxsbHMmjWrpl3+clNvga6UGow70GfUto2mabM1TUvUNC0xIuKM87PXicVkkTZ0IRqh4cOH43A46NKlC88++yx9+vQhIiKC2bNnM2bMGLp27cr48eMBeOaZZygqKqJTp0507dqV1atXA/DKK68wYsQIhgwZQkxMTK3neuKJJ3jyySfp168fTqez5vW7776bpk2b0qVLF7p27cq8efNq3ps4cSJNmjShQ4cODfQJXByladq5N3I3uSw7003RY+93AZYA12uallKXEycmJmoXusDFw6seJr00nSWja//2FUKcnz179tC+fXtPF+Oy9sADD9C9e3emTp16Sc53pn8TpdRmTdMSz7T9RdfQlVJNgcXApLqG+cWy+FqkDV0IcUn17NmTHTt2cMcdd3i6KLWqS7fF/wCDgHClVAbwPGAE0DTtPeA5IAz417EbCY7avj3qi8XXgrXKWuvNCyGEqG+bN2/2dBHOqS69XG47x/t3A3fXW4nqwGwy49AclNpLCfYJvpSnFkKIy5bXjRQFCPV19wuV0aJCCHGCVwa62STzuQghxOm8MtCPD/+XG6NCCHGCVwe6DC4SQogTvDPQTTJBlxBXupNnVRRuXhnofgY/fHQ+clNUCOFxl9Pc6pff7DJ1oJRyz+ciNXQhGsb3f4LsnfV7zOjOcP0rtb49Y8YMmjVrxn333QfAzJkzUUqxdu1aioqKsNvtzJo1i9GjR5/zVGVlZYwePfqM+3366ae8/vrrKKXo0qULn332GTk5OUybNo3U1FQA3n33XWJjYxkxYgS7du0C4PXXX6esrIyZM2cyaNAg+vbtyy+//MKoUaNISEhg1qxZVFdXExYWxty5c4mKiqKsrIwHH3yQpKQklFI8//zzWK1Wdu3axd///ncAPvjgA/bs2cMbb7xxUR8veGmgg0zQJURjM2HCBB555JGaQF+wYAHLly9n+vTpBAcHk5+fT58+fRg1atQ5BxT6+vqyZMmS3+yXnJzMyy+/zC+//EJ4eHjNxFsPPfQQAwcOZMmSJTidTsrKys45v7rVamXNmjWAe2Kw9evXo5Tiww8/5NVXX+Vvf/vbGeds9/HxoUuXLrz66qsYjUY++ugj3n///Yv9+AAvDnSzySw1dCEayllq0g2le/fu5ObmkpmZSV5eHhaLhZiYGKZPn87atWvR6XQcPXqUnJwcoqOjz3osTdN46qmnfrPfqlWruOWWWwgPDwdOzHW+atWqmvnN9Xo9ISEh5wz045OEAWRkZDB+/HiysrKorq6umbu9tjnbhwwZwrJly2jfvj12u53OnTuf56d1Zl4b6BZfC5n5mZ4uhhCiHt1yyy0sXLiQ7OxsJkyYwNy5c8nLy2Pz5s0YjUaaN2/+mznOz6S2/c5nuhCDwYDL5ap5fra51R988EEeffRRRo0axU8//cTMmTOB2udWv/vuu/nzn/9Mu3bt6nXlI6+8KQrHptCVJhchGpUJEybwxRdfsHDhQm655RaKi4uJjIzEaDSyevVqDh8+XKfj1Lbf0KFDWbBgAQUFBcCJuc6HDh3Ku+++C7jXFC0pKSEqKorc3FwKCgqw2WwsW7bsrOc7Prf6J598UvN6bXO29+7dmyNHjjBv3jxuu+2ss6ucF68NdLOvmVJ7KXaX3dNFEULUk44dO1JaWkpcXBwxMTFMnDiRpKQkEhMTmTt3Lu3atavTcWrbr2PHjjz99NMMHDiQrl278uijjwLw5ptvsnr1ajp37kzPnj3ZvXs3RqOR5557jt69ezNixIiznnvmzJmMGzeO/v371zTnQO1ztgPceuut9OvXr05L59VVneZDbwgXMx86wPy985m1YRarb11NuF/4uXcQQpyVzId+aY0YMYLp06czdOjQWre55POhe4rZV+ZzEUJ4H6vVSkJCAn5+fmcN8wvhvTdFj40WlcFFQly5du7cyaRJk055zWQysWHDBg+V6NzMZjMpKQ2zFpD3BrqvDP8X4krXuXNntm3b5uliXDa8tslFJugSQohTeW2gh5hCAKmhCyHEcV4b6EadkSCfIGlDF0KIY7w20EEGFwnR2MiUuBfHqwPd7CvzuQghxHFeHeihplBZhk6IRkjTNB5//HE6depE586dmT9/PgBZWVkMGDCAbt260alTJ37++WecTieTJ0+u2fb4tLRXIq/ttgjuGnpyYbKniyFEo/PXjX9lb+Heej1mu9B2zLhqRp22Xbx4Mdu2bWP79u3k5+fTq1cvBgwYwLx587juuut4+umncTqdVFRUsG3bNo4ePVozb7nVeuVW8ry6hm4xWbBWWfHU9AVCiIbxv//9j9tuuw29Xk9UVBQDBw5k06ZN9OrVi48++oiZM2eyc+dOgoKCaNmyJampqTz44IMsX76c4OBgTxffY7y6hm7xtVDtqqbSUYm/0d/TxRGi0ahrTbqh1FZJGzBgAGvXruXbb79l0qRJPP7449x5551s376dH374gXfeeYcFCxYwZ86cS1ziy4NX19DNJpnPRYjGaMCAAcyfPx+n00leXh5r167lqquu4vDhw0RGRnLPPfcwdepUtmzZQn5+Pi6Xi7Fjx/LSSy+xZcsWTxffY7y+hg5gtVmJD4r3cGmEEPXl5ptvZt26dXTt2hWlFK+++irR0dF88sknvPbaaxiNRgIDA/n00085evQoU6ZMqVmM4i9/+YuHS+85jSLQpS+6EI1DWVkZ4F4I/rXXXuO111475f277rqLu+666zf7Xcm18pN5dZPL8RkXpS+6EEJ4eaAfnxNdauhCCOHlgR5kDMKgDDK4SAgh8PJAV0q5h/9LDV0IIbw70MHddVECXQghGkGgh/rKfC5CCAF1CHSl1BylVK5Salct7yul1FtKqQNKqR1KqR71X8wTqvbtI+e113Aem6/BbDLLwCIhhKBuNfSPgeFnef96oM2xx73AuxdfrNrZMzIo/PccqjOOAu6+6FJDF+LKc7a50w8dOkSnTp0uYWkuD+cMdE3T1gJnqwKPBj7V3NYDZqVUTH0V8HSG6GgAHNlZgDvQi23FOF3OhjqlEEJ4hfoYKRoHHDnpecax17JO31ApdS/uWjxNmza9oJMZjwW6PSsbcDe5aGgUVxcT6ht6QccUQpwq+89/xranfqfPNbVvR/RTT9X6/owZM2jWrBn33XcfADNnzkQpxdq1aykqKsJutzNr1ixGjx59XuetqqriD3/4A0lJSRgMBt544w0GDx7M7t27mTJlCtXV1bhcLhYtWkRsbCy33norGRkZOJ1Onn32WcaPH39R130p1UegqzO8dsap0jRNmw3MBkhMTLygOW/1oaEoHx/sx2vox0aLWqusEuhCeLEJEybwyCOP1AT6ggULWL58OdOnTyc4OJj8/Hz69OnDqFGjUOpMsXNm77zzDgA7d+5k7969DBs2jJSUFN577z0efvhhJk6cSHV1NU6nk++++47Y2Fi+/fZbAIqLi+v/QhtQfQR6BtDkpOfxQGY9HPeMlFIYoqNxZOcAJ83nIsP/hag3Z6tJN5Tu3buTm5tLZmYmeXl5WCwWYmJimD59OmvXrkWn03H06FFycnKIPvaXel3873//48EHHwSgXbt2NGvWjJSUFK6++mpefvllMjIyGDNmDG3atKFz58489thjzJgxgxEjRtC/f/+GutwGUR/dFpcCdx7r7dIHKNY07TfNLfXJGBWFPdvd5CITdAnReNxyyy0sXLiQ+fPnM2HCBObOnUteXh6bN29m27ZtREVFUVVVdV7HrG1u9dtvv52lS5fi5+fHddddx6pVq0hISGDz5s107tyZJ598khdffLE+LuuSOWcNXSn1H2AQEK6UygCeB4wAmqa9B3wH3AAcACqAKQ1V2OMMMdFUJm0GTsyJLjV0IbzfhAkTuOeee8jPz2fNmjUsWLCAyMhIjEYjq1ev5vDhw+d9zAEDBjB37lyGDBlCSkoK6enptG3bltTUVFq2bMlDDz1EamoqO3bsoF27doSGhnLHHXcQGBjIxx9/XP8X2YDOGeiapt12jvc14P56K1EdGKNjKMnNRXM6T8yJXiVdF4Xwdh07dqS0tJS4uDhiYmKYOHEiI0eOJDExkW7dutGuXbvzPuZ9993HtGnT6Ny5MwaDgY8//hiTycT8+fP5/PPPMRqNREdH89xzz7Fp0yYef/xxdDodRqORd99t0F7Y9U55aj3OxMRELSkp6YL2LfrPf8h+4UVar12DMTKS3nN7M6bNGI8vmyWEN9uzZw/t27f3dDHESc70b6KU2qxpWuKZtvfKof+GqON90U+0o8vgIiHElc4rVywyxpzoi+7XpQsWk0Xa0IW4Au3cuZNJkyad8prJZGLDhg0eKpFneWWg14wWzTk2uMhX5nMRoj5omnZefbw9rXPnzmzbts3TxWgQF9Ic7pVNLnqzGWUy1YwWtZgsclNUiIvk6+tLQUHBBQWJqF+aplFQUICvr+957eeVNXSlFMbo6BOjRX2lyUWIixUfH09GRgZ5eXmeLorA/QUbHx9/Xvt4ZaADGGJicGSduCla6aikylGFr+H8vtGEEG5Go5EWLVp4uhjiInhlkwu4J+my57iH/x8fXCQ9XYQQVzKvDXRDdBSO0wYXyfB/IcSVzGsD3RgdA04njry8mhkXJdCFEFcy7w30mBODi8y+Mp+LEEJ4baAf74tuz84m1OSeB13a0IUQVzKvDfSTVy4K8glCp3QyuEgIcUXz2kDXBQej/P1xZGeh1+kJ8QmRwUVCiCua1wb6icFFJ1YukjZ0IcSVzGsDHcAYHVUzWtRsMksvFyHEFc2rA90QfepoUbkpKoS4knl1oBujo3Hk5aHZ7e4mF6mhCyGuYF4d6IboKNC0msFFVpsVl+bydLGEEMIjvDrQjTExgLsvutlkxqk5Ka0u9XCphBDCM7w70Gv6omedWCxa2tGFEFcorw70mpWLsnNkgi4hxBXPqwNdHxSELiAAe3a2TNAlhPAKmqbhcDbMvT6vXeDiOENMNI7sLML8wgDIq5TVVoQQ5+9AbikrknPxM+q4oXMMkcFnXyxH0zT2ZpeSdLiIqmon1U4X1Q5XzU+bw0lplQNrhZ3iSjslle6flZUVTOsXz0M39qz3a/D6QDdGx2DPyibWP5IAYwD7i/Z7ukhCiItQWF5NUUU1/j56/Ix6fI16TAZdvS9erWkau46WsHx3Fst3ZXMwr7zmvReXJdOnZRijusZyfacYQvyNALhcGtsyrPywK5vlu7M5XFDxm+P66HUY9QqTUU+ISdHRJ5t+KpV2+v209EkhxnWAjKppgAT6bxiio6jatxed0tHG3IaUohRPF0mIy0peqY1dR4vpGBdMZFDdlmjUNK1eArTK7mRfdikJUUH4+ejPer71qYV8vv4wP+zOxuE6daFqnQI/o57QQB9aRQTSOiKQ1pGBtI4IoF3VNvytKWS3uIm0MiOp+eUcyi8nLb+co0WVGPQKfx/3F4OfUY+/jx6dTrEhtZCj1kr0OkWflqFM7tucaztEU1pl55vtmSzdnsmfFu/k2a93MTAhgugQX1Yk55BTYsOoV/RtFc60ga0YkBBBsK8BH4MOH70OBZCyHNa9CUe3QMWxLwpTMMR0hbjraJZw/UV/tmfi9YFujI7BmV+AVl1N29C2fJv6bb39xyiEtyqusPPD7myWbs/k14P5HM/H1pGBXN0yjKtbhdGnZRihAT44XRqpeWXsPFrsfmQUk5xVgl6niDP7EW/xI9bsR5zZjziLH81CA2gTFYiv8cwBXWV38tO+PL7bmcWPe3Ior3bio9fRs5mFa9qEc03rcDrFhaDHRUm1i8WbM/h8QzoHcssI8TMypV9zOsaGUGl3UlntpNLupOrY7zmlNg7klpF0MIdrXb/Q2fAdgbrDAPhps/jRcTOfOa9Fb/SheVgATUL90TSNimp380deqQ2f6kL62n5lepCDhG5BtI4MxN+oBwewA6IDIni0QwemD0xkZ56Tb7Zn8s32LP53IJ9BCZEM7xTN4HaRhPgZT71wTYPUn2DVLDiaBJbm0GMSxPaAuB4Q2gp0DXvb0vsDPSYaNA17bi4JlgTm2+eTWZ5JXGCcp4smxCVVXGFnzf48lm7LZG1KHtVOF83C/Ll/cGt6twhjV2Yx6w4WsGhLBp+td4dgi/AAckqqqKh2Au5acIfYYMb1jEcDMq2VZBRVsiGtkNIqR825dAqahQXQNiqIhOgg2kUHoYDvd2XXhLjF38iU9i5+55dCWc4hKvIz8D2Sg2m1lTJdIUFU8IvWh3dsdxDbpAWv3dKFkV1ja/2iAKCyCDZ/jLbhfVRpFuXBrVgT+yxpuuZcl/MBzxV8xlPha9Fd+wK6jsPheMXOaYf9K2DbXHft2eUAK+7H3jOfSgFdzM3oEtmBp3q1xxmegCE8DCw+4HtadKavdwf5oZ8hOB5GvgXdbge98YzHbiheH+iGqBMrFyU0TQAgpTBFAl1cctUOF6n5ZZRVOWgS6k9EoAmd7sx/KbpcGketlRzIKyO9oAKXpqHXKfdDuX8a9IogkxFLgA+hxx7BvgaUUlQ7XOzJKmFbehH7Dh8lMyMNu/UoURTR3a+Ue+NsJPiXEWLPR+3Khh12run7ENPu+j129OzIKGZ9agHbjlgZmBBBp7gQOseF0CoiAEPhAfj5b+CyQ4v20LsDRLanxC+OzGIbaXnl7M0uJSWnlH3Zpfw3ObvmL4BQPz33JVgZZdpGfM5q1L5jaal0EBiFPTSafJXAblsQuaV2bnSsYFjQLvS9nocefUBXS5jn7oFN/4Zt88BejmoxEEb9k4BWQxmo0zEQQBsLB37EsOJZWDgZ1veCfg/DkQ2wfT6U50JABPSeBt0mQmjLM5xIg9IsyEl2nzPX/VMdWIHBdeILDZ8gCG0OlhZgK4XU1RAQCde/Cj0ng8F0Qf8NXSyladq5t2oAiYmJWlJS0kUfx3bwIKk3jiD2tdcwDB9Mn3l9uL/b/UzrOq0eSimuZIXl1Ww6VEiV3YnJoDvWRqrHx+C+6ZVbaiMlu5S9OaWkZJeSll9+StuvyaCjSag/zUL9aRLqT4ifkUMF5RzILSM1r5xKu7tW7IsNC2WEqHLMqowQyglRZZgpw1/Z8MWOiWpMVOOv7AQZHAQ4S4ikkChVRICy/bbwviEQFHPiUZrpbg4IT4Dhr0Drob/dpywXfnoFNn8MRn/wt4A1/cT7Rn+IaAchcWDwA6MvGPxw6E0U2hSU5RGRtRpVmgVKD836Qrsboc0wd/PDmcK64CB8+6i7bLE9YOQ/3O3MAI5q2LvMHeSH/wd6H+g0FvrcBzFdav+Hczndwb/6ZXc46wyQMNwd4m2uvbBas6MaitKgMO23P+2VcNU9cNW94ON//sc+T0qpzZqmJZ7pPa+vodeMFs3OIsQYQJOgJnJjVFyQqmoHO5J3c2hPEuVHdhBSeoCWKov9rjiWuRL52dWZKn5b82oS6kfbqGCGdYyibXQwQT6K3NxcCvKzsRYeoTw/n/LUAjRnMYm+pYzzLSXWXESYq5CA6jwM9rNPV+HS+eDQ+2JXPlQrE9UYqTYGow/pjhbZFC08HhUcC0HRxwI8GnwCfnuglB9g+Z/g8zHQ9ka47mUIbQHVFbDuHfjlH+CogsT/g4EzIDDCXfvM21dTUyVnN+Tvd4eYowocVRjsVUQ6bWAMcH9RHA9x/9Bzf+hhrWDSV7BzIfzwJMwe5K5B+wTClk+gLAfMTeF3L0D3OyAg/NzH1OndbdedxkDaWohLdF/LxTD4QERb9+My5vWBrgsIQBccXDONboIlQQK9EXG6NDYfLqLc5vjNexoaJZXuG115ZTb3z2MPpdztwy3CA2geHkDLY7+b/X3IKaniqLWSTGslOXn5RB36mrZ5y4mzpXKVquSqY8cv84tEC21J58JtjKtei0vvS3HsNeTFXUtO9ECCgs20NWTjV5QCuWvcgZe8B4qPAKf95as79tAMYDgWukGdIDgWAqPAPwz8LOBndv/0Nbt/Nwag0+nwAXyAM8R03SVcBy0Hwfp/wZrX4J3e0O02d9CXZkG7Ee7gDG99Yh9TEMQnuh9n43K5r7m2JpOzUQq6jIM2v4OVL7jLh3J/KfS62/0lcSHH9QmAtg3Tm+Ry5fWBDmCMisKe4165qK2lLavSV1Fhr8Df2PB//oiGkVVcyYJNGczflE5mcdU5tzcZdEQEmWgdUMH9uv8SbM9nXUYbliW3JN11olanlLszQoI6wh36ldyu/5lAVUWavgV7I28guFkXmrVPxDe2I4F+7tHHOO1w+Bd0e7/FsvdbLEdWkqCO9VY4PrunzuhuzmjaGywT3LVT32PhXBPUoe7gbuCeDmdlMME106HLBFj5vLt5Ja4n3DLH3URyoerjmvws7iaXqx9w14jNTS/+mFeYRhHohphoHFnulYsSLAloaBywHqBLxFna2USDqqh2UGV3YfE31q0LqabhzD/A+vQy5u0o4/uUYlyaon+bcJ68oT1NQs/85RzkayAiyERQyQHUundgxwJwVoMpiEG2b3nSBxyBseSHJXLQrwtWpw+J+V8TVbQZl96Eq8NN0PteWsT1pEVt5dQb3TXbloPcN72ytp/oKRHZwf0Ia3XJezRclOAYGDPbfT2+ISd6g1wOTv4LQZyXOgW6Umo48CagBz7UNO2V094PAT4Hmh475uuapn1Uz2WtlTE6hqpduwFICD3W06UoRQK9HhzvjXGooJyYEF+ahwVg0J+5NpZbUsWKPTmsTM7hl4MFVDtcBJoMNAn1p2moH82O9Qs2+xmxVlRTWG6nsNyGwXqQmzPfoJNtG/2AfoDTVw++FvSVoZBkgZAm7vZeSwt374TQFu6mirQ18MPbcGCF+0Zd94nQ5373NrnJkL4Ow+FfiD70C9HlS90FNTeDa19E1+0OdAFh5/eBKAWx3dyPxsDP7OkSiHp0zkBXSumBd4BrgQxgk1JqqaZpySdtdj+QrGnaSKVUBLBPKTVX07TqBin1aQzRUTgLC3HZbMQFxuFv8Gdf4b5LcepGxeZwknSoyN0l7Vjvjf05pTV9lME9rLlVZCBtowJpGx1Mq4gAUnJKWbEnl+1H3FMXNw31Z1KfZsSa/ThSWEF6YQUH88r5aV8eNseJSYlMVPOI7zKm8hXVypdFYb+nTbMmdLA4MdiK3X2OK4ugogAyNsHuxSeaOMDd68FZ7e4uNvgZ9828kwM6upP7cdU97naWwlQoz4P4qzzb7CFEA6lLDf0q4ICmaakASqkvgNHAyYGuAUHK/bd1IFCIe9zVJWGMdi904cjJwadpU7kxeh40TWNHRjGLtmTw9bZMiivtAIT5G+gV4eL6DpW0DygnzqeMgmoDaWVG9hbnsztVx6ptBkrxR0PRK96fZ4ZEM6RVEC3MepSjCnwUBLcFvfs/M5dLI6/MhrXCTmTBesw/PosqPAidx+Fz3Z8ZGxh59sI67e5udDXdxg5BZHvodIu7C93ZKOVuFglrVQ+fmhCXp7oEehxw5KTnGUDv07Z5G1gKZAJBwHhN+/p45mIAACAASURBVO1acEqpe4F7AZo2rb8bHsaY4wtdZNcE+vdp38sUAGeRW1LFkq1HWbg5g/25ZZgMiiea7mNs9VKCbDnoy3MgxwE5J/ZpzWn/8L6goVBokI/78etpJ9IZ3De3Qluis7QgKrQFUVnbYcd8d7PIpCXQakjdCq03SigLcRZ1CfQzJeLpo5GuA7YBQ4BWwAql1M+appWcspOmzQZmg3tg0fkX98xOLHThvjHaNrQtC1IWkFWeRWxgbH2dxmu4XBr55Tayi6vILq4ip6SKrOIqskvcv2cXV5GWX45Lgx5NzbxxQwwjM17HmPKte+BIywEnBqQEH/sZEA4O24lmkEorVBahqo6tEGXwBaPfqT9tpe5mjqI0988jG8FW4u4RMuAJ6P+oe1shRL2oS6BnAE1Oeh6PuyZ+sinAK5p72OkBpVQa0A7YWC+lPIcTg4vc1ckEi/vG6L7CfY0m0I9P21lYVk2ZzUGpzUFZlYNym4PSKjt5ZScCPLfU9pvZ6gw6RWSQiagQX9pGB3Fjl1hu6hpDy8xl7sEm9kp3H+SrH6hpIql3mgYVhe7fz/dmpBDinOryf+4moI1SqgVwFJgA3H7aNunAUOBnpVQU0BZIrc+Cno3Ozw99SAj2YzX0NpY2gLuny+Cmgy9VMS6erQw2fwQHfnT3Vw6OQQuKYXuxH5/ttrM1X4eZMqJUEdGqkChlpbUqJFJfynZTT8rDxtC8VRjRwb5Eh/ie8jMs0IT+5HlFijPgmynu3iFN+sDotyG8TcNen1IS5EI0oHMGuqZpDqXUA8APuLstztE0bbdSatqx998DXgI+VkrtxN1EM0PTtPwGLPdvGGJiakaLBhgDiA+MZ1+Rl/R0qbTCxg/cI+QqCyGyA1pRGq7kTPSuaroB3YDTR51reh8IikYZA+ib9xF/qPgRrn4eOo6pvReHNd09z8Wvb4PmdPdD7nWP9PoQohGo09/WmqZ9B3x32mvvnfR7JjCsfot2foxRUdizs2uetw1te/mvXlSe7w7xjR+425YThqP1/yNrKprz9xUpbK+w0tHi4uGrAhka50RfWegegRgUDUGxKP/QEwNCDq6GFc/Coqmw7m0YNguaX+N+z14Je5bB1s/cc1ugQZvr4IZX3ZMmCSEahUYxUhTco0Urt2+veZ5gSbi8pwBI+gh+eModth1GUdH7ERZlhfH5wsPsy9lEvMWPV8d25eYecRhrGchzilaDocVad++RVS/BxzdCwvXu8N+1GGzF7t4mg/4EXW8DS7OGv0YhxCXVaALdGB2D02rFVVmJzs+Ptpa2aGgctB6kc0RnTxfvVOkb4Ns/QvNrSLtqJnP2+bD43xmUV2fRMTaYV8d24abucfgYzrMZRKdzT7bU8SZ3zf/nv7uHp3cY7R5B2ewaaVoRohFrNIFuiI4CwJ6djalFixM9XYr2XV6BXlEIC/+PqoA4fl/xEGs+ycLHoGNkl1ju6NOUbk3MF9933ugH/f/onoYUzjyVqhCi0Wk0gX7yaFFTixbEBbmnALisRoxqGtrX9+EqzWa8bSb5Dh1P3dCOcT2bYAnwqf/zSZALcUVpPIEe5+5vbjt4kIA+fdApHW0sbS6rQLf/+g7Gfd8zyz6J6PZXM+/WbgSYGs0/gRDCwxpNg6oxPh6f1q0o+WZZzWttLW1JKUzBU8vsnawgZR2seJ7/OnsSMuhB3p3YU8JcCFGvGk2gK6UwjxlL5bZt2A4eBNw9XUrtpWSXZ59j74a1Y/8hqubdSZ5mRn/zv3jk2ra1Lh4shBAXqtEEOkDIqJFgMGBdvBhw90UHPDLAqNzmYPPhQt5ZtZ+jn91LFAVU3/whQ3u0u+RlEUJcGRrV3/yG8HACBw2k+OulRD7yyClTAAxqMqhBz73pUCEb0wpJziwhOauEQwXlaBrcoV/B/cYNVAx8nubdvGgaAiGE12lUgQ5gHjOWspU/UvbzzwQNGeKeAqCBF7v4bP1hnv1qF+Be3KFDTDA3d49jgJZE118+RWs9DP+BjzRoGYQQotEFeuCA/ugjwrEuWkzQkCENvtjF4i0ZPPvVLn7XPpI3xncj2PfYupJpP8PnD0NMV/cCvDKgRwjRwBpdyiiDAfPo0ZT99BOOvDzahrYlvTSdSkdlvZ9r+a4sHvtyO/1ah/H27T1OhPnRLfCfCe51L+9YBKagej+3EEKcrtEFOkDImDHgdFK89BsSLAm4NBcHrQcv/sBOh3sSrOpyftqXy4P/2Ur3phZmT0rE16h3b5O7Fz4f655Ea9IS908hhLgEGmWgm1q2xK97d6yLF5NgPrHYxUVxOWHJ7+Gzm3C81pbDcx9icHgxcyb3OtGfvOgQfHaTe6m0O7+G4MaxuIYQwjs0ykAHMI8dQ/XBg4SlFhBgDGBj9kUsnuRywdKHYNdCcjvdzQ/VXbhdt4LZxdMIWTAWkpdC8VH49Cb37ImTvnKvlymEEJdQow30oOHXo/z8KFm8hHEJ4/g+7Xt25+8+/wNpGnz3GGz7nI1N7+Ha3dfxSsBjFN27FYY8614rc8Ek+EcnKMt1t5lHdaj/CxJCiHNotIGuDwwgePhwSr77jrtbT8Lia+GVja+c1zQAqbmlbPvwPkj6N+85RnJryiASogKZO7UPkbFNYcBj8PB2uO0L6Hgz3D4f4hMb8KqEEKJ2ja7b4snMY8dQvGQJ2k+/8nCPh3n+1+f5Pu17bmh5w1n323y4iGe+2sWNeR/wgOFrvvUbjbHvTH7tHEOs+bRV6nV6aHu9+yGEEB7UaGvoAH49e2Js1pTiRYsZ3Wo07UPb88bmN6iwV9S6T2mVnQfnbeHm0nk8YPia8k6TuPGJT5jav+Vvw1wIIS4jjTrQj0/YVbFpE84jGfzpqj+RU5HDnF1zat3n5W/30K/sB+51zIOutxEw5q0T63YKIcRlrFEHOkDITaNBpyP/X+/SPbI717e4no93f8zRsqO/2XZNSh7fbdrDC35fQLN+MOptGeEphPAajT6tjFFRhN19N8Vff032zBeY3v0RFIo3kt44ZbuSKjt/WrSDmcHf4Ocshev/CvpGfYtBCNHIXBGJFTHdPTFWwezZhNjt/N+4yfxrx3tsyt5Er+heALy8bA9BpQe42fQdqudkiL6M1iEVQog6aPQ1dHC3pUdMf4Tw+++nePFibpybSpxvNH/d+FecLic/7ctlflI670csRJkCYfAzni6yEEKctyuihg7HQv3BB1BGA3n/eJNZpd25u/cOPt+9gHe/ieROy25aFG+E4X+FgDBPF1cIIc7bFRPox4VPm4Yy+pD72mvMtIbyAq9g04/gacNSiGgHvaZ6uohCCHFBrogml9OFTf0/op56irbbC5n+pQlT+Ne84lOKfdhL7om1hBDCC12RgQ5QMXIsc3rdSq+0Ut6cb+drv0CmHvic/Mp8TxdNCCEuyBUZ6JXVTn7/2WaWt+qLZVQrItIVHy6PJzV7D+OXjWdX/i5PF1EIIc7bFRfomqYxY9EOkrNK+PhaRbT/z8Te2Qe/5CN8sLwpAdU67vr+LpYeXOrpogohxHm54gL9g59TWbo9k5eu8aXnpj9CYBQhf/wnca+/Brv28cbXIfQO7MTT/3uaF9e9iM1p83SRhRCiTq6oQF+bkscr3+/lvjZFTNx1N1RXuKe+NQURfMMNxP3j79j3pPDY52X8vvntfJnyJZO+m8SR0iOeLroQQpzTFRPohwvKefA/W7kjdC+PZz+O8g2Gqf+FuB412wRfey3x/3yL6v0HuOH1dbyT8AwZZRmM/2Y8P6b/6MHSCyHEuV0RgV5uc3DPp0ncxCpeqHgZFd4Gpq6AsFa/2TZo0CCavPcu9txcoh54jbnGaTQJbsIjqx/h9U2vY3fZPXAFQghxbnUKdKXUcKXUPqXUAaXUn2rZZpBSaptSardSak39FvPCuVwaj87fynUFn/GC9i6q5UCY/C0ERta6T0DfvrRcshhTQgJVz/yFv61rze3Nx/JJ8idM/WGqdG0UQlyWzhnoSik98A5wPdABuE0p1eG0bczAv4BRmqZ1BMY1QFnPm6ZpvLx0O/1T/sIfDV9Cl/Fw23wwBZ1zX2NsLM0+/YSwe++ldPESxr++mTeaTmdPwR7u+O4OUotTL8EVCCFE3alzrbGplLoamKlp2nXHnj8JoGnaX07a5j4gVtO0Os9qlZiYqCUlJV1Qoetq3g8/0+GXh+mmOwj9HoGhz1/Q/OZl//uFzBkzcJWX4/zDRP5WvAicTh7ocj8Jwa3A5QSdDv/ERHR+sqqREKLhKKU2a5p2xsWL6zKXSxxwcjePDKD3adskAEal1E9AEPCmpmmfnqEg9wL3AjRt2rQOp75w676fyw3rH8dkANfYT9B1vOmCjxV4TT9afrWEo088QcXf/83jNe+8csoHY2rblvh33sYnPv4iSi6EEBemLoF+pvXXTq/WG4CewFDAD1inlFqvaVrKKTtp2mxgNrhr6Odf3DpwOjiy6CmuTn6fNGMrYu+djy6yzUUf1hARQdMPP6Ry61a06mrKXVW8tf1t9lhTuLX9BIabepD9wgscumUccf/4BwF9Tv/OE0KIhlWX9ocMoMlJz+OBzDNss1zTtHJN0/KBtUDX+inieSjJpPyD62mS/D7f+gwn9OE1mOohzI9Tej3+iYkE9O1L5DVDeG7aF7S65gZmlc7nzdAk4r+Yiz4sjPSpUyn87HPO1ZwlhBD1qS6Bvgloo5RqoZTyASYAp4+L/xror5QyKKX8cTfJ7Knfop7DkY043+2PLns7Lxgepuf9nxASdO6bnxfDR+/DKwNeYWqnqXyZ8iXDN07hy8d64uzTjZyXXybr6WdwVVc3aBmEEOK4cza5aJrmUEo9APwA6IE5mqbtVkpNO/b+e5qm7VFKLQd2AC7gQ03TLt0MV7uXoC3+PdkuC/fzF16951aiQ3wvyal1SscjPR/hquir+OrAVyw88i3z+lcx1SeEYYsXU5KSTKt/vY8xsvZukkIIUR/O2culodRLLxdNg1/+AStnUhrRkwFH7uHF2wYysmts/RTyApTby/kx/UeWHVyG9tM67vvGQVVoAF0Xf4t/WJTHyiWEaBzO1svFe0eKOu3wzcOwciZ0HMN/2r1NEcFc0zrco8UKMAYwqtUoZg+bzZ+fWcXup27GP7+cXyeOwFqc69GyCSEaN+8M9KpimHcrbPkE+v8Rxv6bpKMVtAgPwBLg4+nS1Yjwj+CO2/5M0Z/uIuZQGasm30h28VFPF0sI0Uh5X6Bbj8Cc4ZC2Fkb9E4Y+h6YUW9KtdG9i9nTpzmjQpD9R9fAdtN9Txrf3j+Zg0UFPF0kI0Qh5X6Bn74SSozBxIfS4E4CMokryy2x0b2bxcOFq1/MPT8OdY+mbVM7CJ8exLXebp4skhGhkvC/Q290AD++AVoNrXtp6xApw2dbQj2v35EsYRgxj1E+VzH1lMj8d+cnTRRJCNCLeF+gAfqcG99b0InyNOtpFN2y/84ullKL1X17Hp18fJn9v49P3H+SLvV94ulhCiEbCOwP9NFvSrXSJN2PQX/6Xo4xGWrz1Nr4dO/LoYidJ/3qJ1za+iktzebpoQggvd/kn4DlU2Z0kZxbTo+nl235+Ol1AAM0/+ojggQOZ+l8X/q9/zOMrplPpqPR00YQQXszrA313Zgl2p0b3ppd3+/np9EFBNHnnHcL+MI0hOzQGvLKChxfcKYtnCCEumNcH+tb0IgCvC3QApdMR+fDDxL35Jq0LjUx+YzdPvzeOVKssniGEOH+NINCtxFv8iAy6NHO3NITg64bRav6XmIMjuf/DbGbPGsfa9J88XSwhhJdpBIFeRHcvaj+vjW/bBNou/grfnj24a1kFlRP/wKJ3HsHpkEWphRB149WBnl1cRWZx1WXf/7yu9GYzbT76lPBXZhFsCKLDP39g45DeZM2fi2aXYBdCnJ1XB7o3t5/XRun1RNw0lt4r13Foxq0UqSqsz89i7++GUjhvnsyvLoSolXcH+hErPgYdHWNDPF2UeqfT67l+ygvELpjLv+4IJcVYQM6LL3Fw+HCsixajORyeLqIQ4jLj3YGeXkSn2GB8DF59GWfVLao7z//xa5b8MZGXJug4pC8i6+mn2T/iRkqW/yDL3AkhanhtEtqdLnZkFDeKG6LnEu4XzgfXfcjoCc/x8cPteX2MjsOl6Rx95BG2jxxG4ZpVEuxCCO8N9D1ZJdgcrkbVfn42Rp2R8e3GM3fEPJ6ZsYw9b/6ez8eEUpyXQc7v72fDiMEULP1abp4KcQXz2kDfmu6eYdGbhvzXlxYhLXgo8RFeevln/L/8N7/c1pHyohxyn/gTe4YMJH/2BzitVk8XUwhxiXlxoBcRFWwi5hItBn050ikdvZr25e7nF2Ka/z6zJ0WwM8BK3htvsH/QILJmzsSWKqNOhbhSeG+gH7HSvYkFpZSni3JZuKbJAF5+YjmHX5rM43cb+KWDnqJFi0i9cQRHHniAiq1bPV1EIUQD88pAzy+zcbiggh7Nroz287ryN/rzRK8neGXyf/huQnPu+YPG+uuaULJhHYdvu51DE++gdPVqNJdM1StEY+SVgb7tWPv5ldDD5UJ0Cu/EFyO+4O4Bf+Tjq6u4614by0ZGUpx+gIw/3EfqqFEUf/ON9IwRopHxykDfeqQIg07RqREOKKovRp2RKZ2msOKWFTwzeBbr+4dz15Qy5twcRIHNSubjT5B+513Sxi5EI+KVgb7lsJX2McH4+eg9XZTLno/eh5ta38TCkQt5//p/U/m7q7h7opXZ1+sp3LWFA6NGcehvr+Cy2TxdVCHERfK6QHe6NLZnWOlxhfQ/ry9KKXrH9ObtoW+z9OZltLzz97z2aFN+SXBR+cEnrPtdb5Z88SJZZVmeLqoQ4gJ5XaCn5JRSUe2U9vOL0DykOQ92f5B5k75j6Jxv2PHUTWgujXYz/8O3k4by1tt3cSBzl6eLKYQ4T14X6PuyS4HGNcOiJ7Uyt2L8nX+h748bME6eQJ+DOq59eyOVvxvHj6P6sfcff6Zy927pGSOEF1Ce6umQmJioJSUlXdC+heXVWPyN0ge9AbhsNnLXr2XLsjnoNu2gWfaxIDcHY752GMHXX4//VVehDAbPFlSIK5RSarOmaYlnfM8bA11cGsW2Yub/+j57/juf9vsqueqAwlTtwmUOIuS64VhuGIF/Yk+UXm5OC3GpSKCLi1JSXcKS/UtYl/YTrnVbuCrZTuIBDZMdqs0B6Ab2IeLaG4jsPxSdyeTp4grRqEmgi3pTYa8gKSeJDWlrsK76kdZbcumWquFrhyojHGpnpjCxNfprehEV2waLr4VQ31BCfUMxm8wYdGduqnHZbLjKyzGEhl7iKxLCu0igiwaTVZbF/txkCn9Zg/7XrURtOUxQsR2XgsxQKApUWAPBGgDWQIXN7E+Qn5mESjNNSoyEFtgwZhXizMkBTcM8YTyRjz2OPjDA05cmxGXpogNdKTUceBPQAx9qmvZKLdv1AtYD4zVNW3i2Y0qgN06aplG1O5nClT9Qunc3jrxctIIidIXF6KpPXTbP6g/ZFsgN1VEdHUpTl5k2qw5iiI4i7uWXCejb10NXIcTl62yBfs6uCkopPfAOcC2QAWxSSi3VNC35DNv9Ffjh4ossvJVSCr9OHYnr1PGU1zVNw1VWhiMvD626GkN8PPmqjMqCZDIKdpNckMx/8rYTE6vjvu9ycf7fVEqG96btc3/GHBrroasRwrvUpe/ZVcABTdNSAZRSXwCjgeTTtnsQWAT0qtcSikZBKYU+KAh9UFDNa9EEEh0QzZCmQwCwu+xsztnMqv7LCfxkGUN/2MDOX4fy8x2daTVsLAObDCQ6INpTlyDEZa8ugR4HHDnpeQbQ++QNlFJxwM3AECTQxQUy6oz0ielDn5g+uPo/x641i/F98W/c/K+dVH2wk70BsCPYD9+IaMJjWxEe2wq/bl0JvOYalNHo6eIL4XF1CfQzjd45veH9H8AMTdOcZxvso5S6F7gXoGnTpnUto7gC6ZSOLoNuwXX1SKxfLiT/4C5KM/bhyDlKxZE0rHvTcFWuRO+CyiAf0ns3I2dgO5ytmxHgE0ikfyTtQ9vTJCCO6uQ9lK39mbKf1+LIy8M8diyW22/HYJHpI0Tjcs6bokqpq4GZmqZdd+z5kwCapv3lpG3SOBH84UAFcK+maV/Vdly5KSouVGFVIT9n/Mzaw6sJ3LyfDptyaZ9chtEJR8JhbScdhYHQLVWj6yEIrtDQFFQlNME/JAxt4zaUry/mMWMInTIZnyZNPH1JQtTZRfVyUUoZgBRgKHAU2ATcrmna7lq2/xhYJr1cxKXkLC6m5PvlWL/+iqqt2wBwhASQ2TGSLS1gRXQueT7uKYJbF/owfosfnbcUoTQN/eB+xN1zPwFdu8p0EuKyVx/dFm/A3ayiB+ZomvayUmoagKZp75227cdIoAsPqk5Px1laim/79iide/45p8vJoZJDJBckk1yQzO6C3eQcSmbQhgqu3aoRYINiiw/F3VsS2H8Ara+9majw5p69ECHOQAYWCXEGTpeTtOI09mZsoeS77/HbtIdmKcX4VYNDBwebm7B2bY7FEkO4FoDFaSLYbkRXUYWzvAzsjjMe1xAVhXnsGHy7dJEav6h3EuhC1FFVZRkpa5dSsHoFpo27sWSW1rznVFBpApuvHpefCZ2PDwadAb0yYNDp0ev0GDCgjmRBZSWmdu2wjL+V4JEj0QcGevCqRGMigS7EBXIUFFBlr+SoVkS6LYtDJYc5VHKIQyWHyK3IpbCykGpX9Sn7+Nk0Bu81MGybIjazCofJQOnAbhhGXItq0xL8TOiUDoVCp3QYdAaaBzcn0EdCX5ybBLoQDUTTNCocFRRWFVJYVUhRVRHZ5dkcLjnMkZJ0XMkpdP5fFlcnOzEda6HJDYHMUMXRcMgIU2SGKfKDwT86noSoDrQLbVfziPCLkGYbcQoJdCE8yOFykJm9n4K1q1HpR9EdzkSXnokuPRtlO7V2X+avIz/QRWGgoigI8iNMVDWJQLVqhrlJK+KC4okLjKNZSDOaBjWtdfbKurBnZVG1dy/+iYmnjOAVlzcJdCEuQ5rLhSMrC1vaIRw52dhzcnDk5FKVdZSKrCM4c/IwllTUbF/p4+5nnxGuKAwCzaAn2D8US2AEYcFRRARFEx3WnPC4VujDwzGEh6M3m2t6+tizs6nYuJHyjRup2LgJe3o6AHqLhYiHHsQ8bpysROUFJNCF8FJOqxXbgQPYDhygav9+ylP2Ur3/AMpaUrcD6HUYQsPAaMCRmQWALiQEU49uOLu1oyLGgu/85Tg2b8OnVSuiZjxBQP/+v2nmcVVVUb5+PWVr1uAqKcWnZQtMLVvi07IVPs2bycIml5AEuhCNjKZp4HSi2e1odjulFVYOFRzgQOZOUtO2kH1kL3prGeZyjagqXyJUEOlNTGyPd7LTXEKFs+rkgzHgkD+3/1hNaJ6N0m6t0D80ldjo1vhs2EXFmrWUr1+PVlWF8vfHYDZjz8qC49mh02GMj8e3QwfMY8cS0K9vzV8F58uWlob1i/nogoMw33ILxqioevi0GhcJdCGuMJqmkVacRlJOEkk5SRywHsBsMhPuG06YXxjhfuGE+4UTYgrhaNlRDloPkpqfQtOVyYxcU4l/FRyP5HyLnkOdwino0Ry6dyI0KJJglwlLXiWBWcX4HS3AkJ6DtmUnrsIijE2bYhk/npAxN9d5vpyKLVsomDOHsh9XgcEAdjvo9QQOHoRl/HgC+vU745eEpmnYj2biKi/HlNDmiriBLIEuhKgTTdPIytpP1idzKKaCtE5hHAipJLsih6zyLHIqcnC4ahlQ5dC4IT2U3212En2gEM1owP+6a4kcfxtERZBLKZmuIjJs2WSUZZBXlkPfgwbaLd+HY8du9CEhmG+/jdCJE3FVVmJd8CXWRYtwFhZijI/HfOut+HXuhG3/AWz792NLScF24ACu8nIATB3aEzrpToJvuL5RNwFJoAsh6oVLc1FaXUqZvYzS6lL379VllNpLySnPYU/hHpILktGlZjBsq4sBuzT8Tu3Ig0PnHqCllI7AChc5Ztg1tCWx4+9gaNsbCDGF1GyrVVdTunIlRfMXULFhQ83rerMZU0ICpjZtsDWPpri8AP9v1mI/mIo+NBTz+FuxTLgNY1TkKefWNA2n1YqzoABjfDw6X98G/bwaggS6EOKSKrYVs6dwD/sytlG9IYkwhy9hLn/MThNBDiN+dtBsNsp7JPBD82K+P/wD6aXpGHQG+sX2o3dMb5oFN6NZcDNiA2Mx6ozY0tJwZGfjah7HVkcav2atY13mOlKLUwHw1/sxtiSBgevLCN6U4m6yGTQQpRSO3Dzsebk48/6/vbuLbesu4zj+/cVOnPgtUbomS5o0pWJUVBWUTmqTBco6ASowUSQEGhLSLpAQEhdDAqHBDQJpt4gbbhBMTBovmgSDCWkT1QClN90rqfq2riFxSpMuaZOmeandOM7DxTktWdNwEdfxfPx8JOuc83dsP79EfnL0P8fH17BiEQAlEiQPHiR9+DDpw5+iqa9vXQ4zY2VqiuWxMUo35om1tRJrawtura00tLRs6e8VvKE75z7gzIxzs+d4efRlXsm9wtTNqTv3xRQLzr3P9nFz5Sanrp5iZXWFRCzBw50PM9A1wI7MDk5OnmRoYoj3lt6j87rxxNk2DpwLDuTqgXbiHR00d3aR7uqlZVsH+TNnWBo6wXIuB0Bj307Shz9NLJNmOZfj1liO5VwOy+c3rFuJBPGODpKHDpIeHCTZ31/x6+x7Q3fO1QwzY+7WHOPz4+tusYYYh7oOMdA1wIHOAyRiiXWPHZkb4cTECYYuDzE8PUzJSuteoyXeQm+ml92tu9mbb+cjFxZpH86ht89ixSLx7i7o66HU00Ghu53FB1spZppJ541k3mheKtK0WCC+WKB0eYKlk6+xurAAEs379pEafITUwCM0dj1IQ/jVi/frIqCr8wAABIdJREFUHH9v6M65ulQsFZktzDJTmGEmP3NneS1/jUsLlxidG2VicQILv4StqRRcX6fQsP6fwEYyjRk+nNlN/41t7P33MttPT9JwbgRWV9/3c0omUSYNqSSprxyj55vf3lSm/9fQ/WNhzrnIaow10pnqpDO18fnshZUC4/PjjN4YZfTGKMVSkWwiS7YpS6YpQ7YpSzaRJa74+w4Gzy/Ps7i8yNX8VS5ev8jzDW+wsGsBdkHys2JgZjuppRKxpQLxm8u05POkbgWnhCbnT/G1CuT1hu6cq2vN8Wb2tO9hT/uesp7HzLiydIULsxd49/q7jM2PIcVpaUyRbEySjCfvLMt9rY14Q3fOuftAEt3pbrrT3RzZeaQqNWzu87nOOec+cLyhO+dcRHhDd865iPCG7pxzEeEN3TnnIsIbunPORYQ3dOeciwhv6M45FxFVu5aLpKvA+CYf/gBw7T6WU0vqNbvnri+ee2N9Zrb9XndUraGXQ9KbG12cJurqNbvnri+ee3N8ysU55yLCG7pzzkVErTb0X1a7gCqq1+yeu7547k2oyTl055xz69XqHrpzzrm7eEN3zrmIqLmGLumopAuSRiQ9Xe16KkXSs5KmJZ1ZM9Yu6biki+Gysl8vXgWSeiX9Q9J5SWclPRWORzq7pGZJr0s6Feb+STge6dy3SYpJ+pekv4bbkc8tKSfptKRhSW+GY2XlrqmGLikG/AL4PLAX+LqkvdWtqmJ+Axy9a+xp4FUzewh4NdyOmhXge2b2UaAf+E74N4569lvAY2b2cWA/cFRSP9HPfdtTwPk12/WS+4iZ7V9z7nlZuWuqoQMHgREzGzWzZeAPwLEq11QRZjYEzN41fAx4Llx/Dvjylha1Bczsipm9Ha4vELzJdxDx7BZYDDcbw5sR8dwAknqALwK/WjMc+dwbKCt3rTX0HcB/1mxfDsfqRaeZXYGg8QEdVa6noiTtAj4BvEYdZA+nHYaBaeC4mdVFbuDnwA+A1TVj9ZDbgL9JekvSt8KxsnLX2pdE6x5jft5lBElKA38Evmtm89K9/vTRYmYlYL+kNuBFSfuqXVOlSXocmDaztyQ9Wu16ttigmU1K6gCOS3qn3CestT30y0Dvmu0eYLJKtVTDlKQugHA5XeV6KkJSI0Ez/62Z/SkcrovsAGY2B/yT4BhK1HMPAl+SlCOYQn1M0vNEPzdmNhkup4EXCaaUy8pdaw39DeAhSR+S1AQ8AbxU5Zq20kvAk+H6k8BfqlhLRSjYFf81cN7Mfrbmrkhnl7Q93DNHUgvwGeAdIp7bzH5oZj1mtovg/fx3M/sGEc8tKSUpc3sd+BxwhjJz19wnRSV9gWDOLQY8a2bPVLmkipD0e+BRgstpTgE/Bv4MvADsBC4BXzWzuw+c1jRJnwROAKf535zqjwjm0SObXdLHCA6CxQh2tF4ws59K2kaEc68VTrl838wej3puSbsJ9sohmPr+nZk9U27ummvozjnn7q3Wplycc85twBu6c85FhDd055yLCG/ozjkXEd7QnXMuIryhO+dcRHhDd865iPgvhaClDVieKR8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(results3.history)[['accuracy', 'val_accuracy','loss', 'val_loss']].plot()\n",
    "plt.savefig('C:\\\\Users\\\\gold\\\\Desktop\\\\models\\\\experiment2\\\\thesisfigures\\\\Learning Curve3.png',dpi = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9d42286a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 750us/step - loss: 0.3492 - accuracy: 0.8362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3492428660392761, 0.8361669182777405]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X3_test, Y3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d5a5e304",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = pd.DataFrame(results3.history,columns = ['accuracy', 'val_accuracy','loss', 'val_loss'])\n",
    "# save.to_csv('C:\\\\Users\\\\gold\\\\Desktop\\\\models\\\\results.csv',index = False,header = False)\n",
    "save.to_csv('C:\\\\Users\\\\gold\\\\Desktop\\\\models\\\\experiment2\\\\thesisfigures\\\\results3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "07f53d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import csv  # 导入csv模块\n",
    "import matplotlib.pyplot as plt\n",
    "path = 'C:\\\\Users\\\\gold\\\\Desktop\\\\models\\\\results3.csv'\n",
    "data = pd.read_csv(path) #读取文件中所有数据\n",
    "# epoch = data[['epoch']]\n",
    "\n",
    "accuracy = data[['accuracy']]\n",
    "val_accuracy = data[['val_accuracy']]\n",
    "loss = data[['loss']]\n",
    "val_loss = data[['val_loss']]\n",
    "\n",
    "#画折线图\n",
    "plt.plot(accuracy,label='accuracy',color='black',linestyle='-')\n",
    "plt.plot(val_accuracy,label='val_accuracy',color='black',linestyle='--')\n",
    "plt.plot(loss,label='loss',color='black',linestyle='-.')\n",
    "plt.plot(val_loss,label='val_loss',color='black',linestyle=':')\n",
    "\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('performance')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend()\n",
    "plt.savefig('C:\\\\Users\\\\gold\\\\Desktop\\\\models\\\\experiment2\\\\thesisfigures\\\\Learning Curve Image3.png',dpi = 1000)\n",
    "plt.show()\n",
    "\n",
    "# plt.savefig('C:\\\\Users\\\\gold\\\\Desktop\\\\models\\\\Learning Curve Image',dpi = 600)\n",
    "# plt.savefig('plotLearning Curve Image.png',dpi = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cae80b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000281D6B34EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Accuracy: 0.8361669242658424\n",
      "f1_weighted: 0.835160314003273\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "# y_pred=model.predict_classes(X_test)\n",
    "Y_pred = np.argmax(model.predict(X3_test), axis=-1)\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# %%Accuracy\n",
    "\n",
    "# print(\"Accuracy:\",metrics.accuracy_score(np.argmax(y_test, axis = 0),y_pred))\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y3_test, Y_pred))\n",
    "# %%f1 score\n",
    "\n",
    "# print(\"f1_weighted:\",metrics.f1_score(np.argmax(y_test, axis=1), y_pred,average='weighted'))\n",
    "print(\"f1_weighted:\",metrics.f1_score(Y3_test, Y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e0485318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 27 candidates, totalling 270 fits\n",
      "Epoch 1/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 1.3629 - accuracy: 0.3687\n",
      "Epoch 2/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.8025 - accuracy: 0.5371\n",
      "Epoch 3/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6958 - accuracy: 0.5419\n",
      "Epoch 4/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6835 - accuracy: 0.5631\n",
      "Epoch 5/150\n",
      "52/52 [==============================] - 0s 978us/step - loss: 0.6789 - accuracy: 0.5740\n",
      "Epoch 6/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6371 - accuracy: 0.6425\n",
      "Epoch 7/150\n",
      "52/52 [==============================] - 0s 978us/step - loss: 0.6121 - accuracy: 0.6359\n",
      "Epoch 8/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5909 - accuracy: 0.6386\n",
      "Epoch 9/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5894 - accuracy: 0.6481\n",
      "Epoch 10/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5836 - accuracy: 0.6769\n",
      "Epoch 11/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5670 - accuracy: 0.6681\n",
      "Epoch 12/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5734 - accuracy: 0.7006\n",
      "Epoch 13/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5671 - accuracy: 0.7284\n",
      "Epoch 14/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5628 - accuracy: 0.7295\n",
      "Epoch 15/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5612 - accuracy: 0.7329\n",
      "Epoch 16/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5285 - accuracy: 0.7845\n",
      "Epoch 17/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5145 - accuracy: 0.8063\n",
      "Epoch 18/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4976 - accuracy: 0.8068\n",
      "Epoch 19/150\n",
      "52/52 [==============================] - 0s 997us/step - loss: 0.4761 - accuracy: 0.7995\n",
      "Epoch 20/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4638 - accuracy: 0.7939\n",
      "Epoch 21/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4321 - accuracy: 0.8221\n",
      "Epoch 22/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4350 - accuracy: 0.8142\n",
      "Epoch 23/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4373 - accuracy: 0.8029\n",
      "Epoch 24/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4250 - accuracy: 0.8315\n",
      "Epoch 25/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4219 - accuracy: 0.8102\n",
      "Epoch 26/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4213 - accuracy: 0.7957\n",
      "Epoch 27/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4284 - accuracy: 0.7936\n",
      "Epoch 28/150\n",
      "52/52 [==============================] - 0s 979us/step - loss: 0.4075 - accuracy: 0.8126\n",
      "Epoch 29/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4036 - accuracy: 0.8292\n",
      "Epoch 30/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4005 - accuracy: 0.8221\n",
      "Epoch 31/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3990 - accuracy: 0.8269\n",
      "Epoch 32/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3838 - accuracy: 0.8324\n",
      "Epoch 33/150\n",
      "52/52 [==============================] - 0s 997us/step - loss: 0.3910 - accuracy: 0.8184\n",
      "Epoch 34/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3853 - accuracy: 0.8323\n",
      "Epoch 35/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3893 - accuracy: 0.8148\n",
      "Epoch 36/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3980 - accuracy: 0.8171\n",
      "Epoch 37/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3902 - accuracy: 0.8106\n",
      "Epoch 38/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3924 - accuracy: 0.8193\n",
      "Epoch 39/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3789 - accuracy: 0.8235\n",
      "Epoch 40/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3939 - accuracy: 0.8141\n",
      "Epoch 41/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3768 - accuracy: 0.8252\n",
      "Epoch 42/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3741 - accuracy: 0.8291\n",
      "Epoch 43/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3883 - accuracy: 0.8098\n",
      "Epoch 44/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3793 - accuracy: 0.8248\n",
      "Epoch 45/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3601 - accuracy: 0.8317\n",
      "Epoch 46/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3788 - accuracy: 0.8163\n",
      "Epoch 47/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3596 - accuracy: 0.8264\n",
      "Epoch 48/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3504 - accuracy: 0.8354\n",
      "Epoch 49/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3633 - accuracy: 0.8280\n",
      "Epoch 50/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3551 - accuracy: 0.8364\n",
      "Epoch 51/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3407 - accuracy: 0.8445\n",
      "Epoch 52/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3493 - accuracy: 0.8347\n",
      "Epoch 53/150\n",
      "52/52 [==============================] - 0s 998us/step - loss: 0.3521 - accuracy: 0.8255\n",
      "Epoch 54/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3465 - accuracy: 0.8369\n",
      "Epoch 55/150\n",
      "52/52 [==============================] - 0s 980us/step - loss: 0.3422 - accuracy: 0.8365\n",
      "Epoch 56/150\n",
      "52/52 [==============================] - 0s 979us/step - loss: 0.3326 - accuracy: 0.8479\n",
      "Epoch 57/150\n",
      "52/52 [==============================] - 0s 958us/step - loss: 0.3226 - accuracy: 0.8566\n",
      "Epoch 58/150\n",
      "52/52 [==============================] - 0s 997us/step - loss: 0.3431 - accuracy: 0.8337\n",
      "Epoch 59/150\n",
      "52/52 [==============================] - 0s 978us/step - loss: 0.3437 - accuracy: 0.8346\n",
      "Epoch 60/150\n",
      "52/52 [==============================] - 0s 978us/step - loss: 0.3113 - accuracy: 0.8473\n",
      "Epoch 61/150\n",
      "52/52 [==============================] - 0s 978us/step - loss: 0.3329 - accuracy: 0.8451\n",
      "Epoch 62/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3242 - accuracy: 0.8500\n",
      "Epoch 63/150\n",
      "52/52 [==============================] - 0s 958us/step - loss: 0.3492 - accuracy: 0.8324\n",
      "Epoch 64/150\n",
      "52/52 [==============================] - 0s 978us/step - loss: 0.3274 - accuracy: 0.8507\n",
      "Epoch 65/150\n",
      "52/52 [==============================] - 0s 978us/step - loss: 0.3250 - accuracy: 0.8418\n",
      "Epoch 66/150\n",
      "52/52 [==============================] - 0s 939us/step - loss: 0.3248 - accuracy: 0.8377\n",
      "Epoch 67/150\n",
      "52/52 [==============================] - 0s 958us/step - loss: 0.3414 - accuracy: 0.8322\n",
      "Epoch 68/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2993 - accuracy: 0.8610\n",
      "Epoch 69/150\n",
      "52/52 [==============================] - 0s 958us/step - loss: 0.3219 - accuracy: 0.8421\n",
      "Epoch 70/150\n",
      "52/52 [==============================] - 0s 979us/step - loss: 0.3267 - accuracy: 0.8414\n",
      "Epoch 71/150\n",
      "52/52 [==============================] - 0s 997us/step - loss: 0.3246 - accuracy: 0.8460\n",
      "Epoch 72/150\n",
      "52/52 [==============================] - 0s 978us/step - loss: 0.3175 - accuracy: 0.8473\n",
      "Epoch 73/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3080 - accuracy: 0.8386\n",
      "Epoch 74/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3211 - accuracy: 0.8316\n",
      "Epoch 75/150\n",
      "52/52 [==============================] - 0s 958us/step - loss: 0.3134 - accuracy: 0.8419\n",
      "Epoch 76/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3007 - accuracy: 0.8564\n",
      "Epoch 77/150\n",
      "52/52 [==============================] - 0s 979us/step - loss: 0.2941 - accuracy: 0.8518\n",
      "Epoch 78/150\n",
      "52/52 [==============================] - 0s 997us/step - loss: 0.3167 - accuracy: 0.8506\n",
      "Epoch 79/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3136 - accuracy: 0.8432\n",
      "Epoch 80/150\n",
      "52/52 [==============================] - 0s 978us/step - loss: 0.3110 - accuracy: 0.8525\n",
      "Epoch 81/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 991us/step - loss: 0.3002 - accuracy: 0.8488\n",
      "Epoch 82/150\n",
      "52/52 [==============================] - 0s 997us/step - loss: 0.2964 - accuracy: 0.8549\n",
      "Epoch 83/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2982 - accuracy: 0.8616\n",
      "Epoch 84/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3221 - accuracy: 0.8338\n",
      "Epoch 85/150\n",
      "52/52 [==============================] - 0s 978us/step - loss: 0.3120 - accuracy: 0.8393\n",
      "Epoch 86/150\n",
      "52/52 [==============================] - 0s 978us/step - loss: 0.3149 - accuracy: 0.8497\n",
      "Epoch 87/150\n",
      "52/52 [==============================] - 0s 958us/step - loss: 0.3034 - accuracy: 0.8516\n",
      "Epoch 88/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3045 - accuracy: 0.8563\n",
      "Epoch 89/150\n",
      "52/52 [==============================] - 0s 978us/step - loss: 0.2952 - accuracy: 0.8561\n",
      "Epoch 90/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3171 - accuracy: 0.8423\n",
      "Epoch 91/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3114 - accuracy: 0.8478\n",
      "Epoch 92/150\n",
      "52/52 [==============================] - 0s 958us/step - loss: 0.3138 - accuracy: 0.8567\n",
      "Epoch 93/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2884 - accuracy: 0.8501\n",
      "Epoch 94/150\n",
      "52/52 [==============================] - 0s 959us/step - loss: 0.2944 - accuracy: 0.8577\n",
      "Epoch 95/150\n",
      "52/52 [==============================] - 0s 978us/step - loss: 0.3022 - accuracy: 0.8492\n",
      "Epoch 96/150\n",
      "52/52 [==============================] - 0s 996us/step - loss: 0.2995 - accuracy: 0.8512\n",
      "Epoch 97/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2821 - accuracy: 0.8728\n",
      "Epoch 98/150\n",
      "52/52 [==============================] - 0s 997us/step - loss: 0.2849 - accuracy: 0.8709\n",
      "Epoch 99/150\n",
      "52/52 [==============================] - 0s 958us/step - loss: 0.2858 - accuracy: 0.8699\n",
      "Epoch 100/150\n",
      "52/52 [==============================] - 0s 979us/step - loss: 0.2920 - accuracy: 0.8507\n",
      "Epoch 101/150\n",
      "52/52 [==============================] - 0s 958us/step - loss: 0.2893 - accuracy: 0.8602\n",
      "Epoch 102/150\n",
      "52/52 [==============================] - 0s 978us/step - loss: 0.2913 - accuracy: 0.8570\n",
      "Epoch 103/150\n",
      "52/52 [==============================] - 0s 978us/step - loss: 0.2852 - accuracy: 0.8653\n",
      "Epoch 104/150\n",
      "52/52 [==============================] - 0s 997us/step - loss: 0.2875 - accuracy: 0.8586\n",
      "Epoch 105/150\n",
      "52/52 [==============================] - 0s 978us/step - loss: 0.2789 - accuracy: 0.8660\n",
      "Epoch 106/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2992 - accuracy: 0.8663\n",
      "Epoch 107/150\n",
      "52/52 [==============================] - 0s 978us/step - loss: 0.2810 - accuracy: 0.8683\n",
      "Epoch 108/150\n",
      "52/52 [==============================] - 0s 950us/step - loss: 0.2827 - accuracy: 0.8641\n",
      "Epoch 109/150\n",
      "52/52 [==============================] - 0s 977us/step - loss: 0.3032 - accuracy: 0.8534\n",
      "Epoch 110/150\n",
      "52/52 [==============================] - 0s 992us/step - loss: 0.2965 - accuracy: 0.8551\n",
      "Epoch 111/150\n",
      "52/52 [==============================] - 0s 978us/step - loss: 0.2676 - accuracy: 0.8731\n",
      "Epoch 112/150\n",
      "52/52 [==============================] - 0s 978us/step - loss: 0.2774 - accuracy: 0.8715\n",
      "Epoch 113/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2915 - accuracy: 0.8614\n",
      "Epoch 114/150\n",
      "52/52 [==============================] - 0s 997us/step - loss: 0.2961 - accuracy: 0.8581\n",
      "Epoch 115/150\n",
      "52/52 [==============================] - 0s 978us/step - loss: 0.2605 - accuracy: 0.8820\n",
      "Epoch 116/150\n",
      "52/52 [==============================] - 0s 958us/step - loss: 0.2822 - accuracy: 0.8636\n",
      "Epoch 117/150\n",
      "52/52 [==============================] - 0s 958us/step - loss: 0.2759 - accuracy: 0.8692\n",
      "Epoch 118/150\n",
      "52/52 [==============================] - 0s 997us/step - loss: 0.2667 - accuracy: 0.8705\n",
      "Epoch 119/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2875 - accuracy: 0.8556\n",
      "Epoch 120/150\n",
      "52/52 [==============================] - 0s 978us/step - loss: 0.2818 - accuracy: 0.8629\n",
      "Epoch 121/150\n",
      "52/52 [==============================] - 0s 958us/step - loss: 0.2852 - accuracy: 0.8616\n",
      "Epoch 122/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2785 - accuracy: 0.8650\n",
      "Epoch 123/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2811 - accuracy: 0.8608\n",
      "Epoch 124/150\n",
      "52/52 [==============================] - 0s 958us/step - loss: 0.2899 - accuracy: 0.8571\n",
      "Epoch 125/150\n",
      "52/52 [==============================] - 0s 958us/step - loss: 0.2762 - accuracy: 0.8587\n",
      "Epoch 126/150\n",
      "52/52 [==============================] - 0s 979us/step - loss: 0.2842 - accuracy: 0.8637\n",
      "Epoch 127/150\n",
      "52/52 [==============================] - 0s 978us/step - loss: 0.2835 - accuracy: 0.8650\n",
      "Epoch 128/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3043 - accuracy: 0.8488\n",
      "Epoch 129/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2769 - accuracy: 0.8620\n",
      "Epoch 130/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2782 - accuracy: 0.8693\n",
      "Epoch 131/150\n",
      "52/52 [==============================] - 0s 978us/step - loss: 0.2823 - accuracy: 0.8677\n",
      "Epoch 132/150\n",
      "52/52 [==============================] - 0s 997us/step - loss: 0.2720 - accuracy: 0.8656\n",
      "Epoch 133/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2713 - accuracy: 0.8705\n",
      "Epoch 134/150\n",
      "52/52 [==============================] - 0s 997us/step - loss: 0.2681 - accuracy: 0.8748\n",
      "Epoch 135/150\n",
      "52/52 [==============================] - 0s 978us/step - loss: 0.2589 - accuracy: 0.8736\n",
      "Epoch 136/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2623 - accuracy: 0.8758\n",
      "Epoch 137/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2780 - accuracy: 0.8570\n",
      "Epoch 138/150\n",
      "52/52 [==============================] - 0s 919us/step - loss: 0.2765 - accuracy: 0.8631\n",
      "Epoch 139/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2810 - accuracy: 0.8713\n",
      "Epoch 140/150\n",
      "52/52 [==============================] - 0s 958us/step - loss: 0.2866 - accuracy: 0.8576\n",
      "Epoch 141/150\n",
      "52/52 [==============================] - 0s 958us/step - loss: 0.2879 - accuracy: 0.8611\n",
      "Epoch 142/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2759 - accuracy: 0.8579\n",
      "Epoch 143/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2757 - accuracy: 0.8729\n",
      "Epoch 144/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2725 - accuracy: 0.8676\n",
      "Epoch 145/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2614 - accuracy: 0.8747\n",
      "Epoch 146/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2669 - accuracy: 0.8742\n",
      "Epoch 147/150\n",
      "52/52 [==============================] - 0s 978us/step - loss: 0.2734 - accuracy: 0.8644\n",
      "Epoch 148/150\n",
      "52/52 [==============================] - 0s 978us/step - loss: 0.2802 - accuracy: 0.8642\n",
      "Epoch 149/150\n",
      "52/52 [==============================] - 0s 997us/step - loss: 0.2771 - accuracy: 0.8682\n",
      "Epoch 150/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2714 - accuracy: 0.8699\n",
      "0.799150 (0.031343) with: {'batch_size': 32, 'epochs': 50, 'optimizer': 'RMSprop'}\n",
      "0.816575 (0.011037) with: {'batch_size': 32, 'epochs': 50, 'optimizer': 'Adam'}\n",
      "0.320071 (0.025089) with: {'batch_size': 32, 'epochs': 50, 'optimizer': 'SGD'}\n",
      "0.824725 (0.026233) with: {'batch_size': 32, 'epochs': 100, 'optimizer': 'RMSprop'}\n",
      "0.834378 (0.019615) with: {'batch_size': 32, 'epochs': 100, 'optimizer': 'Adam'}\n",
      "0.553024 (0.028206) with: {'batch_size': 32, 'epochs': 100, 'optimizer': 'SGD'}\n",
      "0.827043 (0.025441) with: {'batch_size': 32, 'epochs': 150, 'optimizer': 'RMSprop'}\n",
      "0.835156 (0.023114) with: {'batch_size': 32, 'epochs': 150, 'optimizer': 'Adam'}\n",
      "0.654024 (0.040616) with: {'batch_size': 32, 'epochs': 150, 'optimizer': 'SGD'}\n",
      "0.808080 (0.023546) with: {'batch_size': 50, 'epochs': 50, 'optimizer': 'RMSprop'}\n",
      "0.813081 (0.019822) with: {'batch_size': 50, 'epochs': 50, 'optimizer': 'Adam'}\n",
      "0.320071 (0.025089) with: {'batch_size': 50, 'epochs': 50, 'optimizer': 'SGD'}\n",
      "0.813470 (0.024219) with: {'batch_size': 50, 'epochs': 100, 'optimizer': 'RMSprop'}\n",
      "0.834005 (0.024961) with: {'batch_size': 50, 'epochs': 100, 'optimizer': 'Adam'}\n",
      "0.344490 (0.075479) with: {'batch_size': 50, 'epochs': 100, 'optimizer': 'SGD'}\n",
      "0.831675 (0.027760) with: {'batch_size': 50, 'epochs': 150, 'optimizer': 'RMSprop'}\n",
      "0.844053 (0.022503) with: {'batch_size': 50, 'epochs': 150, 'optimizer': 'Adam'}\n",
      "0.547603 (0.023022) with: {'batch_size': 50, 'epochs': 150, 'optimizer': 'SGD'}\n",
      "0.777100 (0.036305) with: {'batch_size': 100, 'epochs': 50, 'optimizer': 'RMSprop'}\n",
      "0.806121 (0.021127) with: {'batch_size': 100, 'epochs': 50, 'optimizer': 'Adam'}\n",
      "0.320071 (0.025089) with: {'batch_size': 100, 'epochs': 50, 'optimizer': 'SGD'}\n",
      "0.781737 (0.035343) with: {'batch_size': 100, 'epochs': 100, 'optimizer': 'RMSprop'}\n",
      "0.823537 (0.012272) with: {'batch_size': 100, 'epochs': 100, 'optimizer': 'Adam'}\n",
      "0.320071 (0.025089) with: {'batch_size': 100, 'epochs': 100, 'optimizer': 'SGD'}\n",
      "0.811960 (0.028520) with: {'batch_size': 100, 'epochs': 150, 'optimizer': 'RMSprop'}\n",
      "0.839411 (0.019728) with: {'batch_size': 100, 'epochs': 150, 'optimizer': 'Adam'}\n",
      "0.320071 (0.025089) with: {'batch_size': 100, 'epochs': 150, 'optimizer': 'SGD'}\n",
      "Best: 0.844053 using {'batch_size': 50, 'epochs': 150, 'optimizer': 'Adam'}\n"
     ]
    }
   ],
   "source": [
    "# Grid Search Cross Validation\n",
    "# GridSearch Cross Validation Parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def build_classifier(optimizer):\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units = 50, kernel_initializer = 'uniform', activation = 'relu', input_dim = 6))\n",
    "    classifier.add(Dense(units = 50, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(units = 50, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'softmax'))\n",
    "    classifier.compile(optimizer = optimizer, loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier\n",
    "\n",
    "param_grid = {\n",
    "   \n",
    "    'epochs': [50,100,150], \n",
    "    'batch_size':[32,50,100],\n",
    "    'optimizer':['RMSprop', 'Adam','SGD'],\n",
    "    \n",
    "}\n",
    "\n",
    "# create model\n",
    "\n",
    "# Creating Model Object with KerasClassifier\n",
    "# create_model = model\n",
    "model_cv = KerasClassifier(build_fn = build_classifier, verbose=1)\n",
    "\n",
    "\n",
    "grid = GridSearchCV(estimator = model_cv,  \n",
    "                    n_jobs = -1, \n",
    "                    verbose = 1,\n",
    "                    cv = 10,\n",
    "                    param_grid = param_grid)\n",
    "\n",
    "grid_cv_model = grid.fit(X3_train, Y3_train) # Fitting the GridSearch Object on the Train Set\n",
    "\n",
    "\n",
    "means = grid_cv_model.cv_results_['mean_test_score'] # Mean of test scores\n",
    "stds = grid_cv_model.cv_results_['std_test_score'] # standard deviations of test scores\n",
    "params = grid_cv_model.cv_results_['params'] # parameters used\n",
    "# to print all scores, standard deviations and parameters used\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "# Printing the Best Parameters as a Result of Grid Search Cross Validation on the Screen\n",
    "print(\"Best: %f using %s\" % (grid_cv_model.best_score_, grid_cv_model.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "14d9d595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.3856 - accuracy: 0.2920\n",
      "Epoch 2/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.3783 - accuracy: 0.3436\n",
      "Epoch 3/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.3453 - accuracy: 0.4332\n",
      "Epoch 4/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.2319 - accuracy: 0.5558\n",
      "Epoch 5/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9797 - accuracy: 0.6100\n",
      "Epoch 6/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.7718 - accuracy: 0.5933\n",
      "Epoch 7/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.7172 - accuracy: 0.5873\n",
      "Epoch 8/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6927 - accuracy: 0.5855\n",
      "Epoch 9/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.7000 - accuracy: 0.5579\n",
      "Epoch 10/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6808 - accuracy: 0.5793\n",
      "Epoch 11/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6791 - accuracy: 0.5764\n",
      "Epoch 12/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6833 - accuracy: 0.5839\n",
      "Epoch 13/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6819 - accuracy: 0.5715\n",
      "Epoch 14/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6787 - accuracy: 0.5954\n",
      "Epoch 15/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6670 - accuracy: 0.6290\n",
      "Epoch 16/150\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6183 - accuracy: 0.78 - 0s 1ms/step - loss: 0.6500 - accuracy: 0.6697\n",
      "Epoch 17/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6530 - accuracy: 0.6564\n",
      "Epoch 18/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6534 - accuracy: 0.6341\n",
      "Epoch 19/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6555 - accuracy: 0.6380\n",
      "Epoch 20/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6369 - accuracy: 0.6538\n",
      "Epoch 21/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6233 - accuracy: 0.6620\n",
      "Epoch 22/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6298 - accuracy: 0.6353\n",
      "Epoch 23/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6121 - accuracy: 0.6747\n",
      "Epoch 24/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6179 - accuracy: 0.6630\n",
      "Epoch 25/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6216 - accuracy: 0.6596\n",
      "Epoch 26/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6113 - accuracy: 0.6575\n",
      "Epoch 27/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6140 - accuracy: 0.6297\n",
      "Epoch 28/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5964 - accuracy: 0.6620\n",
      "Epoch 29/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5834 - accuracy: 0.6829\n",
      "Epoch 30/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5910 - accuracy: 0.6704\n",
      "Epoch 31/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6100 - accuracy: 0.6219\n",
      "Epoch 32/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5802 - accuracy: 0.6652\n",
      "Epoch 33/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5811 - accuracy: 0.6670\n",
      "Epoch 34/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5802 - accuracy: 0.6575\n",
      "Epoch 35/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5931 - accuracy: 0.6396\n",
      "Epoch 36/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5649 - accuracy: 0.6702\n",
      "Epoch 37/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5788 - accuracy: 0.6585\n",
      "Epoch 38/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5762 - accuracy: 0.6525\n",
      "Epoch 39/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5551 - accuracy: 0.6789\n",
      "Epoch 40/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5525 - accuracy: 0.6879\n",
      "Epoch 41/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5556 - accuracy: 0.6729\n",
      "Epoch 42/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5673 - accuracy: 0.6830\n",
      "Epoch 43/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5699 - accuracy: 0.6748\n",
      "Epoch 44/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5520 - accuracy: 0.6885\n",
      "Epoch 45/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5315 - accuracy: 0.7024\n",
      "Epoch 46/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5388 - accuracy: 0.7172\n",
      "Epoch 47/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5561 - accuracy: 0.6680\n",
      "Epoch 48/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5451 - accuracy: 0.7418\n",
      "Epoch 49/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5708 - accuracy: 0.6775\n",
      "Epoch 50/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5394 - accuracy: 0.7208\n",
      "Epoch 51/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5480 - accuracy: 0.7125\n",
      "Epoch 52/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5381 - accuracy: 0.6995\n",
      "Epoch 53/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5192 - accuracy: 0.7582\n",
      "Epoch 54/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5238 - accuracy: 0.7051\n",
      "Epoch 55/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5368 - accuracy: 0.7493\n",
      "Epoch 56/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5377 - accuracy: 0.7215\n",
      "Epoch 57/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5248 - accuracy: 0.7658\n",
      "Epoch 58/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5336 - accuracy: 0.7629\n",
      "Epoch 59/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5023 - accuracy: 0.7649\n",
      "Epoch 60/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5049 - accuracy: 0.7663\n",
      "Epoch 61/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5179 - accuracy: 0.7803\n",
      "Epoch 62/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5097 - accuracy: 0.7767\n",
      "Epoch 63/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5172 - accuracy: 0.7719\n",
      "Epoch 64/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4764 - accuracy: 0.8016\n",
      "Epoch 65/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4852 - accuracy: 0.8208\n",
      "Epoch 66/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4893 - accuracy: 0.8171\n",
      "Epoch 67/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4827 - accuracy: 0.8055\n",
      "Epoch 68/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4863 - accuracy: 0.8238\n",
      "Epoch 69/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4673 - accuracy: 0.8170\n",
      "Epoch 70/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4602 - accuracy: 0.8318\n",
      "Epoch 71/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4827 - accuracy: 0.7916\n",
      "Epoch 72/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4949 - accuracy: 0.7727\n",
      "Epoch 73/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4192 - accuracy: 0.8452\n",
      "Epoch 74/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4795 - accuracy: 0.7686\n",
      "Epoch 75/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.4791 - accuracy: 0.7863\n",
      "Epoch 76/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4222 - accuracy: 0.8431\n",
      "Epoch 77/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.4454 - accuracy: 0.8078\n",
      "Epoch 78/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.4606 - accuracy: 0.7875\n",
      "Epoch 79/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4722 - accuracy: 0.8011\n",
      "Epoch 80/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3961 - accuracy: 0.8591\n",
      "Epoch 81/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4283 - accuracy: 0.8094\n",
      "Epoch 82/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 997us/step - loss: 0.4441 - accuracy: 0.8152\n",
      "Epoch 83/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.4184 - accuracy: 0.8163\n",
      "Epoch 84/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4146 - accuracy: 0.8247\n",
      "Epoch 85/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.4069 - accuracy: 0.8088\n",
      "Epoch 86/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4151 - accuracy: 0.8244\n",
      "Epoch 87/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3743 - accuracy: 0.8528\n",
      "Epoch 88/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3992 - accuracy: 0.8079\n",
      "Epoch 89/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4103 - accuracy: 0.8308\n",
      "Epoch 90/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4315 - accuracy: 0.7892\n",
      "Epoch 91/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4205 - accuracy: 0.8186\n",
      "Epoch 92/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3977 - accuracy: 0.8183\n",
      "Epoch 93/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4047 - accuracy: 0.7956\n",
      "Epoch 94/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3829 - accuracy: 0.8415\n",
      "Epoch 95/150\n",
      "12/12 [==============================] - 0s 996us/step - loss: 0.3964 - accuracy: 0.8204\n",
      "Epoch 96/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3826 - accuracy: 0.8407\n",
      "Epoch 97/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3965 - accuracy: 0.8226\n",
      "Epoch 98/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3759 - accuracy: 0.8332\n",
      "Epoch 99/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4322 - accuracy: 0.7944\n",
      "Epoch 100/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4054 - accuracy: 0.8192\n",
      "Epoch 101/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3694 - accuracy: 0.8447\n",
      "Epoch 102/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3835 - accuracy: 0.8300\n",
      "Epoch 103/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3593 - accuracy: 0.8434\n",
      "Epoch 104/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8120\n",
      "Epoch 105/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4021 - accuracy: 0.8378\n",
      "Epoch 106/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3916 - accuracy: 0.8105\n",
      "Epoch 107/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3766 - accuracy: 0.8417\n",
      "Epoch 108/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3947 - accuracy: 0.8132\n",
      "Epoch 109/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3782 - accuracy: 0.8233\n",
      "Epoch 110/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3698 - accuracy: 0.8317\n",
      "Epoch 111/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3696 - accuracy: 0.8243\n",
      "Epoch 112/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3807 - accuracy: 0.8234\n",
      "Epoch 113/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.4029 - accuracy: 0.8088\n",
      "Epoch 114/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3710 - accuracy: 0.8358\n",
      "Epoch 115/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3596 - accuracy: 0.8459\n",
      "Epoch 116/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3667 - accuracy: 0.8167\n",
      "Epoch 117/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3953 - accuracy: 0.8270\n",
      "Epoch 118/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3580 - accuracy: 0.8391\n",
      "Epoch 119/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3722 - accuracy: 0.8357\n",
      "Epoch 120/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3720 - accuracy: 0.8118\n",
      "Epoch 121/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4078 - accuracy: 0.7908\n",
      "Epoch 122/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4187 - accuracy: 0.8231\n",
      "Epoch 123/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3997 - accuracy: 0.8240\n",
      "Epoch 124/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3743 - accuracy: 0.8140\n",
      "Epoch 125/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3663 - accuracy: 0.8348\n",
      "Epoch 126/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3680 - accuracy: 0.8159\n",
      "Epoch 127/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3967 - accuracy: 0.8298\n",
      "Epoch 128/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3552 - accuracy: 0.8323\n",
      "Epoch 129/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3749 - accuracy: 0.8089\n",
      "Epoch 130/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3733 - accuracy: 0.8312\n",
      "Epoch 131/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3751 - accuracy: 0.8227\n",
      "Epoch 132/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3767 - accuracy: 0.8211\n",
      "Epoch 133/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3635 - accuracy: 0.8414\n",
      "Epoch 134/150\n",
      "12/12 [==============================] - 0s 996us/step - loss: 0.3629 - accuracy: 0.8278\n",
      "Epoch 135/150\n",
      "12/12 [==============================] - 0s 967us/step - loss: 0.3729 - accuracy: 0.8334\n",
      "Epoch 136/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3428 - accuracy: 0.8651\n",
      "Epoch 137/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3822 - accuracy: 0.7996\n",
      "Epoch 138/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3394 - accuracy: 0.8669\n",
      "Epoch 139/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3921 - accuracy: 0.8010\n",
      "Epoch 140/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3828 - accuracy: 0.8271\n",
      "Epoch 141/150\n",
      "12/12 [==============================] - 0s 994us/step - loss: 0.3667 - accuracy: 0.8214\n",
      "Epoch 142/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3596 - accuracy: 0.8320\n",
      "Epoch 143/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3658 - accuracy: 0.8441\n",
      "Epoch 144/150\n",
      "12/12 [==============================] - 0s 999us/step - loss: 0.3788 - accuracy: 0.8077\n",
      "Epoch 145/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3489 - accuracy: 0.8632\n",
      "Epoch 146/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3413 - accuracy: 0.8450\n",
      "Epoch 147/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3746 - accuracy: 0.8038\n",
      "Epoch 148/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3687 - accuracy: 0.8226\n",
      "Epoch 149/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3583 - accuracy: 0.8172\n",
      "Epoch 150/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3739 - accuracy: 0.8123\n",
      "2/2 [==============================] - 0s 997us/step\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\position\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 1ms/step - loss: 1.3858 - accuracy: 0.2769\n",
      "Epoch 2/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.3788 - accuracy: 0.4380\n",
      "Epoch 3/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.3513 - accuracy: 0.5612\n",
      "Epoch 4/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.2508 - accuracy: 0.5584\n",
      "Epoch 5/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0197 - accuracy: 0.6012\n",
      "Epoch 6/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8090 - accuracy: 0.5526\n",
      "Epoch 7/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.7083 - accuracy: 0.5965\n",
      "Epoch 8/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6915 - accuracy: 0.5866\n",
      "Epoch 9/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6857 - accuracy: 0.5930\n",
      "Epoch 10/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6900 - accuracy: 0.5601\n",
      "Epoch 11/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6920 - accuracy: 0.5477\n",
      "Epoch 12/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6848 - accuracy: 0.5672\n",
      "Epoch 13/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6695 - accuracy: 0.5947\n",
      "Epoch 14/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6769 - accuracy: 0.5743\n",
      "Epoch 15/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6674 - accuracy: 0.5977\n",
      "Epoch 16/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6790 - accuracy: 0.5665\n",
      "Epoch 17/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6664 - accuracy: 0.6002\n",
      "Epoch 18/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6734 - accuracy: 0.5816\n",
      "Epoch 19/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6645 - accuracy: 0.6144\n",
      "Epoch 20/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6469 - accuracy: 0.6576\n",
      "Epoch 21/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6467 - accuracy: 0.6340\n",
      "Epoch 22/150\n",
      "12/12 [==============================] - 0s 998us/step - loss: 0.6407 - accuracy: 0.6540\n",
      "Epoch 23/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6499 - accuracy: 0.6204\n",
      "Epoch 24/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6350 - accuracy: 0.6458\n",
      "Epoch 25/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6170 - accuracy: 0.6703\n",
      "Epoch 26/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6165 - accuracy: 0.6506\n",
      "Epoch 27/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.6243 - accuracy: 0.6520\n",
      "Epoch 28/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6151 - accuracy: 0.6374\n",
      "Epoch 29/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5999 - accuracy: 0.6583\n",
      "Epoch 30/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6039 - accuracy: 0.6488\n",
      "Epoch 31/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5837 - accuracy: 0.6585\n",
      "Epoch 32/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5690 - accuracy: 0.6857\n",
      "Epoch 33/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5705 - accuracy: 0.6948\n",
      "Epoch 34/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5679 - accuracy: 0.6642\n",
      "Epoch 35/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5773 - accuracy: 0.6688\n",
      "Epoch 36/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5568 - accuracy: 0.6679\n",
      "Epoch 37/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.5626 - accuracy: 0.6681\n",
      "Epoch 38/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.5423 - accuracy: 0.6876\n",
      "Epoch 39/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5649 - accuracy: 0.6791\n",
      "Epoch 40/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5480 - accuracy: 0.6886\n",
      "Epoch 41/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5165 - accuracy: 0.7244\n",
      "Epoch 42/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5422 - accuracy: 0.7230\n",
      "Epoch 43/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5514 - accuracy: 0.7025\n",
      "Epoch 44/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5145 - accuracy: 0.7214\n",
      "Epoch 45/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5258 - accuracy: 0.7142\n",
      "Epoch 46/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5255 - accuracy: 0.7295\n",
      "Epoch 47/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5445 - accuracy: 0.7175\n",
      "Epoch 48/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5421 - accuracy: 0.7115\n",
      "Epoch 49/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5451 - accuracy: 0.7389\n",
      "Epoch 50/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5433 - accuracy: 0.7032\n",
      "Epoch 51/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5067 - accuracy: 0.7328\n",
      "Epoch 52/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5310 - accuracy: 0.7373\n",
      "Epoch 53/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5245 - accuracy: 0.7252\n",
      "Epoch 54/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5160 - accuracy: 0.7712\n",
      "Epoch 55/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5398 - accuracy: 0.7230\n",
      "Epoch 56/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5216 - accuracy: 0.7233\n",
      "Epoch 57/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5261 - accuracy: 0.7514\n",
      "Epoch 58/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5107 - accuracy: 0.7300\n",
      "Epoch 59/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5130 - accuracy: 0.7379\n",
      "Epoch 60/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5115 - accuracy: 0.7414\n",
      "Epoch 61/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4966 - accuracy: 0.7685\n",
      "Epoch 62/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5258 - accuracy: 0.7429\n",
      "Epoch 63/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5008 - accuracy: 0.7577\n",
      "Epoch 64/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5013 - accuracy: 0.7569\n",
      "Epoch 65/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4665 - accuracy: 0.7767\n",
      "Epoch 66/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4905 - accuracy: 0.7836\n",
      "Epoch 67/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4808 - accuracy: 0.7796\n",
      "Epoch 68/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4663 - accuracy: 0.7828\n",
      "Epoch 69/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4751 - accuracy: 0.7952\n",
      "Epoch 70/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4566 - accuracy: 0.8144\n",
      "Epoch 71/150\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4960 - accuracy: 0.76 - 0s 1ms/step - loss: 0.4642 - accuracy: 0.7897\n",
      "Epoch 72/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4350 - accuracy: 0.8176\n",
      "Epoch 73/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4434 - accuracy: 0.8302\n",
      "Epoch 74/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4789 - accuracy: 0.7551\n",
      "Epoch 75/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4553 - accuracy: 0.8014\n",
      "Epoch 76/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4558 - accuracy: 0.7969\n",
      "Epoch 77/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4501 - accuracy: 0.8186\n",
      "Epoch 78/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4798 - accuracy: 0.7762\n",
      "Epoch 79/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.4646 - accuracy: 0.7990\n",
      "Epoch 80/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4178 - accuracy: 0.8211\n",
      "Epoch 81/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4353 - accuracy: 0.7918\n",
      "Epoch 82/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4195 - accuracy: 0.8312\n",
      "Epoch 83/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4239 - accuracy: 0.8021\n",
      "Epoch 84/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4393 - accuracy: 0.8250\n",
      "Epoch 85/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4437 - accuracy: 0.7791\n",
      "Epoch 86/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4377 - accuracy: 0.8194\n",
      "Epoch 87/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4112 - accuracy: 0.8190\n",
      "Epoch 88/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4066 - accuracy: 0.8311\n",
      "Epoch 89/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4178 - accuracy: 0.7947\n",
      "Epoch 90/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4060 - accuracy: 0.8134\n",
      "Epoch 91/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.4164 - accuracy: 0.7917\n",
      "Epoch 92/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4220 - accuracy: 0.7987\n",
      "Epoch 93/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3900 - accuracy: 0.8260\n",
      "Epoch 94/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.4323 - accuracy: 0.7823\n",
      "Epoch 95/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3950 - accuracy: 0.8446\n",
      "Epoch 96/150\n",
      "12/12 [==============================] - 0s 999us/step - loss: 0.4034 - accuracy: 0.7992\n",
      "Epoch 97/150\n",
      "12/12 [==============================] - 0s 998us/step - loss: 0.3972 - accuracy: 0.8319\n",
      "Epoch 98/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3799 - accuracy: 0.8355\n",
      "Epoch 99/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3882 - accuracy: 0.8281\n",
      "Epoch 100/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4201 - accuracy: 0.8034\n",
      "Epoch 101/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4196 - accuracy: 0.7930\n",
      "Epoch 102/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4078 - accuracy: 0.8171\n",
      "Epoch 103/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3839 - accuracy: 0.8191\n",
      "Epoch 104/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3998 - accuracy: 0.8154\n",
      "Epoch 105/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3889 - accuracy: 0.8254\n",
      "Epoch 106/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3894 - accuracy: 0.8262\n",
      "Epoch 107/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3830 - accuracy: 0.8297\n",
      "Epoch 108/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3931 - accuracy: 0.8072\n",
      "Epoch 109/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3778 - accuracy: 0.8704\n",
      "Epoch 110/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3758 - accuracy: 0.8465\n",
      "Epoch 111/150\n",
      "12/12 [==============================] - 0s 907us/step - loss: 0.4082 - accuracy: 0.8160\n",
      "Epoch 112/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3856 - accuracy: 0.8123\n",
      "Epoch 113/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3775 - accuracy: 0.8519\n",
      "Epoch 114/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.4010 - accuracy: 0.7996\n",
      "Epoch 115/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3989 - accuracy: 0.8178\n",
      "Epoch 116/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3748 - accuracy: 0.8448\n",
      "Epoch 117/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3559 - accuracy: 0.8388\n",
      "Epoch 118/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3932 - accuracy: 0.8223\n",
      "Epoch 119/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3331 - accuracy: 0.8525\n",
      "Epoch 120/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3638 - accuracy: 0.8377\n",
      "Epoch 121/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3321 - accuracy: 0.8644\n",
      "Epoch 122/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3731 - accuracy: 0.8291\n",
      "Epoch 123/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3437 - accuracy: 0.8654\n",
      "Epoch 124/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3686 - accuracy: 0.8113\n",
      "Epoch 125/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3752 - accuracy: 0.8353\n",
      "Epoch 126/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3517 - accuracy: 0.8402\n",
      "Epoch 127/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3596 - accuracy: 0.8363\n",
      "Epoch 128/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3646 - accuracy: 0.8414\n",
      "Epoch 129/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3499 - accuracy: 0.8456\n",
      "Epoch 130/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3541 - accuracy: 0.8470\n",
      "Epoch 131/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3435 - accuracy: 0.8574\n",
      "Epoch 132/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3465 - accuracy: 0.8563\n",
      "Epoch 133/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3490 - accuracy: 0.8512\n",
      "Epoch 134/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3279 - accuracy: 0.8662\n",
      "Epoch 135/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3733 - accuracy: 0.8384\n",
      "Epoch 136/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3585 - accuracy: 0.8526\n",
      "Epoch 137/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3425 - accuracy: 0.8382\n",
      "Epoch 138/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3534 - accuracy: 0.8532\n",
      "Epoch 139/150\n",
      "12/12 [==============================] - 0s 998us/step - loss: 0.3554 - accuracy: 0.8436\n",
      "Epoch 140/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3500 - accuracy: 0.8488\n",
      "Epoch 141/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3469 - accuracy: 0.8375\n",
      "Epoch 142/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3353 - accuracy: 0.8471\n",
      "Epoch 143/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3439 - accuracy: 0.8517\n",
      "Epoch 144/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3609 - accuracy: 0.8281\n",
      "Epoch 145/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3360 - accuracy: 0.8634\n",
      "Epoch 146/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3535 - accuracy: 0.8447\n",
      "Epoch 147/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3414 - accuracy: 0.8457\n",
      "Epoch 148/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3366 - accuracy: 0.8518\n",
      "Epoch 149/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3475 - accuracy: 0.8423\n",
      "Epoch 150/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3299 - accuracy: 0.8669\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\position\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 1ms/step - loss: 1.3855 - accuracy: 0.3119\n",
      "Epoch 2/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.3772 - accuracy: 0.3594\n",
      "Epoch 3/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.3409 - accuracy: 0.5736\n",
      "Epoch 4/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.2265 - accuracy: 0.5832\n",
      "Epoch 5/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9770 - accuracy: 0.5840\n",
      "Epoch 6/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.7750 - accuracy: 0.5492\n",
      "Epoch 7/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.7065 - accuracy: 0.5871\n",
      "Epoch 8/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6816 - accuracy: 0.5861\n",
      "Epoch 9/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6801 - accuracy: 0.5860\n",
      "Epoch 10/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6822 - accuracy: 0.5821\n",
      "Epoch 11/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6734 - accuracy: 0.5772\n",
      "Epoch 12/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6621 - accuracy: 0.5976\n",
      "Epoch 13/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6671 - accuracy: 0.6240\n",
      "Epoch 14/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6444 - accuracy: 0.6646\n",
      "Epoch 15/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6635 - accuracy: 0.6470\n",
      "Epoch 16/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6445 - accuracy: 0.6660\n",
      "Epoch 17/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6502 - accuracy: 0.6363\n",
      "Epoch 18/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6412 - accuracy: 0.6691\n",
      "Epoch 19/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6411 - accuracy: 0.6532\n",
      "Epoch 20/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5998 - accuracy: 0.6866\n",
      "Epoch 21/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6114 - accuracy: 0.6744\n",
      "Epoch 22/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6056 - accuracy: 0.6680\n",
      "Epoch 23/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6007 - accuracy: 0.6798\n",
      "Epoch 24/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5957 - accuracy: 0.6959\n",
      "Epoch 25/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6260 - accuracy: 0.6392\n",
      "Epoch 26/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5890 - accuracy: 0.6811\n",
      "Epoch 27/150\n",
      "12/12 [==============================] - 0s 1000us/step - loss: 0.6089 - accuracy: 0.6560\n",
      "Epoch 28/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6116 - accuracy: 0.6914\n",
      "Epoch 29/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5981 - accuracy: 0.6680\n",
      "Epoch 30/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5667 - accuracy: 0.7052\n",
      "Epoch 31/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5802 - accuracy: 0.6847\n",
      "Epoch 32/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5841 - accuracy: 0.6666\n",
      "Epoch 33/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5468 - accuracy: 0.7037\n",
      "Epoch 34/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5630 - accuracy: 0.6888\n",
      "Epoch 35/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5589 - accuracy: 0.6749\n",
      "Epoch 36/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5589 - accuracy: 0.6771\n",
      "Epoch 37/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5695 - accuracy: 0.6698\n",
      "Epoch 38/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5463 - accuracy: 0.6697\n",
      "Epoch 39/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5525 - accuracy: 0.7020\n",
      "Epoch 40/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5519 - accuracy: 0.6694\n",
      "Epoch 41/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5369 - accuracy: 0.6940\n",
      "Epoch 42/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5343 - accuracy: 0.6993\n",
      "Epoch 43/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5257 - accuracy: 0.7044\n",
      "Epoch 44/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5466 - accuracy: 0.6645\n",
      "Epoch 45/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5537 - accuracy: 0.6884\n",
      "Epoch 46/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5320 - accuracy: 0.6837\n",
      "Epoch 47/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5313 - accuracy: 0.6720\n",
      "Epoch 48/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5101 - accuracy: 0.7095\n",
      "Epoch 49/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5457 - accuracy: 0.6879\n",
      "Epoch 50/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5325 - accuracy: 0.6930\n",
      "Epoch 51/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5135 - accuracy: 0.7020\n",
      "Epoch 52/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5151 - accuracy: 0.7084\n",
      "Epoch 53/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5306 - accuracy: 0.7068\n",
      "Epoch 54/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4997 - accuracy: 0.7257\n",
      "Epoch 55/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5150 - accuracy: 0.7334\n",
      "Epoch 56/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5239 - accuracy: 0.6933\n",
      "Epoch 57/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5092 - accuracy: 0.7014\n",
      "Epoch 58/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5076 - accuracy: 0.7392\n",
      "Epoch 59/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5089 - accuracy: 0.7222\n",
      "Epoch 60/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5056 - accuracy: 0.7347\n",
      "Epoch 61/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5052 - accuracy: 0.7479\n",
      "Epoch 62/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5021 - accuracy: 0.7887\n",
      "Epoch 63/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4940 - accuracy: 0.8000\n",
      "Epoch 64/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5169 - accuracy: 0.7080\n",
      "Epoch 65/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4839 - accuracy: 0.8425\n",
      "Epoch 66/150\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5206 - accuracy: 0.72 - 0s 1ms/step - loss: 0.4761 - accuracy: 0.7611\n",
      "Epoch 67/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4821 - accuracy: 0.8155\n",
      "Epoch 68/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4492 - accuracy: 0.7800\n",
      "Epoch 69/150\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5255 - accuracy: 0.76 - 0s 1ms/step - loss: 0.4826 - accuracy: 0.7944\n",
      "Epoch 70/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4660 - accuracy: 0.7637\n",
      "Epoch 71/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4657 - accuracy: 0.8051\n",
      "Epoch 72/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4909 - accuracy: 0.7901\n",
      "Epoch 73/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4442 - accuracy: 0.8381\n",
      "Epoch 74/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4555 - accuracy: 0.8075\n",
      "Epoch 75/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4382 - accuracy: 0.8444\n",
      "Epoch 76/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4588 - accuracy: 0.8127\n",
      "Epoch 77/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4626 - accuracy: 0.8241\n",
      "Epoch 78/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4369 - accuracy: 0.8395\n",
      "Epoch 79/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.4325 - accuracy: 0.8234\n",
      "Epoch 80/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4323 - accuracy: 0.8322\n",
      "Epoch 81/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4114 - accuracy: 0.8261\n",
      "Epoch 82/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4012 - accuracy: 0.8293\n",
      "Epoch 83/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3964 - accuracy: 0.8461\n",
      "Epoch 84/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3982 - accuracy: 0.8306\n",
      "Epoch 85/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3810 - accuracy: 0.8576\n",
      "Epoch 86/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3890 - accuracy: 0.8345\n",
      "Epoch 87/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3785 - accuracy: 0.8610\n",
      "Epoch 88/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3836 - accuracy: 0.8442\n",
      "Epoch 89/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3841 - accuracy: 0.8367\n",
      "Epoch 90/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3923 - accuracy: 0.8263\n",
      "Epoch 91/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3880 - accuracy: 0.8278\n",
      "Epoch 92/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8143\n",
      "Epoch 93/150\n",
      "12/12 [==============================] - 0s 1000us/step - loss: 0.4180 - accuracy: 0.7916\n",
      "Epoch 94/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3548 - accuracy: 0.8779\n",
      "Epoch 95/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3972 - accuracy: 0.8329\n",
      "Epoch 96/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3986 - accuracy: 0.8304\n",
      "Epoch 97/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3460 - accuracy: 0.8732\n",
      "Epoch 98/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3801 - accuracy: 0.8127\n",
      "Epoch 99/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3805 - accuracy: 0.8301\n",
      "Epoch 100/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3601 - accuracy: 0.8311\n",
      "Epoch 101/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3692 - accuracy: 0.8518\n",
      "Epoch 102/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3692 - accuracy: 0.8359\n",
      "Epoch 103/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3498 - accuracy: 0.8525\n",
      "Epoch 104/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3674 - accuracy: 0.8287\n",
      "Epoch 105/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3774 - accuracy: 0.8377\n",
      "Epoch 106/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3546 - accuracy: 0.8460\n",
      "Epoch 107/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3423 - accuracy: 0.8518\n",
      "Epoch 108/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3495 - accuracy: 0.8430\n",
      "Epoch 109/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3650 - accuracy: 0.8154\n",
      "Epoch 110/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3651 - accuracy: 0.8450\n",
      "Epoch 111/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3278 - accuracy: 0.8681\n",
      "Epoch 112/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3792 - accuracy: 0.8150\n",
      "Epoch 113/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3811 - accuracy: 0.8177\n",
      "Epoch 114/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3618 - accuracy: 0.8357\n",
      "Epoch 115/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3274 - accuracy: 0.8565\n",
      "Epoch 116/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3685 - accuracy: 0.8271\n",
      "Epoch 117/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3479 - accuracy: 0.8615\n",
      "Epoch 118/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3474 - accuracy: 0.8400\n",
      "Epoch 119/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3658 - accuracy: 0.8246\n",
      "Epoch 120/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3625 - accuracy: 0.8347\n",
      "Epoch 121/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3364 - accuracy: 0.8411\n",
      "Epoch 122/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3361 - accuracy: 0.8529\n",
      "Epoch 123/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3573 - accuracy: 0.8378\n",
      "Epoch 124/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3550 - accuracy: 0.8362\n",
      "Epoch 125/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3112 - accuracy: 0.8641\n",
      "Epoch 126/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3325 - accuracy: 0.8601\n",
      "Epoch 127/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3192 - accuracy: 0.8613\n",
      "Epoch 128/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3603 - accuracy: 0.8433\n",
      "Epoch 129/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3481 - accuracy: 0.8393\n",
      "Epoch 130/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3318 - accuracy: 0.8684\n",
      "Epoch 131/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3363 - accuracy: 0.8228\n",
      "Epoch 132/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3494 - accuracy: 0.8515\n",
      "Epoch 133/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3242 - accuracy: 0.8504\n",
      "Epoch 134/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3615 - accuracy: 0.8286\n",
      "Epoch 135/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3241 - accuracy: 0.8542\n",
      "Epoch 136/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3341 - accuracy: 0.8504\n",
      "Epoch 137/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3573 - accuracy: 0.8155\n",
      "Epoch 138/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3480 - accuracy: 0.8333\n",
      "Epoch 139/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3369 - accuracy: 0.8315\n",
      "Epoch 140/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3294 - accuracy: 0.8487\n",
      "Epoch 141/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3520 - accuracy: 0.8383\n",
      "Epoch 142/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3128 - accuracy: 0.8600\n",
      "Epoch 143/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3236 - accuracy: 0.8508\n",
      "Epoch 144/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3057 - accuracy: 0.8597\n",
      "Epoch 145/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3068 - accuracy: 0.8630\n",
      "Epoch 146/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3122 - accuracy: 0.8692\n",
      "Epoch 147/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3082 - accuracy: 0.8703\n",
      "Epoch 148/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3319 - accuracy: 0.8188\n",
      "Epoch 149/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3307 - accuracy: 0.8591\n",
      "Epoch 150/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3150 - accuracy: 0.8632\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\position\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 1ms/step - loss: 1.3854 - accuracy: 0.2944\n",
      "Epoch 2/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.3781 - accuracy: 0.3533\n",
      "Epoch 3/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.3472 - accuracy: 0.5875\n",
      "Epoch 4/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.2412 - accuracy: 0.5803\n",
      "Epoch 5/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0064 - accuracy: 0.5940\n",
      "Epoch 6/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.7894 - accuracy: 0.5912\n",
      "Epoch 7/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6996 - accuracy: 0.5898\n",
      "Epoch 8/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6906 - accuracy: 0.5841\n",
      "Epoch 9/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6798 - accuracy: 0.5706\n",
      "Epoch 10/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6790 - accuracy: 0.5862\n",
      "Epoch 11/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6687 - accuracy: 0.6038\n",
      "Epoch 12/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6591 - accuracy: 0.6010\n",
      "Epoch 13/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6678 - accuracy: 0.5972\n",
      "Epoch 14/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6741 - accuracy: 0.5744\n",
      "Epoch 15/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6722 - accuracy: 0.5750\n",
      "Epoch 16/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6559 - accuracy: 0.5991\n",
      "Epoch 17/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6549 - accuracy: 0.5914\n",
      "Epoch 18/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6532 - accuracy: 0.6047\n",
      "Epoch 19/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6605 - accuracy: 0.6226\n",
      "Epoch 20/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.6476 - accuracy: 0.6137\n",
      "Epoch 21/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6239 - accuracy: 0.6899\n",
      "Epoch 22/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6465 - accuracy: 0.6547\n",
      "Epoch 23/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6094 - accuracy: 0.6839\n",
      "Epoch 24/150\n",
      "12/12 [==============================] - 0s 998us/step - loss: 0.6099 - accuracy: 0.6740\n",
      "Epoch 25/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5875 - accuracy: 0.7018\n",
      "Epoch 26/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5932 - accuracy: 0.6613\n",
      "Epoch 27/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5849 - accuracy: 0.6851\n",
      "Epoch 28/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5756 - accuracy: 0.6734\n",
      "Epoch 29/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5748 - accuracy: 0.6643\n",
      "Epoch 30/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5549 - accuracy: 0.6809\n",
      "Epoch 31/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5480 - accuracy: 0.6664\n",
      "Epoch 32/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5461 - accuracy: 0.6828\n",
      "Epoch 33/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5333 - accuracy: 0.6963\n",
      "Epoch 34/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5336 - accuracy: 0.7065\n",
      "Epoch 35/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5396 - accuracy: 0.7144\n",
      "Epoch 36/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5238 - accuracy: 0.6926\n",
      "Epoch 37/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5321 - accuracy: 0.7143\n",
      "Epoch 38/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4904 - accuracy: 0.7521\n",
      "Epoch 39/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4904 - accuracy: 0.8282\n",
      "Epoch 40/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4715 - accuracy: 0.8005\n",
      "Epoch 41/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4916 - accuracy: 0.8077\n",
      "Epoch 42/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4666 - accuracy: 0.7968\n",
      "Epoch 43/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4463 - accuracy: 0.8237\n",
      "Epoch 44/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4611 - accuracy: 0.8150\n",
      "Epoch 45/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4115 - accuracy: 0.8595\n",
      "Epoch 46/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4451 - accuracy: 0.8077\n",
      "Epoch 47/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4165 - accuracy: 0.8282\n",
      "Epoch 48/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4359 - accuracy: 0.8140\n",
      "Epoch 49/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4160 - accuracy: 0.8282\n",
      "Epoch 50/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4033 - accuracy: 0.8389\n",
      "Epoch 51/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4140 - accuracy: 0.8052\n",
      "Epoch 52/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3942 - accuracy: 0.8383\n",
      "Epoch 53/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3792 - accuracy: 0.8381\n",
      "Epoch 54/150\n",
      "12/12 [==============================] - 0s 996us/step - loss: 0.4059 - accuracy: 0.8194\n",
      "Epoch 55/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4051 - accuracy: 0.8284\n",
      "Epoch 56/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3565 - accuracy: 0.8560\n",
      "Epoch 57/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3845 - accuracy: 0.8280\n",
      "Epoch 58/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4020 - accuracy: 0.8237\n",
      "Epoch 59/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3558 - accuracy: 0.8430\n",
      "Epoch 60/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3799 - accuracy: 0.8447\n",
      "Epoch 61/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3911 - accuracy: 0.8030\n",
      "Epoch 62/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3762 - accuracy: 0.8311\n",
      "Epoch 63/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3886 - accuracy: 0.8186\n",
      "Epoch 64/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3754 - accuracy: 0.8312\n",
      "Epoch 65/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3857 - accuracy: 0.8107\n",
      "Epoch 66/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3776 - accuracy: 0.8304\n",
      "Epoch 67/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3608 - accuracy: 0.8318\n",
      "Epoch 68/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3473 - accuracy: 0.8473\n",
      "Epoch 69/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3808 - accuracy: 0.7958\n",
      "Epoch 70/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3572 - accuracy: 0.8295\n",
      "Epoch 71/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3560 - accuracy: 0.8320\n",
      "Epoch 72/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3860 - accuracy: 0.8025\n",
      "Epoch 73/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3362 - accuracy: 0.8514\n",
      "Epoch 74/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3540 - accuracy: 0.8325\n",
      "Epoch 75/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3606 - accuracy: 0.8479\n",
      "Epoch 76/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3483 - accuracy: 0.8495\n",
      "Epoch 77/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3842 - accuracy: 0.8243\n",
      "Epoch 78/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3740 - accuracy: 0.8193\n",
      "Epoch 79/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3418 - accuracy: 0.8378\n",
      "Epoch 80/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3295 - accuracy: 0.8535\n",
      "Epoch 81/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3584 - accuracy: 0.8142\n",
      "Epoch 82/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3703 - accuracy: 0.8102\n",
      "Epoch 83/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3605 - accuracy: 0.8303\n",
      "Epoch 84/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3182 - accuracy: 0.8559\n",
      "Epoch 85/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3612 - accuracy: 0.8303\n",
      "Epoch 86/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3982 - accuracy: 0.8158\n",
      "Epoch 87/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3314 - accuracy: 0.8508\n",
      "Epoch 88/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3140 - accuracy: 0.8692\n",
      "Epoch 89/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3568 - accuracy: 0.8351\n",
      "Epoch 90/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3598 - accuracy: 0.8308\n",
      "Epoch 91/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3572 - accuracy: 0.8564\n",
      "Epoch 92/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3768 - accuracy: 0.8105\n",
      "Epoch 93/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3388 - accuracy: 0.8351\n",
      "Epoch 94/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3550 - accuracy: 0.8206\n",
      "Epoch 95/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3383 - accuracy: 0.8498\n",
      "Epoch 96/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3189 - accuracy: 0.8525\n",
      "Epoch 97/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3457 - accuracy: 0.8201\n",
      "Epoch 98/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3035 - accuracy: 0.8808\n",
      "Epoch 99/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3452 - accuracy: 0.8261\n",
      "Epoch 100/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3571 - accuracy: 0.8489\n",
      "Epoch 101/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3320 - accuracy: 0.8430\n",
      "Epoch 102/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3340 - accuracy: 0.8406\n",
      "Epoch 103/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3533 - accuracy: 0.8179\n",
      "Epoch 104/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3561 - accuracy: 0.8220\n",
      "Epoch 105/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3539 - accuracy: 0.8334\n",
      "Epoch 106/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3639 - accuracy: 0.8411\n",
      "Epoch 107/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3480 - accuracy: 0.8284\n",
      "Epoch 108/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3296 - accuracy: 0.8504\n",
      "Epoch 109/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3176 - accuracy: 0.8532\n",
      "Epoch 110/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3733 - accuracy: 0.8198\n",
      "Epoch 111/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3320 - accuracy: 0.8513\n",
      "Epoch 112/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3165 - accuracy: 0.8513\n",
      "Epoch 113/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3422 - accuracy: 0.8441\n",
      "Epoch 114/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3293 - accuracy: 0.8442\n",
      "Epoch 115/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3041 - accuracy: 0.8626\n",
      "Epoch 116/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3425 - accuracy: 0.8277\n",
      "Epoch 117/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3338 - accuracy: 0.8445\n",
      "Epoch 118/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3174 - accuracy: 0.8600\n",
      "Epoch 119/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3042 - accuracy: 0.8697\n",
      "Epoch 120/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2980 - accuracy: 0.8640\n",
      "Epoch 121/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8417\n",
      "Epoch 122/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3337 - accuracy: 0.8340\n",
      "Epoch 123/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3344 - accuracy: 0.8424\n",
      "Epoch 124/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3082 - accuracy: 0.8629\n",
      "Epoch 125/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3117 - accuracy: 0.8598\n",
      "Epoch 126/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3308 - accuracy: 0.8399\n",
      "Epoch 127/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3355 - accuracy: 0.8315\n",
      "Epoch 128/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3153 - accuracy: 0.8547\n",
      "Epoch 129/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3329 - accuracy: 0.8288\n",
      "Epoch 130/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2991 - accuracy: 0.8586\n",
      "Epoch 131/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3059 - accuracy: 0.8520\n",
      "Epoch 132/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3279 - accuracy: 0.8499\n",
      "Epoch 133/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2984 - accuracy: 0.8668\n",
      "Epoch 134/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3192 - accuracy: 0.8424\n",
      "Epoch 135/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3284 - accuracy: 0.8327\n",
      "Epoch 136/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3073 - accuracy: 0.8564\n",
      "Epoch 137/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3142 - accuracy: 0.8575\n",
      "Epoch 138/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2953 - accuracy: 0.8525\n",
      "Epoch 139/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3073 - accuracy: 0.8433\n",
      "Epoch 140/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2828 - accuracy: 0.8766\n",
      "Epoch 141/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3110 - accuracy: 0.8710\n",
      "Epoch 142/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3179 - accuracy: 0.8601\n",
      "Epoch 143/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3026 - accuracy: 0.8600\n",
      "Epoch 144/150\n",
      "12/12 [==============================] - 0s 998us/step - loss: 0.3284 - accuracy: 0.8238\n",
      "Epoch 145/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3281 - accuracy: 0.8588\n",
      "Epoch 146/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3287 - accuracy: 0.8440\n",
      "Epoch 147/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2973 - accuracy: 0.8828\n",
      "Epoch 148/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3295 - accuracy: 0.8369\n",
      "Epoch 149/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3143 - accuracy: 0.8635\n",
      "Epoch 150/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3020 - accuracy: 0.8691\n",
      "WARNING:tensorflow:5 out of the last 28 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000281D7D1D3A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 998us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\position\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.3855 - accuracy: 0.3195\n",
      "Epoch 2/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.3777 - accuracy: 0.4323\n",
      "Epoch 3/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.3425 - accuracy: 0.5702\n",
      "Epoch 4/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.2244 - accuracy: 0.5435\n",
      "Epoch 5/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9819 - accuracy: 0.5808\n",
      "Epoch 6/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.7737 - accuracy: 0.5770\n",
      "Epoch 7/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6967 - accuracy: 0.5812\n",
      "Epoch 8/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6949 - accuracy: 0.5544\n",
      "Epoch 9/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6781 - accuracy: 0.5830\n",
      "Epoch 10/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6763 - accuracy: 0.5808\n",
      "Epoch 11/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6858 - accuracy: 0.6012\n",
      "Epoch 12/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6700 - accuracy: 0.6049\n",
      "Epoch 13/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6789 - accuracy: 0.6012\n",
      "Epoch 14/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6651 - accuracy: 0.6249\n",
      "Epoch 15/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6674 - accuracy: 0.6172\n",
      "Epoch 16/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6517 - accuracy: 0.6336\n",
      "Epoch 17/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6464 - accuracy: 0.6393\n",
      "Epoch 18/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6270 - accuracy: 0.6714\n",
      "Epoch 19/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6304 - accuracy: 0.6490\n",
      "Epoch 20/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6445 - accuracy: 0.6162\n",
      "Epoch 21/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5998 - accuracy: 0.6949\n",
      "Epoch 22/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5899 - accuracy: 0.6786\n",
      "Epoch 23/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5972 - accuracy: 0.6978\n",
      "Epoch 24/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6113 - accuracy: 0.6691\n",
      "Epoch 25/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6202 - accuracy: 0.6413\n",
      "Epoch 26/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6094 - accuracy: 0.6482\n",
      "Epoch 27/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6164 - accuracy: 0.6354\n",
      "Epoch 28/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5941 - accuracy: 0.6682\n",
      "Epoch 29/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5944 - accuracy: 0.6696\n",
      "Epoch 30/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5597 - accuracy: 0.7066\n",
      "Epoch 31/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5855 - accuracy: 0.6817\n",
      "Epoch 32/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5640 - accuracy: 0.6999\n",
      "Epoch 33/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5736 - accuracy: 0.6785\n",
      "Epoch 34/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5757 - accuracy: 0.6826\n",
      "Epoch 35/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5782 - accuracy: 0.6580\n",
      "Epoch 36/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5892 - accuracy: 0.6636\n",
      "Epoch 37/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5764 - accuracy: 0.6587\n",
      "Epoch 38/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5732 - accuracy: 0.6705\n",
      "Epoch 39/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5565 - accuracy: 0.6725\n",
      "Epoch 40/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5654 - accuracy: 0.6571\n",
      "Epoch 41/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5700 - accuracy: 0.6715\n",
      "Epoch 42/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5366 - accuracy: 0.7084\n",
      "Epoch 43/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5518 - accuracy: 0.6706\n",
      "Epoch 44/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5534 - accuracy: 0.6676\n",
      "Epoch 45/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5291 - accuracy: 0.6973\n",
      "Epoch 46/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5501 - accuracy: 0.6832\n",
      "Epoch 47/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5438 - accuracy: 0.6877\n",
      "Epoch 48/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5400 - accuracy: 0.6924\n",
      "Epoch 49/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5345 - accuracy: 0.6825\n",
      "Epoch 50/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5270 - accuracy: 0.6924\n",
      "Epoch 51/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5269 - accuracy: 0.7063\n",
      "Epoch 52/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5192 - accuracy: 0.7131\n",
      "Epoch 53/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5413 - accuracy: 0.7088\n",
      "Epoch 54/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5413 - accuracy: 0.6687\n",
      "Epoch 55/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5398 - accuracy: 0.6984\n",
      "Epoch 56/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5228 - accuracy: 0.7074\n",
      "Epoch 57/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5346 - accuracy: 0.7305\n",
      "Epoch 58/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5400 - accuracy: 0.6931\n",
      "Epoch 59/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5174 - accuracy: 0.7006\n",
      "Epoch 60/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5127 - accuracy: 0.7667\n",
      "Epoch 61/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5196 - accuracy: 0.6930\n",
      "Epoch 62/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5366 - accuracy: 0.6993\n",
      "Epoch 63/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5152 - accuracy: 0.6853\n",
      "Epoch 64/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5325 - accuracy: 0.7026\n",
      "Epoch 65/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5185 - accuracy: 0.7103\n",
      "Epoch 66/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5127 - accuracy: 0.7384\n",
      "Epoch 67/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5013 - accuracy: 0.7075\n",
      "Epoch 68/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4985 - accuracy: 0.7409\n",
      "Epoch 69/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.5123 - accuracy: 0.7434\n",
      "Epoch 70/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5161 - accuracy: 0.7351\n",
      "Epoch 71/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5068 - accuracy: 0.7341\n",
      "Epoch 72/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5115 - accuracy: 0.7579\n",
      "Epoch 73/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5169 - accuracy: 0.7044\n",
      "Epoch 74/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5228 - accuracy: 0.8195\n",
      "Epoch 75/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4990 - accuracy: 0.7377\n",
      "Epoch 76/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5025 - accuracy: 0.8189\n",
      "Epoch 77/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4763 - accuracy: 0.7454\n",
      "Epoch 78/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4881 - accuracy: 0.8304\n",
      "Epoch 79/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4992 - accuracy: 0.7528\n",
      "Epoch 80/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4761 - accuracy: 0.8008\n",
      "Epoch 81/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4787 - accuracy: 0.8429\n",
      "Epoch 82/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4706 - accuracy: 0.7873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4662 - accuracy: 0.8304\n",
      "Epoch 84/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4645 - accuracy: 0.8392\n",
      "Epoch 85/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4608 - accuracy: 0.8360\n",
      "Epoch 86/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4841 - accuracy: 0.8211\n",
      "Epoch 87/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4581 - accuracy: 0.8163\n",
      "Epoch 88/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4455 - accuracy: 0.8459\n",
      "Epoch 89/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4396 - accuracy: 0.8383\n",
      "Epoch 90/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4371 - accuracy: 0.8531\n",
      "Epoch 91/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4016 - accuracy: 0.8656\n",
      "Epoch 92/150\n",
      "12/12 [==============================] - 0s 998us/step - loss: 0.4289 - accuracy: 0.8392\n",
      "Epoch 93/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4401 - accuracy: 0.8298\n",
      "Epoch 94/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4266 - accuracy: 0.8480\n",
      "Epoch 95/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3894 - accuracy: 0.8403\n",
      "Epoch 96/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3934 - accuracy: 0.8617\n",
      "Epoch 97/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4190 - accuracy: 0.8464\n",
      "Epoch 98/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3848 - accuracy: 0.8564\n",
      "Epoch 99/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4200 - accuracy: 0.8243\n",
      "Epoch 100/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3868 - accuracy: 0.8494\n",
      "Epoch 101/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8427\n",
      "Epoch 102/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3803 - accuracy: 0.8690\n",
      "Epoch 103/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3789 - accuracy: 0.8508\n",
      "Epoch 104/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3782 - accuracy: 0.8465\n",
      "Epoch 105/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3686 - accuracy: 0.8610\n",
      "Epoch 106/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8276\n",
      "Epoch 107/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3660 - accuracy: 0.8545\n",
      "Epoch 108/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3952 - accuracy: 0.8259\n",
      "Epoch 109/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3623 - accuracy: 0.8588\n",
      "Epoch 110/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3905 - accuracy: 0.8246\n",
      "Epoch 111/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3968 - accuracy: 0.8307\n",
      "Epoch 112/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3519 - accuracy: 0.8575\n",
      "Epoch 113/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3705 - accuracy: 0.8611\n",
      "Epoch 114/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3471 - accuracy: 0.8499\n",
      "Epoch 115/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3587 - accuracy: 0.8707\n",
      "Epoch 116/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3653 - accuracy: 0.8480\n",
      "Epoch 117/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3714 - accuracy: 0.8475\n",
      "Epoch 118/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3656 - accuracy: 0.8530\n",
      "Epoch 119/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3891 - accuracy: 0.8418\n",
      "Epoch 120/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3571 - accuracy: 0.8676\n",
      "Epoch 121/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3571 - accuracy: 0.8566\n",
      "Epoch 122/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3780 - accuracy: 0.8405\n",
      "Epoch 123/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3457 - accuracy: 0.8618\n",
      "Epoch 124/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3646 - accuracy: 0.8556\n",
      "Epoch 125/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3638 - accuracy: 0.8560\n",
      "Epoch 126/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3476 - accuracy: 0.8649\n",
      "Epoch 127/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3496 - accuracy: 0.8486\n",
      "Epoch 128/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3440 - accuracy: 0.8565\n",
      "Epoch 129/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3339 - accuracy: 0.8695\n",
      "Epoch 130/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3633 - accuracy: 0.8413\n",
      "Epoch 131/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3491 - accuracy: 0.8555\n",
      "Epoch 132/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3389 - accuracy: 0.8513\n",
      "Epoch 133/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3428 - accuracy: 0.8615\n",
      "Epoch 134/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3385 - accuracy: 0.8536\n",
      "Epoch 135/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3189 - accuracy: 0.8688\n",
      "Epoch 136/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3687 - accuracy: 0.8364\n",
      "Epoch 137/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3810 - accuracy: 0.8415\n",
      "Epoch 138/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3594 - accuracy: 0.8345\n",
      "Epoch 139/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3735 - accuracy: 0.8339\n",
      "Epoch 140/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3665 - accuracy: 0.8502\n",
      "Epoch 141/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3486 - accuracy: 0.8578\n",
      "Epoch 142/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3436 - accuracy: 0.8474\n",
      "Epoch 143/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3544 - accuracy: 0.8475\n",
      "Epoch 144/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3397 - accuracy: 0.8645\n",
      "Epoch 145/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3386 - accuracy: 0.8526\n",
      "Epoch 146/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3319 - accuracy: 0.8595\n",
      "Epoch 147/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3358 - accuracy: 0.8497\n",
      "Epoch 148/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3348 - accuracy: 0.8622\n",
      "Epoch 149/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3316 - accuracy: 0.8570\n",
      "Epoch 150/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3358 - accuracy: 0.8651\n",
      "WARNING:tensorflow:6 out of the last 30 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000281D5837280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 997us/step\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\position\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 1ms/step - loss: 1.3856 - accuracy: 0.3408\n",
      "Epoch 2/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.3791 - accuracy: 0.5599\n",
      "Epoch 3/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.3508 - accuracy: 0.5429\n",
      "Epoch 4/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.2483 - accuracy: 0.5545\n",
      "Epoch 5/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0277 - accuracy: 0.5776\n",
      "Epoch 6/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8301 - accuracy: 0.5702\n",
      "Epoch 7/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.7130 - accuracy: 0.5689\n",
      "Epoch 8/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6893 - accuracy: 0.5795\n",
      "Epoch 9/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6817 - accuracy: 0.5847\n",
      "Epoch 10/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.6834 - accuracy: 0.5858\n",
      "Epoch 11/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6845 - accuracy: 0.5536\n",
      "Epoch 12/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6778 - accuracy: 0.5770\n",
      "Epoch 13/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6713 - accuracy: 0.5856\n",
      "Epoch 14/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6807 - accuracy: 0.5535\n",
      "Epoch 15/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6758 - accuracy: 0.5818\n",
      "Epoch 16/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6787 - accuracy: 0.5699\n",
      "Epoch 17/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6740 - accuracy: 0.5782\n",
      "Epoch 18/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6881 - accuracy: 0.5507\n",
      "Epoch 19/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6779 - accuracy: 0.5568\n",
      "Epoch 20/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6697 - accuracy: 0.5725\n",
      "Epoch 21/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6719 - accuracy: 0.5623\n",
      "Epoch 22/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6770 - accuracy: 0.5654\n",
      "Epoch 23/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6701 - accuracy: 0.5682\n",
      "Epoch 24/150\n",
      "12/12 [==============================] - 0s 996us/step - loss: 0.6616 - accuracy: 0.5785\n",
      "Epoch 25/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6618 - accuracy: 0.5795\n",
      "Epoch 26/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6686 - accuracy: 0.5567\n",
      "Epoch 27/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6572 - accuracy: 0.5756\n",
      "Epoch 28/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6492 - accuracy: 0.5938\n",
      "Epoch 29/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6422 - accuracy: 0.5888\n",
      "Epoch 30/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6389 - accuracy: 0.6199\n",
      "Epoch 31/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6502 - accuracy: 0.5842\n",
      "Epoch 32/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6348 - accuracy: 0.6105\n",
      "Epoch 33/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6292 - accuracy: 0.6236\n",
      "Epoch 34/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6183 - accuracy: 0.6273\n",
      "Epoch 35/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6235 - accuracy: 0.6379\n",
      "Epoch 36/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6098 - accuracy: 0.6361\n",
      "Epoch 37/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6219 - accuracy: 0.6285\n",
      "Epoch 38/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5991 - accuracy: 0.6337\n",
      "Epoch 39/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5966 - accuracy: 0.6362\n",
      "Epoch 40/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5878 - accuracy: 0.6503\n",
      "Epoch 41/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5924 - accuracy: 0.6497\n",
      "Epoch 42/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5832 - accuracy: 0.6584\n",
      "Epoch 43/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5836 - accuracy: 0.6488\n",
      "Epoch 44/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5847 - accuracy: 0.6430\n",
      "Epoch 45/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5798 - accuracy: 0.6416\n",
      "Epoch 46/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5639 - accuracy: 0.6541\n",
      "Epoch 47/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5740 - accuracy: 0.6351\n",
      "Epoch 48/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5489 - accuracy: 0.6735\n",
      "Epoch 49/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5769 - accuracy: 0.6612\n",
      "Epoch 50/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5694 - accuracy: 0.6731\n",
      "Epoch 51/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5476 - accuracy: 0.7240\n",
      "Epoch 52/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5455 - accuracy: 0.6886\n",
      "Epoch 53/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5532 - accuracy: 0.6643\n",
      "Epoch 54/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5378 - accuracy: 0.7183\n",
      "Epoch 55/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5597 - accuracy: 0.6847\n",
      "Epoch 56/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5342 - accuracy: 0.7155\n",
      "Epoch 57/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5314 - accuracy: 0.7568\n",
      "Epoch 58/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5263 - accuracy: 0.7355\n",
      "Epoch 59/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.5494 - accuracy: 0.7134\n",
      "Epoch 60/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.5226 - accuracy: 0.7401\n",
      "Epoch 61/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5367 - accuracy: 0.7563\n",
      "Epoch 62/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5212 - accuracy: 0.7544\n",
      "Epoch 63/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5141 - accuracy: 0.7662\n",
      "Epoch 64/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5051 - accuracy: 0.7735\n",
      "Epoch 65/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5124 - accuracy: 0.7876\n",
      "Epoch 66/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4878 - accuracy: 0.7802\n",
      "Epoch 67/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5169 - accuracy: 0.7697\n",
      "Epoch 68/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4820 - accuracy: 0.7657\n",
      "Epoch 69/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.4706 - accuracy: 0.8066\n",
      "Epoch 70/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4841 - accuracy: 0.7865\n",
      "Epoch 71/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4613 - accuracy: 0.7956\n",
      "Epoch 72/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.4676 - accuracy: 0.7784\n",
      "Epoch 73/150\n",
      "12/12 [==============================] - 0s 996us/step - loss: 0.4935 - accuracy: 0.8031\n",
      "Epoch 74/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.4603 - accuracy: 0.7788\n",
      "Epoch 75/150\n",
      "12/12 [==============================] - 0s 996us/step - loss: 0.4758 - accuracy: 0.7762\n",
      "Epoch 76/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5004 - accuracy: 0.7956\n",
      "Epoch 77/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4610 - accuracy: 0.7756\n",
      "Epoch 78/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4456 - accuracy: 0.7840\n",
      "Epoch 79/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4365 - accuracy: 0.8091\n",
      "Epoch 80/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4669 - accuracy: 0.7634\n",
      "Epoch 81/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4556 - accuracy: 0.7892\n",
      "Epoch 82/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4125 - accuracy: 0.8215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4471 - accuracy: 0.7834\n",
      "Epoch 84/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4193 - accuracy: 0.8103\n",
      "Epoch 85/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4362 - accuracy: 0.7904\n",
      "Epoch 86/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4298 - accuracy: 0.8124\n",
      "Epoch 87/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4432 - accuracy: 0.7741\n",
      "Epoch 88/150\n",
      "12/12 [==============================] - 0s 994us/step - loss: 0.4364 - accuracy: 0.7859\n",
      "Epoch 89/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4249 - accuracy: 0.8147\n",
      "Epoch 90/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4228 - accuracy: 0.7877\n",
      "Epoch 91/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4308 - accuracy: 0.7826\n",
      "Epoch 92/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4056 - accuracy: 0.8309\n",
      "Epoch 93/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4137 - accuracy: 0.8033\n",
      "Epoch 94/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3906 - accuracy: 0.8096\n",
      "Epoch 95/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4180 - accuracy: 0.7856\n",
      "Epoch 96/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4342 - accuracy: 0.8184\n",
      "Epoch 97/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4183 - accuracy: 0.7975\n",
      "Epoch 98/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4044 - accuracy: 0.8169\n",
      "Epoch 99/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.4065 - accuracy: 0.8016\n",
      "Epoch 100/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3939 - accuracy: 0.8168\n",
      "Epoch 101/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3979 - accuracy: 0.8135\n",
      "Epoch 102/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4054 - accuracy: 0.8292\n",
      "Epoch 103/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4397 - accuracy: 0.7717\n",
      "Epoch 104/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4070 - accuracy: 0.8123\n",
      "Epoch 105/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4177 - accuracy: 0.7958\n",
      "Epoch 106/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3956 - accuracy: 0.8207\n",
      "Epoch 107/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3926 - accuracy: 0.8104\n",
      "Epoch 108/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3863 - accuracy: 0.8116\n",
      "Epoch 109/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4118 - accuracy: 0.8024\n",
      "Epoch 110/150\n",
      "12/12 [==============================] - 0s 996us/step - loss: 0.3797 - accuracy: 0.8138\n",
      "Epoch 111/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3925 - accuracy: 0.8210\n",
      "Epoch 112/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4131 - accuracy: 0.7908\n",
      "Epoch 113/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3911 - accuracy: 0.8326\n",
      "Epoch 114/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3817 - accuracy: 0.8544\n",
      "Epoch 115/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3873 - accuracy: 0.8040\n",
      "Epoch 116/150\n",
      "12/12 [==============================] - 0s 998us/step - loss: 0.3749 - accuracy: 0.8300\n",
      "Epoch 117/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4425 - accuracy: 0.7751\n",
      "Epoch 118/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3846 - accuracy: 0.8290\n",
      "Epoch 119/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3765 - accuracy: 0.8039\n",
      "Epoch 120/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4127 - accuracy: 0.8025\n",
      "Epoch 121/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3498 - accuracy: 0.8445\n",
      "Epoch 122/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4017 - accuracy: 0.7920\n",
      "Epoch 123/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3753 - accuracy: 0.8343\n",
      "Epoch 124/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4087 - accuracy: 0.8000\n",
      "Epoch 125/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3684 - accuracy: 0.8395\n",
      "Epoch 126/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3910 - accuracy: 0.8105\n",
      "Epoch 127/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3690 - accuracy: 0.8224\n",
      "Epoch 128/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3957 - accuracy: 0.8002\n",
      "Epoch 129/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3834 - accuracy: 0.8210\n",
      "Epoch 130/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3811 - accuracy: 0.8249\n",
      "Epoch 131/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3553 - accuracy: 0.8435\n",
      "Epoch 132/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3566 - accuracy: 0.8205\n",
      "Epoch 133/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3527 - accuracy: 0.8331\n",
      "Epoch 134/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3530 - accuracy: 0.8225\n",
      "Epoch 135/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3826 - accuracy: 0.8229\n",
      "Epoch 136/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3613 - accuracy: 0.8268\n",
      "Epoch 137/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3572 - accuracy: 0.8442\n",
      "Epoch 138/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3611 - accuracy: 0.8265\n",
      "Epoch 139/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3711 - accuracy: 0.8124\n",
      "Epoch 140/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4186 - accuracy: 0.7875\n",
      "Epoch 141/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3728 - accuracy: 0.8503\n",
      "Epoch 142/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3671 - accuracy: 0.8248\n",
      "Epoch 143/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3399 - accuracy: 0.8480\n",
      "Epoch 144/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3546 - accuracy: 0.8440\n",
      "Epoch 145/150\n",
      "12/12 [==============================] - 0s 996us/step - loss: 0.3745 - accuracy: 0.8210\n",
      "Epoch 146/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3668 - accuracy: 0.8365\n",
      "Epoch 147/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3499 - accuracy: 0.8350\n",
      "Epoch 148/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3845 - accuracy: 0.8205\n",
      "Epoch 149/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3238 - accuracy: 0.8436\n",
      "Epoch 150/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3514 - accuracy: 0.8285\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000281D8F33550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\position\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 1ms/step - loss: 1.3851 - accuracy: 0.3623\n",
      "Epoch 2/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.3727 - accuracy: 0.3566\n",
      "Epoch 3/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.3230 - accuracy: 0.5688\n",
      "Epoch 4/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.1844 - accuracy: 0.5607\n",
      "Epoch 5/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9262 - accuracy: 0.5562\n",
      "Epoch 6/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.7586 - accuracy: 0.5492\n",
      "Epoch 7/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.7019 - accuracy: 0.5588\n",
      "Epoch 8/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6995 - accuracy: 0.5535\n",
      "Epoch 9/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6885 - accuracy: 0.5491\n",
      "Epoch 10/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6914 - accuracy: 0.5356\n",
      "Epoch 11/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6842 - accuracy: 0.5500\n",
      "Epoch 12/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6856 - accuracy: 0.5408\n",
      "Epoch 13/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6779 - accuracy: 0.5859\n",
      "Epoch 14/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6742 - accuracy: 0.5968\n",
      "Epoch 15/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6723 - accuracy: 0.5899\n",
      "Epoch 16/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6661 - accuracy: 0.6070\n",
      "Epoch 17/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6668 - accuracy: 0.6074\n",
      "Epoch 18/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.6604 - accuracy: 0.6245\n",
      "Epoch 19/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6345 - accuracy: 0.6776\n",
      "Epoch 20/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6414 - accuracy: 0.6573\n",
      "Epoch 21/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6427 - accuracy: 0.6450\n",
      "Epoch 22/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6365 - accuracy: 0.6268\n",
      "Epoch 23/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6317 - accuracy: 0.6432\n",
      "Epoch 24/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6147 - accuracy: 0.6454\n",
      "Epoch 25/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6027 - accuracy: 0.6483\n",
      "Epoch 26/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.5972 - accuracy: 0.6565\n",
      "Epoch 27/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5953 - accuracy: 0.6488\n",
      "Epoch 28/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6142 - accuracy: 0.6353\n",
      "Epoch 29/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5913 - accuracy: 0.6433\n",
      "Epoch 30/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5953 - accuracy: 0.6315\n",
      "Epoch 31/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5998 - accuracy: 0.6517\n",
      "Epoch 32/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5710 - accuracy: 0.6761\n",
      "Epoch 33/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5673 - accuracy: 0.6807\n",
      "Epoch 34/150\n",
      "12/12 [==============================] - 0s 999us/step - loss: 0.5704 - accuracy: 0.6775\n",
      "Epoch 35/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5596 - accuracy: 0.6860\n",
      "Epoch 36/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5618 - accuracy: 0.6700\n",
      "Epoch 37/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5700 - accuracy: 0.6435\n",
      "Epoch 38/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5798 - accuracy: 0.6545\n",
      "Epoch 39/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5594 - accuracy: 0.6592\n",
      "Epoch 40/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5584 - accuracy: 0.6646\n",
      "Epoch 41/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5604 - accuracy: 0.6569\n",
      "Epoch 42/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5753 - accuracy: 0.6792\n",
      "Epoch 43/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5551 - accuracy: 0.6625\n",
      "Epoch 44/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5448 - accuracy: 0.7314\n",
      "Epoch 45/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5690 - accuracy: 0.6511\n",
      "Epoch 46/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5598 - accuracy: 0.7139\n",
      "Epoch 47/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5795 - accuracy: 0.6469\n",
      "Epoch 48/150\n",
      "12/12 [==============================] - 0s 999us/step - loss: 0.5305 - accuracy: 0.6894\n",
      "Epoch 49/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5260 - accuracy: 0.7203\n",
      "Epoch 50/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5431 - accuracy: 0.7359\n",
      "Epoch 51/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.5522 - accuracy: 0.6956\n",
      "Epoch 52/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5431 - accuracy: 0.7185\n",
      "Epoch 53/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5489 - accuracy: 0.6652\n",
      "Epoch 54/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.5271 - accuracy: 0.7276\n",
      "Epoch 55/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5057 - accuracy: 0.7198\n",
      "Epoch 56/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5404 - accuracy: 0.6873\n",
      "Epoch 57/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5504 - accuracy: 0.7081\n",
      "Epoch 58/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5238 - accuracy: 0.7334\n",
      "Epoch 59/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5350 - accuracy: 0.7487\n",
      "Epoch 60/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5145 - accuracy: 0.7740\n",
      "Epoch 61/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5325 - accuracy: 0.7505\n",
      "Epoch 62/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5228 - accuracy: 0.7006\n",
      "Epoch 63/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5283 - accuracy: 0.7345\n",
      "Epoch 64/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5454 - accuracy: 0.6732\n",
      "Epoch 65/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5052 - accuracy: 0.7817\n",
      "Epoch 66/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5282 - accuracy: 0.7298\n",
      "Epoch 67/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5231 - accuracy: 0.7285\n",
      "Epoch 68/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5135 - accuracy: 0.7341\n",
      "Epoch 69/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5265 - accuracy: 0.7474\n",
      "Epoch 70/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5194 - accuracy: 0.7448\n",
      "Epoch 71/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5156 - accuracy: 0.7534\n",
      "Epoch 72/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4990 - accuracy: 0.7747\n",
      "Epoch 73/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5225 - accuracy: 0.7550\n",
      "Epoch 74/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5236 - accuracy: 0.7860\n",
      "Epoch 75/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5170 - accuracy: 0.7443\n",
      "Epoch 76/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4774 - accuracy: 0.8319\n",
      "Epoch 77/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4823 - accuracy: 0.7963\n",
      "Epoch 78/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5025 - accuracy: 0.7879\n",
      "Epoch 79/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4855 - accuracy: 0.8042\n",
      "Epoch 80/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4883 - accuracy: 0.8098\n",
      "Epoch 81/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4582 - accuracy: 0.8278\n",
      "Epoch 82/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4580 - accuracy: 0.8042\n",
      "Epoch 83/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4676 - accuracy: 0.8051\n",
      "Epoch 84/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4553 - accuracy: 0.8185\n",
      "Epoch 85/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4436 - accuracy: 0.8246\n",
      "Epoch 86/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4398 - accuracy: 0.8241\n",
      "Epoch 87/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4496 - accuracy: 0.8142\n",
      "Epoch 88/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4399 - accuracy: 0.7845\n",
      "Epoch 89/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3971 - accuracy: 0.8541\n",
      "Epoch 90/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4482 - accuracy: 0.7915\n",
      "Epoch 91/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.4318 - accuracy: 0.7982\n",
      "Epoch 92/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.4243 - accuracy: 0.8328\n",
      "Epoch 93/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4181 - accuracy: 0.7905\n",
      "Epoch 94/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4363 - accuracy: 0.7979\n",
      "Epoch 95/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4125 - accuracy: 0.8440\n",
      "Epoch 96/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3973 - accuracy: 0.8125\n",
      "Epoch 97/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4281 - accuracy: 0.8259\n",
      "Epoch 98/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3799 - accuracy: 0.8653\n",
      "Epoch 99/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3823 - accuracy: 0.8229\n",
      "Epoch 100/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3946 - accuracy: 0.8205\n",
      "Epoch 101/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4047 - accuracy: 0.8296\n",
      "Epoch 102/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3996 - accuracy: 0.8043\n",
      "Epoch 103/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3697 - accuracy: 0.8245\n",
      "Epoch 104/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3966 - accuracy: 0.8215\n",
      "Epoch 105/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3812 - accuracy: 0.8426\n",
      "Epoch 106/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4101 - accuracy: 0.7724\n",
      "Epoch 107/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3779 - accuracy: 0.8403\n",
      "Epoch 108/150\n",
      "12/12 [==============================] - 0s 963us/step - loss: 0.4151 - accuracy: 0.8216\n",
      "Epoch 109/150\n",
      "12/12 [==============================] - 0s 998us/step - loss: 0.4260 - accuracy: 0.7702\n",
      "Epoch 110/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3673 - accuracy: 0.8515\n",
      "Epoch 111/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3792 - accuracy: 0.8199\n",
      "Epoch 112/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4099 - accuracy: 0.7999\n",
      "Epoch 113/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3861 - accuracy: 0.8251\n",
      "Epoch 114/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3981 - accuracy: 0.8089\n",
      "Epoch 115/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4024 - accuracy: 0.8376\n",
      "Epoch 116/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3907 - accuracy: 0.7843\n",
      "Epoch 117/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4028 - accuracy: 0.8343\n",
      "Epoch 118/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3684 - accuracy: 0.8131\n",
      "Epoch 119/150\n",
      "12/12 [==============================] - 0s 1000us/step - loss: 0.3502 - accuracy: 0.8448\n",
      "Epoch 120/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3638 - accuracy: 0.8297\n",
      "Epoch 121/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3609 - accuracy: 0.8239\n",
      "Epoch 122/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3848 - accuracy: 0.8213\n",
      "Epoch 123/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4141 - accuracy: 0.7842\n",
      "Epoch 124/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3769 - accuracy: 0.8338\n",
      "Epoch 125/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3897 - accuracy: 0.8144\n",
      "Epoch 126/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3535 - accuracy: 0.8382\n",
      "Epoch 127/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3708 - accuracy: 0.8405\n",
      "Epoch 128/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3645 - accuracy: 0.8274\n",
      "Epoch 129/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3863 - accuracy: 0.8406\n",
      "Epoch 130/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3767 - accuracy: 0.8441\n",
      "Epoch 131/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3859 - accuracy: 0.8128\n",
      "Epoch 132/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3683 - accuracy: 0.8139\n",
      "Epoch 133/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3581 - accuracy: 0.8233\n",
      "Epoch 134/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3468 - accuracy: 0.8382\n",
      "Epoch 135/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3439 - accuracy: 0.8280\n",
      "Epoch 136/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3671 - accuracy: 0.8430\n",
      "Epoch 137/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3642 - accuracy: 0.8422\n",
      "Epoch 138/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3452 - accuracy: 0.8505\n",
      "Epoch 139/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3675 - accuracy: 0.7920\n",
      "Epoch 140/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3340 - accuracy: 0.8599\n",
      "Epoch 141/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3678 - accuracy: 0.8100\n",
      "Epoch 142/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3725 - accuracy: 0.8274\n",
      "Epoch 143/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3937 - accuracy: 0.7954\n",
      "Epoch 144/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3571 - accuracy: 0.8375\n",
      "Epoch 145/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3426 - accuracy: 0.8375\n",
      "Epoch 146/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3472 - accuracy: 0.8310\n",
      "Epoch 147/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3582 - accuracy: 0.8203\n",
      "Epoch 148/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3684 - accuracy: 0.8286\n",
      "Epoch 149/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3368 - accuracy: 0.8514\n",
      "Epoch 150/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3426 - accuracy: 0.8403\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000281D6A25040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 998us/step\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\position\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 1ms/step - loss: 1.3855 - accuracy: 0.2994\n",
      "Epoch 2/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.3771 - accuracy: 0.3561\n",
      "Epoch 3/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.3437 - accuracy: 0.5606\n",
      "Epoch 4/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.2278 - accuracy: 0.5775\n",
      "Epoch 5/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9909 - accuracy: 0.5789\n",
      "Epoch 6/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.7711 - accuracy: 0.5730\n",
      "Epoch 7/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.7267 - accuracy: 0.5400\n",
      "Epoch 8/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6986 - accuracy: 0.5665\n",
      "Epoch 9/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6894 - accuracy: 0.5571\n",
      "Epoch 10/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6774 - accuracy: 0.5838\n",
      "Epoch 11/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6799 - accuracy: 0.5724\n",
      "Epoch 12/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6776 - accuracy: 0.5740\n",
      "Epoch 13/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6844 - accuracy: 0.5612\n",
      "Epoch 14/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6773 - accuracy: 0.5757\n",
      "Epoch 15/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6688 - accuracy: 0.5851\n",
      "Epoch 16/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6647 - accuracy: 0.6084\n",
      "Epoch 17/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6753 - accuracy: 0.5795\n",
      "Epoch 18/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6685 - accuracy: 0.6030\n",
      "Epoch 19/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6859 - accuracy: 0.5786\n",
      "Epoch 20/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6599 - accuracy: 0.6088\n",
      "Epoch 21/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6587 - accuracy: 0.6496\n",
      "Epoch 22/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6342 - accuracy: 0.6544\n",
      "Epoch 23/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6397 - accuracy: 0.6420\n",
      "Epoch 24/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.6164 - accuracy: 0.6805\n",
      "Epoch 25/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6088 - accuracy: 0.6665\n",
      "Epoch 26/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6095 - accuracy: 0.6907\n",
      "Epoch 27/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6137 - accuracy: 0.6508\n",
      "Epoch 28/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5972 - accuracy: 0.6860\n",
      "Epoch 29/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5837 - accuracy: 0.6739\n",
      "Epoch 30/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5744 - accuracy: 0.6829\n",
      "Epoch 31/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5983 - accuracy: 0.6466\n",
      "Epoch 32/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5877 - accuracy: 0.6677\n",
      "Epoch 33/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5649 - accuracy: 0.6840\n",
      "Epoch 34/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5758 - accuracy: 0.6653\n",
      "Epoch 35/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5666 - accuracy: 0.6669\n",
      "Epoch 36/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5700 - accuracy: 0.6627\n",
      "Epoch 37/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5670 - accuracy: 0.6648\n",
      "Epoch 38/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5602 - accuracy: 0.6544\n",
      "Epoch 39/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5539 - accuracy: 0.6854\n",
      "Epoch 40/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5514 - accuracy: 0.6796\n",
      "Epoch 41/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5189 - accuracy: 0.6968\n",
      "Epoch 42/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5490 - accuracy: 0.6787\n",
      "Epoch 43/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5462 - accuracy: 0.6704\n",
      "Epoch 44/150\n",
      "12/12 [==============================] - 0s 999us/step - loss: 0.5576 - accuracy: 0.6507\n",
      "Epoch 45/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5529 - accuracy: 0.6599\n",
      "Epoch 46/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5513 - accuracy: 0.6757\n",
      "Epoch 47/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5197 - accuracy: 0.6917\n",
      "Epoch 48/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5402 - accuracy: 0.6730\n",
      "Epoch 49/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5186 - accuracy: 0.6910\n",
      "Epoch 50/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5301 - accuracy: 0.7210\n",
      "Epoch 51/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5454 - accuracy: 0.6631\n",
      "Epoch 52/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5252 - accuracy: 0.7467\n",
      "Epoch 53/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5054 - accuracy: 0.6959\n",
      "Epoch 54/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5320 - accuracy: 0.7249\n",
      "Epoch 55/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5344 - accuracy: 0.6780\n",
      "Epoch 56/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5506 - accuracy: 0.7058\n",
      "Epoch 57/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5204 - accuracy: 0.6867\n",
      "Epoch 58/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5234 - accuracy: 0.7330\n",
      "Epoch 59/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5239 - accuracy: 0.7272\n",
      "Epoch 60/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5455 - accuracy: 0.7567\n",
      "Epoch 61/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5423 - accuracy: 0.6679\n",
      "Epoch 62/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5375 - accuracy: 0.6869\n",
      "Epoch 63/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5184 - accuracy: 0.7096\n",
      "Epoch 64/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5220 - accuracy: 0.7311\n",
      "Epoch 65/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5147 - accuracy: 0.7026\n",
      "Epoch 66/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5133 - accuracy: 0.7160\n",
      "Epoch 67/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5007 - accuracy: 0.7455\n",
      "Epoch 68/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4991 - accuracy: 0.7305\n",
      "Epoch 69/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5035 - accuracy: 0.7236\n",
      "Epoch 70/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4937 - accuracy: 0.7585\n",
      "Epoch 71/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5181 - accuracy: 0.7238\n",
      "Epoch 72/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5271 - accuracy: 0.7129\n",
      "Epoch 73/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5303 - accuracy: 0.7493\n",
      "Epoch 74/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4920 - accuracy: 0.7582\n",
      "Epoch 75/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.4994 - accuracy: 0.7317\n",
      "Epoch 76/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4902 - accuracy: 0.7396\n",
      "Epoch 77/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4826 - accuracy: 0.7824\n",
      "Epoch 78/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4920 - accuracy: 0.7505\n",
      "Epoch 79/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5212 - accuracy: 0.7140\n",
      "Epoch 80/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5154 - accuracy: 0.7001\n",
      "Epoch 81/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.4820 - accuracy: 0.8069\n",
      "Epoch 82/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4962 - accuracy: 0.7488\n",
      "Epoch 83/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5032 - accuracy: 0.7388\n",
      "Epoch 84/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5118 - accuracy: 0.7376\n",
      "Epoch 85/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4888 - accuracy: 0.7310\n",
      "Epoch 86/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4821 - accuracy: 0.7787\n",
      "Epoch 87/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4713 - accuracy: 0.7623\n",
      "Epoch 88/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5088 - accuracy: 0.7656\n",
      "Epoch 89/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.4796 - accuracy: 0.7745\n",
      "Epoch 90/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4627 - accuracy: 0.7949\n",
      "Epoch 91/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4546 - accuracy: 0.7888\n",
      "Epoch 92/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4729 - accuracy: 0.8161\n",
      "Epoch 93/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4750 - accuracy: 0.7624\n",
      "Epoch 94/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4850 - accuracy: 0.7612\n",
      "Epoch 95/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4748 - accuracy: 0.8078\n",
      "Epoch 96/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4605 - accuracy: 0.8255\n",
      "Epoch 97/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4212 - accuracy: 0.8184\n",
      "Epoch 98/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4556 - accuracy: 0.7685\n",
      "Epoch 99/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4279 - accuracy: 0.8104\n",
      "Epoch 100/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.4506 - accuracy: 0.7816\n",
      "Epoch 101/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4417 - accuracy: 0.7998\n",
      "Epoch 102/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4622 - accuracy: 0.7809\n",
      "Epoch 103/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4461 - accuracy: 0.7779\n",
      "Epoch 104/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4471 - accuracy: 0.8099\n",
      "Epoch 105/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4138 - accuracy: 0.8246\n",
      "Epoch 106/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4449 - accuracy: 0.7800\n",
      "Epoch 107/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4265 - accuracy: 0.8067\n",
      "Epoch 108/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.4184 - accuracy: 0.8086\n",
      "Epoch 109/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4197 - accuracy: 0.8146\n",
      "Epoch 110/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.4164 - accuracy: 0.8107\n",
      "Epoch 111/150\n",
      "12/12 [==============================] - 0s 996us/step - loss: 0.4258 - accuracy: 0.8034\n",
      "Epoch 112/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4148 - accuracy: 0.8095\n",
      "Epoch 113/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4026 - accuracy: 0.8246\n",
      "Epoch 114/150\n",
      "12/12 [==============================] - 0s 907us/step - loss: 0.3978 - accuracy: 0.8196\n",
      "Epoch 115/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4066 - accuracy: 0.8060\n",
      "Epoch 116/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3807 - accuracy: 0.8341\n",
      "Epoch 117/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4218 - accuracy: 0.7842\n",
      "Epoch 118/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4047 - accuracy: 0.8447\n",
      "Epoch 119/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4099 - accuracy: 0.8162\n",
      "Epoch 120/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.4372 - accuracy: 0.7876\n",
      "Epoch 121/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3858 - accuracy: 0.8263\n",
      "Epoch 122/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3897 - accuracy: 0.8090\n",
      "Epoch 123/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3960 - accuracy: 0.8448\n",
      "Epoch 124/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4063 - accuracy: 0.8046\n",
      "Epoch 125/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4368 - accuracy: 0.8161\n",
      "Epoch 126/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4212 - accuracy: 0.8073\n",
      "Epoch 127/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3948 - accuracy: 0.7943\n",
      "Epoch 128/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3642 - accuracy: 0.8316\n",
      "Epoch 129/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4024 - accuracy: 0.7952\n",
      "Epoch 130/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.4050 - accuracy: 0.7930\n",
      "Epoch 131/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4093 - accuracy: 0.7940\n",
      "Epoch 132/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3709 - accuracy: 0.8222\n",
      "Epoch 133/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3879 - accuracy: 0.7975\n",
      "Epoch 134/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4010 - accuracy: 0.8143\n",
      "Epoch 135/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3868 - accuracy: 0.8134\n",
      "Epoch 136/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3915 - accuracy: 0.8046\n",
      "Epoch 137/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3988 - accuracy: 0.8189\n",
      "Epoch 138/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3688 - accuracy: 0.8280\n",
      "Epoch 139/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3724 - accuracy: 0.8226\n",
      "Epoch 140/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3452 - accuracy: 0.8324\n",
      "Epoch 141/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4091 - accuracy: 0.8181\n",
      "Epoch 142/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3723 - accuracy: 0.8314\n",
      "Epoch 143/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.4085 - accuracy: 0.8016\n",
      "Epoch 144/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3595 - accuracy: 0.8307\n",
      "Epoch 145/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3771 - accuracy: 0.8203\n",
      "Epoch 146/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3626 - accuracy: 0.8177\n",
      "Epoch 147/150\n",
      "12/12 [==============================] - 0s 996us/step - loss: 0.3735 - accuracy: 0.8194\n",
      "Epoch 148/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3496 - accuracy: 0.8376\n",
      "Epoch 149/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3622 - accuracy: 0.8300\n",
      "Epoch 150/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3761 - accuracy: 0.8128\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000281D6A33E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 997us/step\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\position\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 1ms/step - loss: 1.3855 - accuracy: 0.3609\n",
      "Epoch 2/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.3784 - accuracy: 0.3828\n",
      "Epoch 3/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.3459 - accuracy: 0.5821\n",
      "Epoch 4/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.2336 - accuracy: 0.5568\n",
      "Epoch 5/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9990 - accuracy: 0.5978\n",
      "Epoch 6/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8033 - accuracy: 0.5724\n",
      "Epoch 7/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.7394 - accuracy: 0.5783\n",
      "Epoch 8/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6888 - accuracy: 0.5922\n",
      "Epoch 9/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6823 - accuracy: 0.6157\n",
      "Epoch 10/150\n",
      "12/12 [==============================] - 0s 998us/step - loss: 0.6967 - accuracy: 0.5724\n",
      "Epoch 11/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6754 - accuracy: 0.6092\n",
      "Epoch 12/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6700 - accuracy: 0.6158\n",
      "Epoch 13/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6707 - accuracy: 0.6120\n",
      "Epoch 14/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6609 - accuracy: 0.6089\n",
      "Epoch 15/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6770 - accuracy: 0.6202\n",
      "Epoch 16/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6581 - accuracy: 0.6315\n",
      "Epoch 17/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6376 - accuracy: 0.6599\n",
      "Epoch 18/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6476 - accuracy: 0.6457\n",
      "Epoch 19/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6343 - accuracy: 0.6487\n",
      "Epoch 20/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6241 - accuracy: 0.6526\n",
      "Epoch 21/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6034 - accuracy: 0.6888\n",
      "Epoch 22/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6182 - accuracy: 0.6513\n",
      "Epoch 23/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5948 - accuracy: 0.6948\n",
      "Epoch 24/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6194 - accuracy: 0.6208\n",
      "Epoch 25/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6165 - accuracy: 0.6473\n",
      "Epoch 26/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5881 - accuracy: 0.6578\n",
      "Epoch 27/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5841 - accuracy: 0.6548\n",
      "Epoch 28/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5782 - accuracy: 0.6612\n",
      "Epoch 29/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6237 - accuracy: 0.5950\n",
      "Epoch 30/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5729 - accuracy: 0.6630\n",
      "Epoch 31/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.5967 - accuracy: 0.6512\n",
      "Epoch 32/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5568 - accuracy: 0.6811\n",
      "Epoch 33/150\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5528 - accuracy: 0.66 - 0s 1ms/step - loss: 0.5658 - accuracy: 0.6603\n",
      "Epoch 34/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5884 - accuracy: 0.6231\n",
      "Epoch 35/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5595 - accuracy: 0.6692\n",
      "Epoch 36/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5607 - accuracy: 0.6737\n",
      "Epoch 37/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5561 - accuracy: 0.6783\n",
      "Epoch 38/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5679 - accuracy: 0.6658\n",
      "Epoch 39/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5589 - accuracy: 0.7002\n",
      "Epoch 40/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5571 - accuracy: 0.6742\n",
      "Epoch 41/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5524 - accuracy: 0.6705\n",
      "Epoch 42/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.5339 - accuracy: 0.6910\n",
      "Epoch 43/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.5447 - accuracy: 0.6840\n",
      "Epoch 44/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5346 - accuracy: 0.6788\n",
      "Epoch 45/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5262 - accuracy: 0.6923\n",
      "Epoch 46/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5489 - accuracy: 0.6889\n",
      "Epoch 47/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5570 - accuracy: 0.6457\n",
      "Epoch 48/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.5504 - accuracy: 0.6669\n",
      "Epoch 49/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5325 - accuracy: 0.6660\n",
      "Epoch 50/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5308 - accuracy: 0.6987\n",
      "Epoch 51/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5558 - accuracy: 0.6527\n",
      "Epoch 52/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5517 - accuracy: 0.6785\n",
      "Epoch 53/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5164 - accuracy: 0.7054\n",
      "Epoch 54/150\n",
      "12/12 [==============================] - 0s 998us/step - loss: 0.5390 - accuracy: 0.7137\n",
      "Epoch 55/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5514 - accuracy: 0.6606\n",
      "Epoch 56/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5470 - accuracy: 0.6926\n",
      "Epoch 57/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5058 - accuracy: 0.6957\n",
      "Epoch 58/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5358 - accuracy: 0.7416\n",
      "Epoch 59/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5206 - accuracy: 0.6855\n",
      "Epoch 60/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5294 - accuracy: 0.7289\n",
      "Epoch 61/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5318 - accuracy: 0.7089\n",
      "Epoch 62/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.5372 - accuracy: 0.6970\n",
      "Epoch 63/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5225 - accuracy: 0.6896\n",
      "Epoch 64/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.5225 - accuracy: 0.7363\n",
      "Epoch 65/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5088 - accuracy: 0.7380\n",
      "Epoch 66/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5168 - accuracy: 0.7241\n",
      "Epoch 67/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4852 - accuracy: 0.7424\n",
      "Epoch 68/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5151 - accuracy: 0.8004\n",
      "Epoch 69/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4853 - accuracy: 0.7470\n",
      "Epoch 70/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4945 - accuracy: 0.8021\n",
      "Epoch 71/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4869 - accuracy: 0.7957\n",
      "Epoch 72/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.4889 - accuracy: 0.7990\n",
      "Epoch 73/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5265 - accuracy: 0.7543\n",
      "Epoch 74/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4895 - accuracy: 0.7386\n",
      "Epoch 75/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4817 - accuracy: 0.7729\n",
      "Epoch 76/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4767 - accuracy: 0.8006\n",
      "Epoch 77/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5003 - accuracy: 0.7961\n",
      "Epoch 78/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4727 - accuracy: 0.8000\n",
      "Epoch 79/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4810 - accuracy: 0.8181\n",
      "Epoch 80/150\n",
      "12/12 [==============================] - 0s 962us/step - loss: 0.4514 - accuracy: 0.8303\n",
      "Epoch 81/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4992 - accuracy: 0.8085\n",
      "Epoch 82/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4713 - accuracy: 0.7936\n",
      "Epoch 83/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4384 - accuracy: 0.8333\n",
      "Epoch 84/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4448 - accuracy: 0.8363\n",
      "Epoch 85/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4363 - accuracy: 0.8478\n",
      "Epoch 86/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4433 - accuracy: 0.8271\n",
      "Epoch 87/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3935 - accuracy: 0.8676\n",
      "Epoch 88/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4305 - accuracy: 0.7871\n",
      "Epoch 89/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4243 - accuracy: 0.8418\n",
      "Epoch 90/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4473 - accuracy: 0.7935\n",
      "Epoch 91/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4469 - accuracy: 0.7988\n",
      "Epoch 92/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4191 - accuracy: 0.8453\n",
      "Epoch 93/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4010 - accuracy: 0.8430\n",
      "Epoch 94/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4228 - accuracy: 0.8338\n",
      "Epoch 95/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.4247 - accuracy: 0.8108\n",
      "Epoch 96/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4416 - accuracy: 0.7914\n",
      "Epoch 97/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4213 - accuracy: 0.8359\n",
      "Epoch 98/150\n",
      "12/12 [==============================] - 0s 999us/step - loss: 0.4346 - accuracy: 0.7648\n",
      "Epoch 99/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.4067 - accuracy: 0.8303\n",
      "Epoch 100/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4223 - accuracy: 0.8061\n",
      "Epoch 101/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3881 - accuracy: 0.8326\n",
      "Epoch 102/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3877 - accuracy: 0.8325\n",
      "Epoch 103/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3956 - accuracy: 0.8221\n",
      "Epoch 104/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4010 - accuracy: 0.8289\n",
      "Epoch 105/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4131 - accuracy: 0.8041\n",
      "Epoch 106/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4050 - accuracy: 0.8005\n",
      "Epoch 107/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3745 - accuracy: 0.8470\n",
      "Epoch 108/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4078 - accuracy: 0.8322\n",
      "Epoch 109/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4173 - accuracy: 0.7894\n",
      "Epoch 110/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3963 - accuracy: 0.8289\n",
      "Epoch 111/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4070 - accuracy: 0.8083\n",
      "Epoch 112/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3826 - accuracy: 0.8267\n",
      "Epoch 113/150\n",
      "12/12 [==============================] - 0s 998us/step - loss: 0.4043 - accuracy: 0.8055\n",
      "Epoch 114/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3739 - accuracy: 0.8491\n",
      "Epoch 115/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3973 - accuracy: 0.7988\n",
      "Epoch 116/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3750 - accuracy: 0.8341\n",
      "Epoch 117/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4195 - accuracy: 0.7780\n",
      "Epoch 118/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3705 - accuracy: 0.8485\n",
      "Epoch 119/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4105 - accuracy: 0.8204\n",
      "Epoch 120/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3716 - accuracy: 0.8123\n",
      "Epoch 121/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.4055 - accuracy: 0.7949\n",
      "Epoch 122/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3734 - accuracy: 0.8512\n",
      "Epoch 123/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3917 - accuracy: 0.8227\n",
      "Epoch 124/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4121 - accuracy: 0.8030\n",
      "Epoch 125/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3935 - accuracy: 0.8241\n",
      "Epoch 126/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3654 - accuracy: 0.8393\n",
      "Epoch 127/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3749 - accuracy: 0.8294\n",
      "Epoch 128/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3667 - accuracy: 0.8368\n",
      "Epoch 129/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3716 - accuracy: 0.8234\n",
      "Epoch 130/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3887 - accuracy: 0.8254\n",
      "Epoch 131/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3606 - accuracy: 0.8312\n",
      "Epoch 132/150\n",
      "12/12 [==============================] - 0s 907us/step - loss: 0.3507 - accuracy: 0.8541\n",
      "Epoch 133/150\n",
      "12/12 [==============================] - 0s 1000us/step - loss: 0.3592 - accuracy: 0.8543\n",
      "Epoch 134/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4365 - accuracy: 0.7852\n",
      "Epoch 135/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3551 - accuracy: 0.8439\n",
      "Epoch 136/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3869 - accuracy: 0.8087\n",
      "Epoch 137/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3643 - accuracy: 0.8330\n",
      "Epoch 138/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3462 - accuracy: 0.8379\n",
      "Epoch 139/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3809 - accuracy: 0.8252\n",
      "Epoch 140/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3581 - accuracy: 0.8330\n",
      "Epoch 141/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3532 - accuracy: 0.8203\n",
      "Epoch 142/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3528 - accuracy: 0.8582\n",
      "Epoch 143/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3618 - accuracy: 0.8525\n",
      "Epoch 144/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3553 - accuracy: 0.8442\n",
      "Epoch 145/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3648 - accuracy: 0.8268\n",
      "Epoch 146/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3895 - accuracy: 0.8258\n",
      "Epoch 147/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3793 - accuracy: 0.8172\n",
      "Epoch 148/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3791 - accuracy: 0.8304\n",
      "Epoch 149/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3751 - accuracy: 0.8456\n",
      "Epoch 150/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3642 - accuracy: 0.8026\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000281D57E3040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\position\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 1ms/step - loss: 1.3855 - accuracy: 0.3636\n",
      "Epoch 2/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.3774 - accuracy: 0.3351\n",
      "Epoch 3/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.3350 - accuracy: 0.4582\n",
      "Epoch 4/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.2043 - accuracy: 0.5888\n",
      "Epoch 5/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9690 - accuracy: 0.5748\n",
      "Epoch 6/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8192 - accuracy: 0.5267\n",
      "Epoch 7/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.7140 - accuracy: 0.5441\n",
      "Epoch 8/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.7070 - accuracy: 0.5304\n",
      "Epoch 9/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.6845 - accuracy: 0.5663\n",
      "Epoch 10/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6972 - accuracy: 0.5417\n",
      "Epoch 11/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6807 - accuracy: 0.5874\n",
      "Epoch 12/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6772 - accuracy: 0.6058\n",
      "Epoch 13/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6778 - accuracy: 0.6035\n",
      "Epoch 14/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6699 - accuracy: 0.5896\n",
      "Epoch 15/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6727 - accuracy: 0.5968\n",
      "Epoch 16/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6576 - accuracy: 0.6487\n",
      "Epoch 17/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6575 - accuracy: 0.6405\n",
      "Epoch 18/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6620 - accuracy: 0.5948\n",
      "Epoch 19/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6312 - accuracy: 0.6504\n",
      "Epoch 20/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6383 - accuracy: 0.6375\n",
      "Epoch 21/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6250 - accuracy: 0.6497\n",
      "Epoch 22/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6146 - accuracy: 0.6563\n",
      "Epoch 23/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6165 - accuracy: 0.6627\n",
      "Epoch 24/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5889 - accuracy: 0.6712\n",
      "Epoch 25/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5820 - accuracy: 0.6858\n",
      "Epoch 26/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6154 - accuracy: 0.6274\n",
      "Epoch 27/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6226 - accuracy: 0.6484\n",
      "Epoch 28/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5804 - accuracy: 0.6702\n",
      "Epoch 29/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5784 - accuracy: 0.6818\n",
      "Epoch 30/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.6146 - accuracy: 0.6423\n",
      "Epoch 31/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5806 - accuracy: 0.6501\n",
      "Epoch 32/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5595 - accuracy: 0.6877\n",
      "Epoch 33/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5677 - accuracy: 0.6482\n",
      "Epoch 34/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5683 - accuracy: 0.6685\n",
      "Epoch 35/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5562 - accuracy: 0.6608\n",
      "Epoch 36/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5440 - accuracy: 0.6847\n",
      "Epoch 37/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5638 - accuracy: 0.6868\n",
      "Epoch 38/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5612 - accuracy: 0.6524\n",
      "Epoch 39/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5299 - accuracy: 0.6925\n",
      "Epoch 40/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5579 - accuracy: 0.6888\n",
      "Epoch 41/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5468 - accuracy: 0.6651\n",
      "Epoch 42/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5530 - accuracy: 0.6954\n",
      "Epoch 43/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5441 - accuracy: 0.6803\n",
      "Epoch 44/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5330 - accuracy: 0.7150\n",
      "Epoch 45/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5405 - accuracy: 0.6802\n",
      "Epoch 46/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5502 - accuracy: 0.6819\n",
      "Epoch 47/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5273 - accuracy: 0.6942\n",
      "Epoch 48/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5223 - accuracy: 0.7111\n",
      "Epoch 49/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5441 - accuracy: 0.7159\n",
      "Epoch 50/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5465 - accuracy: 0.6648\n",
      "Epoch 51/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5478 - accuracy: 0.6743\n",
      "Epoch 52/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5204 - accuracy: 0.7189\n",
      "Epoch 53/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5172 - accuracy: 0.7136\n",
      "Epoch 54/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5330 - accuracy: 0.7197\n",
      "Epoch 55/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5160 - accuracy: 0.7329\n",
      "Epoch 56/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5289 - accuracy: 0.7616\n",
      "Epoch 57/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4893 - accuracy: 0.7373\n",
      "Epoch 58/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.5125 - accuracy: 0.7254\n",
      "Epoch 59/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5072 - accuracy: 0.7222\n",
      "Epoch 60/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5127 - accuracy: 0.7642\n",
      "Epoch 61/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5162 - accuracy: 0.7073\n",
      "Epoch 62/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5177 - accuracy: 0.7062\n",
      "Epoch 63/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5006 - accuracy: 0.7875\n",
      "Epoch 64/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5130 - accuracy: 0.6887\n",
      "Epoch 65/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5057 - accuracy: 0.7762\n",
      "Epoch 66/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4921 - accuracy: 0.7766\n",
      "Epoch 67/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4887 - accuracy: 0.8035\n",
      "Epoch 68/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4997 - accuracy: 0.7510\n",
      "Epoch 69/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4886 - accuracy: 0.8141\n",
      "Epoch 70/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4986 - accuracy: 0.7495\n",
      "Epoch 71/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4648 - accuracy: 0.7982\n",
      "Epoch 72/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4844 - accuracy: 0.7805\n",
      "Epoch 73/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4725 - accuracy: 0.7794\n",
      "Epoch 74/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4570 - accuracy: 0.7981\n",
      "Epoch 75/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4576 - accuracy: 0.7967\n",
      "Epoch 76/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4788 - accuracy: 0.8235\n",
      "Epoch 77/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4599 - accuracy: 0.8231\n",
      "Epoch 78/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4455 - accuracy: 0.8178\n",
      "Epoch 79/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4684 - accuracy: 0.7867\n",
      "Epoch 80/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4517 - accuracy: 0.8236\n",
      "Epoch 81/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4348 - accuracy: 0.8092\n",
      "Epoch 82/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4119 - accuracy: 0.8317\n",
      "Epoch 83/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4377 - accuracy: 0.8073\n",
      "Epoch 84/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4028 - accuracy: 0.8449\n",
      "Epoch 85/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4239 - accuracy: 0.8039\n",
      "Epoch 86/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4420 - accuracy: 0.8115\n",
      "Epoch 87/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4184 - accuracy: 0.8463\n",
      "Epoch 88/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3993 - accuracy: 0.8490\n",
      "Epoch 89/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3989 - accuracy: 0.8243\n",
      "Epoch 90/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3915 - accuracy: 0.8474\n",
      "Epoch 91/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4057 - accuracy: 0.8350\n",
      "Epoch 92/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3880 - accuracy: 0.8500\n",
      "Epoch 93/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4045 - accuracy: 0.8387\n",
      "Epoch 94/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3717 - accuracy: 0.8475\n",
      "Epoch 95/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3605 - accuracy: 0.8560\n",
      "Epoch 96/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3731 - accuracy: 0.8474\n",
      "Epoch 97/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3491 - accuracy: 0.8641\n",
      "Epoch 98/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3986 - accuracy: 0.8191\n",
      "Epoch 99/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4054 - accuracy: 0.8192\n",
      "Epoch 100/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3825 - accuracy: 0.8444\n",
      "Epoch 101/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3778 - accuracy: 0.8298\n",
      "Epoch 102/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4084 - accuracy: 0.8257\n",
      "Epoch 103/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4207 - accuracy: 0.8377\n",
      "Epoch 104/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3498 - accuracy: 0.8529\n",
      "Epoch 105/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3516 - accuracy: 0.8543\n",
      "Epoch 106/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4034 - accuracy: 0.8338\n",
      "Epoch 107/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3729 - accuracy: 0.8521\n",
      "Epoch 108/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3944 - accuracy: 0.8395\n",
      "Epoch 109/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3660 - accuracy: 0.8495\n",
      "Epoch 110/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3777 - accuracy: 0.8478\n",
      "Epoch 111/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3766 - accuracy: 0.8322\n",
      "Epoch 112/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3624 - accuracy: 0.8482\n",
      "Epoch 113/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3625 - accuracy: 0.8555\n",
      "Epoch 114/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3686 - accuracy: 0.8623\n",
      "Epoch 115/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3667 - accuracy: 0.8375\n",
      "Epoch 116/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3410 - accuracy: 0.8488\n",
      "Epoch 117/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3826 - accuracy: 0.8328\n",
      "Epoch 118/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3882 - accuracy: 0.8302\n",
      "Epoch 119/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3537 - accuracy: 0.8603\n",
      "Epoch 120/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3349 - accuracy: 0.8653\n",
      "Epoch 121/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3814 - accuracy: 0.8482\n",
      "Epoch 122/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3455 - accuracy: 0.8590\n",
      "Epoch 123/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3448 - accuracy: 0.8667\n",
      "Epoch 124/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3405 - accuracy: 0.8574\n",
      "Epoch 125/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3408 - accuracy: 0.8710\n",
      "Epoch 126/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3455 - accuracy: 0.8502\n",
      "Epoch 127/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3537 - accuracy: 0.8558\n",
      "Epoch 128/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3342 - accuracy: 0.8704\n",
      "Epoch 129/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3278 - accuracy: 0.8742\n",
      "Epoch 130/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3322 - accuracy: 0.8533\n",
      "Epoch 131/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3569 - accuracy: 0.8552\n",
      "Epoch 132/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3515 - accuracy: 0.8571\n",
      "Epoch 133/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3374 - accuracy: 0.8604\n",
      "Epoch 134/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3430 - accuracy: 0.8717\n",
      "Epoch 135/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3580 - accuracy: 0.8463\n",
      "Epoch 136/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3722 - accuracy: 0.8407\n",
      "Epoch 137/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3528 - accuracy: 0.8529\n",
      "Epoch 138/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3324 - accuracy: 0.8696\n",
      "Epoch 139/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3313 - accuracy: 0.8697\n",
      "Epoch 140/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3043 - accuracy: 0.8847\n",
      "Epoch 141/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3304 - accuracy: 0.8650\n",
      "Epoch 142/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3273 - accuracy: 0.8604\n",
      "Epoch 143/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3459 - accuracy: 0.8368\n",
      "Epoch 144/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3243 - accuracy: 0.8713\n",
      "Epoch 145/150\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.3425 - accuracy: 0.8376\n",
      "Epoch 146/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3630 - accuracy: 0.8458\n",
      "Epoch 147/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3434 - accuracy: 0.8410\n",
      "Epoch 148/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3279 - accuracy: 0.8594\n",
      "Epoch 149/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3002 - accuracy: 0.8751\n",
      "Epoch 150/150\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3346 - accuracy: 0.8499\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000281D5929AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "K-fold Cross Validation Accuracy Results:  [0.83076923 0.75384615 0.78461538 0.75384615 0.75384615 0.76923077\n",
      " 0.86153846 0.8125     0.796875   0.71875   ]\n",
      "K-fold Cross Validation Accuracy Results Mean:  0.7835817307692308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\position\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "cv_model = grid_cv_model.best_estimator_\n",
    "\n",
    "#%% K-FOLD\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# K-fold accuracy scores\n",
    "\n",
    "kfold = KFold(n_splits = 10, shuffle=True)\n",
    "# results = cross_val_score(cv_model, X_test, np.argmax(y_test, axis=1), cv=kfold,scoring= 'accuracy')\n",
    "results = cross_val_score(cv_model, X3_test, Y3_test, cv=kfold,scoring= 'accuracy')\n",
    "\n",
    "print('K-fold Cross Validation Accuracy Results: ', results)\n",
    "print('K-fold Cross Validation Accuracy Results Mean: ', results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "06c385fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.64      0.69       134\n",
      "           1       0.73      0.82      0.77       154\n",
      "           2       0.90      0.89      0.89       141\n",
      "           3       0.93      0.93      0.93       218\n",
      "\n",
      "    accuracy                           0.84       647\n",
      "   macro avg       0.83      0.82      0.82       647\n",
      "weighted avg       0.84      0.84      0.84       647\n",
      "\n",
      "[[ 86  48   0   0]\n",
      " [ 27 127   0   0]\n",
      " [  1   0 125  15]\n",
      " [  1   0  14 203]]\n"
     ]
    }
   ],
   "source": [
    "#%% Confusion Matrix and Classification Report\n",
    "from sklearn.metrics import confusion_matrix, classification_report \n",
    "\n",
    "# Classification Report\n",
    "model_report = classification_report(Y3_test, Y_pred)\n",
    "print(model_report)\n",
    "\n",
    "# Confusion Matrix\n",
    "model_conf = confusion_matrix(Y3_test, Y_pred)\n",
    "print(model_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f63f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
